device: cuda.
Representation model loaded from /content/drive/MyDrive/Laplacian/laplacian_code_Wu_run3_room/log/laprepr/HardMaze/test/model.ckpt.
Start collecting transitions.
(10000/10000) steps collected.
Replay buffer initialization finished, time cost: 1.4805047512054443s
Training steps per second: 0.
Step 1; q_loss 1.127; mean_q -1.079; min_q -1.238; max_q -0.5197; mean_r -1.081; mean_dsc 0.98; 
Tested 50 episodes at step 1, reward mean -50, std 0, time cost 1.22s.
Training steps per second: 182.8.
Step 1000; q_loss 0.003591; mean_q -1.597; min_q -1.907; max_q -0.3814; mean_r -1.041; mean_dsc 0.98; 
Tested 50 episodes at step 1000, reward mean -49.94, std 0.2375, time cost 1.229s.
Training steps per second: 182.9.
Step 2000; q_loss 0.00368; mean_q -2.384; min_q -2.886; max_q -0.6582; mean_r -1.043; mean_dsc 0.98; 
Tested 50 episodes at step 2000, reward mean -50, std 0, time cost 1.208s.
Training steps per second: 183.7.
Step 3000; q_loss 0.007462; mean_q -3.119; min_q -3.87; max_q -0.9663; mean_r -1.015; mean_dsc 0.98; 
Tested 50 episodes at step 3000, reward mean -49.52, std 3.36, time cost 1.212s.
Training steps per second: 183.3.
Step 4000; q_loss 0.01473; mean_q -3.915; min_q -4.779; max_q -1.749; mean_r -1.02; mean_dsc 0.98; 
Tested 50 episodes at step 4000, reward mean -41.74, std 17.65, time cost 1.2s.
Training steps per second: 182.5.
Step 5000; q_loss 0.01119; mean_q -4.741; min_q -5.81; max_q -1.366; mean_r -1.03; mean_dsc 0.98; 
Tested 50 episodes at step 5000, reward mean -43.52, std 16.07, time cost 1.21s.
Training steps per second: 181.
Step 6000; q_loss 0.0123; mean_q -5.467; min_q -6.6; max_q -1.372; mean_r -1.029; mean_dsc 0.98; 
Tested 50 episodes at step 6000, reward mean -46.3, std 12.58, time cost 1.262s.
Training steps per second: 178.6.
Step 7000; q_loss 0.01455; mean_q -6.09; min_q -7.604; max_q -1.333; mean_r -1.008; mean_dsc 0.98; 
Tested 50 episodes at step 7000, reward mean -41.22, std 17.66, time cost 1.21s.
Training steps per second: 178.8.
Step 8000; q_loss 0.02523; mean_q -6.917; min_q -8.347; max_q -1.272; mean_r -1.024; mean_dsc 0.98; 
Tested 50 episodes at step 8000, reward mean -39.46, std 18.83, time cost 1.223s.
Training steps per second: 179.9.
Step 9000; q_loss 0.02056; mean_q -7.686; min_q -9.435; max_q -1.244; mean_r -1.025; mean_dsc 0.98; 
Tested 50 episodes at step 9000, reward mean -39.86, std 19.15, time cost 1.227s.
Training steps per second: 169.8.
Step 10000; q_loss 0.0347; mean_q -7.87; min_q -10.29; max_q -1.216; mean_r -0.9904; mean_dsc 0.98; 
Tested 50 episodes at step 10000, reward mean -39.08, std 19.46, time cost 1.195s.
Training steps per second: 182.5.
Step 11000; q_loss 0.03671; mean_q -8.847; min_q -11.12; max_q -1.258; mean_r -1.017; mean_dsc 0.98; 
Tested 50 episodes at step 11000, reward mean -37.56, std 20.02, time cost 1.275s.
Training steps per second: 178.8.
Step 12000; q_loss 0.02889; mean_q -9.628; min_q -11.95; max_q -1.273; mean_r -1.002; mean_dsc 0.98; 
Tested 50 episodes at step 12000, reward mean -40.22, std 18.56, time cost 1.23s.
Training steps per second: 180.3.
Step 13000; q_loss 0.03403; mean_q -9.528; min_q -12.8; max_q -1.283; mean_r -0.9722; mean_dsc 0.98; 
Tested 50 episodes at step 13000, reward mean -38.84, std 19.05, time cost 1.213s.
Training steps per second: 180.5.
Step 14000; q_loss 0.0542; mean_q -10.37; min_q -13.47; max_q -1.304; mean_r -0.9715; mean_dsc 0.98; 
Tested 50 episodes at step 14000, reward mean -39.32, std 19.11, time cost 1.212s.
Training steps per second: 178.2.
Step 15000; q_loss 0.01831; mean_q -10.28; min_q -14.36; max_q -1.28; mean_r -0.9311; mean_dsc 0.98; 
Tested 50 episodes at step 15000, reward mean -33.24, std 21.54, time cost 1.198s.
Training steps per second: 181.1.
Step 16000; q_loss 0.02884; mean_q -11.39; min_q -15.22; max_q -1.236; mean_r -0.9716; mean_dsc 0.98; 
Tested 50 episodes at step 16000, reward mean -33.82, std 20.81, time cost 1.214s.
Training steps per second: 178.6.
Step 17000; q_loss 0.05251; mean_q -11.78; min_q -15.94; max_q -1.228; mean_r -0.9466; mean_dsc 0.98; 
Tested 50 episodes at step 17000, reward mean -35.16, std 20.85, time cost 1.223s.
Training steps per second: 179.
Step 18000; q_loss 0.04227; mean_q -12.08; min_q -16.67; max_q -1.244; mean_r -0.9466; mean_dsc 0.98; 
Tested 50 episodes at step 18000, reward mean -36.06, std 20.52, time cost 1.224s.
Training steps per second: 178.9.
Step 19000; q_loss 0.07394; mean_q -12.77; min_q -17.32; max_q -1.233; mean_r -0.998; mean_dsc 0.98; 
Tested 50 episodes at step 19000, reward mean -35.04, std 21.05, time cost 1.209s.
Training steps per second: 172.
Step 20000; q_loss 0.04178; mean_q -12.64; min_q -18.17; max_q -1.217; mean_r -0.9181; mean_dsc 0.98; 
Tested 50 episodes at step 20000, reward mean -36.56, std 20.61, time cost 1.209s.
Training steps per second: 179.2.
Step 21000; q_loss 0.04198; mean_q -13.58; min_q -18.8; max_q -1.197; mean_r -0.9747; mean_dsc 0.98; 
Tested 50 episodes at step 21000, reward mean -35.82, std 20.81, time cost 1.21s.
Training steps per second: 179.7.
Step 22000; q_loss 0.04246; mean_q -12.98; min_q -19.52; max_q -1.186; mean_r -0.9001; mean_dsc 0.98; 
Tested 50 episodes at step 22000, reward mean -35.7, std 20.14, time cost 1.24s.
Training steps per second: 177.9.
Step 23000; q_loss 0.04921; mean_q -13.93; min_q -20.28; max_q -1.186; mean_r -0.9412; mean_dsc 0.98; 
Tested 50 episodes at step 23000, reward mean -34.04, std 20.65, time cost 1.242s.
Training steps per second: 177.8.
Step 24000; q_loss 0.01786; mean_q -13.71; min_q -20.8; max_q -1.174; mean_r -0.9311; mean_dsc 0.98; 
Tested 50 episodes at step 24000, reward mean -34.68, std 20.72, time cost 1.24s.
Training steps per second: 179.6.
Step 25000; q_loss 0.04522; mean_q -15.49; min_q -21.57; max_q -1.143; mean_r -0.9653; mean_dsc 0.98; 
Tested 50 episodes at step 25000, reward mean -33.38, std 20.72, time cost 1.233s.
Training steps per second: 178.9.
Step 26000; q_loss 0.03067; mean_q -14.8; min_q -22.13; max_q -1.117; mean_r -0.9519; mean_dsc 0.98; 
Tested 50 episodes at step 26000, reward mean -36.04, std 19.59, time cost 1.223s.
Training steps per second: 178.7.
Step 27000; q_loss 0.01942; mean_q -16.44; min_q -22.77; max_q -1.071; mean_r -0.9979; mean_dsc 0.98; 
Tested 50 episodes at step 27000, reward mean -36.34, std 20, time cost 1.241s.
Training steps per second: 176.3.
Step 28000; q_loss 0.02952; mean_q -16.35; min_q -23.37; max_q -1.033; mean_r -0.9482; mean_dsc 0.98; 
Tested 50 episodes at step 28000, reward mean -32.56, std 20.83, time cost 1.218s.
Training steps per second: 177.6.
Step 29000; q_loss 0.01444; mean_q -15.92; min_q -24.07; max_q -1.016; mean_r -0.909; mean_dsc 0.98; 
Tested 50 episodes at step 29000, reward mean -35.26, std 20.03, time cost 1.212s.
Training steps per second: 168.4.
Step 30000; q_loss 0.03045; mean_q -15.74; min_q -24.67; max_q -1.024; mean_r -0.9282; mean_dsc 0.98; 
Tested 50 episodes at step 30000, reward mean -37.68, std 19.06, time cost 1.214s.
Training steps per second: 180.1.
Step 31000; q_loss 0.03317; mean_q -17; min_q -25.29; max_q -1.007; mean_r -0.9417; mean_dsc 0.98; 
Tested 50 episodes at step 31000, reward mean -28.78, std 20.94, time cost 1.199s.
Training steps per second: 180.2.
Step 32000; q_loss 0.05149; mean_q -17.68; min_q -25.83; max_q -1.013; mean_r -0.961; mean_dsc 0.98; 
Tested 50 episodes at step 32000, reward mean -33.82, std 20.12, time cost 1.189s.
Training steps per second: 180.9.
Step 33000; q_loss 0.01887; mean_q -15.99; min_q -26.39; max_q -0.99; mean_r -0.8844; mean_dsc 0.98; 
Tested 50 episodes at step 33000, reward mean -38.12, std 19.27, time cost 1.226s.
Training steps per second: 180.1.
Step 34000; q_loss 0.0217; mean_q -17.66; min_q -26.95; max_q -0.9631; mean_r -0.8932; mean_dsc 0.98; 
Tested 50 episodes at step 34000, reward mean -35.16, std 18.58, time cost 1.208s.
Training steps per second: 154.1.
Step 35000; q_loss 0.02456; mean_q -17.9; min_q -27.26; max_q -0.9413; mean_r -0.9139; mean_dsc 0.98; 
Tested 50 episodes at step 35000, reward mean -32.46, std 21.1, time cost 1.215s.
Training steps per second: 176.3.
Step 36000; q_loss 0.06852; mean_q -17.9; min_q -27.64; max_q -0.9288; mean_r -0.8848; mean_dsc 0.98; 
Tested 50 episodes at step 36000, reward mean -31.16, std 20.31, time cost 1.209s.
Training steps per second: 178.1.
Step 37000; q_loss 0.03536; mean_q -18.66; min_q -28.44; max_q -0.9174; mean_r -0.943; mean_dsc 0.98; 
Tested 50 episodes at step 37000, reward mean -32.2, std 19.65, time cost 1.205s.
Training steps per second: 179.3.
Step 38000; q_loss 0.03163; mean_q -19.99; min_q -28.71; max_q -0.9053; mean_r -0.9148; mean_dsc 0.98; 
Tested 50 episodes at step 38000, reward mean -35.4, std 20.09, time cost 1.22s.
Training steps per second: 177.4.
Step 39000; q_loss 0.01515; mean_q -20.67; min_q -29.14; max_q -0.8867; mean_r -0.949; mean_dsc 0.98; 
Tested 50 episodes at step 39000, reward mean -37.16, std 17.67, time cost 1.211s.
Training steps per second: 170.8.
Step 40000; q_loss 0.02337; mean_q -20.46; min_q -29.64; max_q -0.8813; mean_r -0.9199; mean_dsc 0.98; 
Tested 50 episodes at step 40000, reward mean -32.44, std 20.29, time cost 1.219s.
Training steps per second: 179.7.
Step 41000; q_loss 0.03253; mean_q -18.13; min_q -30.08; max_q -0.8688; mean_r -0.8695; mean_dsc 0.98; 
Tested 50 episodes at step 41000, reward mean -33.54, std 19.09, time cost 1.21s.
Training steps per second: 179.2.
Step 42000; q_loss 0.03225; mean_q -19.85; min_q -30.81; max_q -0.857; mean_r -0.9059; mean_dsc 0.98; 
Tested 50 episodes at step 42000, reward mean -38.66, std 16.73, time cost 1.194s.
Training steps per second: 180.5.
Step 43000; q_loss 0.03369; mean_q -20.48; min_q -31.05; max_q -0.8447; mean_r -0.9054; mean_dsc 0.98; 
Tested 50 episodes at step 43000, reward mean -28.16, std 20.21, time cost 1.214s.
Training steps per second: 178.7.
Step 44000; q_loss 0.02615; mean_q -20.65; min_q -31.68; max_q -0.8362; mean_r -0.9076; mean_dsc 0.98; 
Tested 50 episodes at step 44000, reward mean -36.54, std 18.74, time cost 1.237s.
Training steps per second: 172.4.
Step 45000; q_loss 0.04068; mean_q -19.08; min_q -31.91; max_q -0.8293; mean_r -0.8552; mean_dsc 0.98; 
Tested 50 episodes at step 45000, reward mean -30.34, std 20.22, time cost 1.217s.
Training steps per second: 179.1.
Step 46000; q_loss 0.01791; mean_q -19.73; min_q -32.37; max_q -0.8134; mean_r -0.8937; mean_dsc 0.98; 
Tested 50 episodes at step 46000, reward mean -32.5, std 20.3, time cost 1.2s.
Training steps per second: 180.
Step 47000; q_loss 0.079; mean_q -21.95; min_q -32.8; max_q -0.8017; mean_r -0.8902; mean_dsc 0.98; 
Tested 50 episodes at step 47000, reward mean -34.46, std 20.4, time cost 1.209s.
Training steps per second: 179.7.
Step 48000; q_loss 0.04182; mean_q -19.3; min_q -33.27; max_q -0.7806; mean_r -0.8876; mean_dsc 0.98; 
Tested 50 episodes at step 48000, reward mean -29.7, std 19.99, time cost 1.195s.
Training steps per second: 180.
Step 49000; q_loss 0.01691; mean_q -22.59; min_q -33.82; max_q -0.7597; mean_r -0.8948; mean_dsc 0.98; 
Tested 50 episodes at step 49000, reward mean -31.66, std 19.13, time cost 1.282s.
Training steps per second: 166.2.
Step 50000; q_loss 0.01811; mean_q -19.73; min_q -34.13; max_q -0.7421; mean_r -0.8696; mean_dsc 0.98; 
Tested 50 episodes at step 50000, reward mean -29.44, std 19.38, time cost 1.218s.
Training steps per second: 177.2.
Step 51000; q_loss 0.1067; mean_q -20.42; min_q -34.56; max_q -0.7217; mean_r -0.8447; mean_dsc 0.98; 
Tested 50 episodes at step 51000, reward mean -30.7, std 20.29, time cost 1.238s.
Training steps per second: 177.1.
Step 52000; q_loss 0.03819; mean_q -18.8; min_q -35.06; max_q -0.7152; mean_r -0.8385; mean_dsc 0.98; 
Tested 50 episodes at step 52000, reward mean -30.88, std 18.04, time cost 1.211s.
Training steps per second: 178.4.
Step 53000; q_loss 0.01911; mean_q -20.12; min_q -35.34; max_q -0.7162; mean_r -0.8156; mean_dsc 0.98; 
Tested 50 episodes at step 53000, reward mean -29.68, std 18.77, time cost 1.19s.
Training steps per second: 178.6.
Step 54000; q_loss 0.01801; mean_q -21.17; min_q -35.74; max_q -0.7037; mean_r -0.8713; mean_dsc 0.98; 
Tested 50 episodes at step 54000, reward mean -29.92, std 18.77, time cost 1.219s.
Training steps per second: 177.9.
Step 55000; q_loss 0.03492; mean_q -19.52; min_q -36.1; max_q -0.6918; mean_r -0.8362; mean_dsc 0.98; 
Tested 50 episodes at step 55000, reward mean -32.6, std 19.62, time cost 1.213s.
Training steps per second: 178.8.
Step 56000; q_loss 0.03352; mean_q -22.17; min_q -36.42; max_q -0.6829; mean_r -0.908; mean_dsc 0.98; 
Tested 50 episodes at step 56000, reward mean -27.06, std 19.17, time cost 1.202s.
Training steps per second: 179.2.
Step 57000; q_loss 0.02488; mean_q -18.9; min_q -36.88; max_q -0.6896; mean_r -0.8348; mean_dsc 0.98; 
Tested 50 episodes at step 57000, reward mean -33.76, std 18.48, time cost 1.184s.
Training steps per second: 179.7.
Step 58000; q_loss 0.03315; mean_q -20; min_q -37.16; max_q -0.6736; mean_r -0.7801; mean_dsc 0.98; 
Tested 50 episodes at step 58000, reward mean -32.28, std 19.25, time cost 1.209s.
Training steps per second: 178.7.
Step 59000; q_loss 0.02608; mean_q -21.72; min_q -37.54; max_q -0.6429; mean_r -0.8488; mean_dsc 0.98; 
Tested 50 episodes at step 59000, reward mean -30.2, std 18.02, time cost 1.206s.
Training steps per second: 171.1.
Step 60000; q_loss 0.03754; mean_q -18.99; min_q -37.81; max_q -0.6268; mean_r -0.7799; mean_dsc 0.98; 
Tested 50 episodes at step 60000, reward mean -24.88, std 19.13, time cost 1.222s.
Training steps per second: 172.8.
Step 61000; q_loss 0.04362; mean_q -23.43; min_q -38.15; max_q -0.6194; mean_r -0.8927; mean_dsc 0.98; 
Tested 50 episodes at step 61000, reward mean -31.92, std 17.79, time cost 1.202s.
Training steps per second: 178.2.
Step 62000; q_loss 0.06002; mean_q -24.04; min_q -38.5; max_q -0.6089; mean_r -0.9063; mean_dsc 0.98; 
Tested 50 episodes at step 62000, reward mean -27.9, std 20.47, time cost 1.202s.
Training steps per second: 179.3.
Step 63000; q_loss 0.03345; mean_q -20.35; min_q -38.81; max_q -0.611; mean_r -0.8269; mean_dsc 0.98; 
Tested 50 episodes at step 63000, reward mean -32.24, std 18.85, time cost 1.225s.
Training steps per second: 176.
Step 64000; q_loss 0.03182; mean_q -20.09; min_q -39.08; max_q -0.5892; mean_r -0.8104; mean_dsc 0.98; 
Tested 50 episodes at step 64000, reward mean -33.92, std 18.69, time cost 1.198s.
Training steps per second: 178.8.
Step 65000; q_loss 0.02217; mean_q -23.47; min_q -39.35; max_q -0.5657; mean_r -0.8458; mean_dsc 0.98; 
Tested 50 episodes at step 65000, reward mean -26.58, std 18.19, time cost 1.201s.
Training steps per second: 176.9.
Step 66000; q_loss 0.03922; mean_q -21.28; min_q -39.89; max_q -0.5816; mean_r -0.8181; mean_dsc 0.98; 
Tested 50 episodes at step 66000, reward mean -28.14, std 18.77, time cost 1.224s.
Training steps per second: 177.3.
Step 67000; q_loss 0.07434; mean_q -21.62; min_q -39.98; max_q -0.5819; mean_r -0.9188; mean_dsc 0.98; 
Tested 50 episodes at step 67000, reward mean -33.28, std 18.77, time cost 1.24s.
Training steps per second: 177.8.
Step 68000; q_loss 0.02296; mean_q -21.14; min_q -40.43; max_q -0.5775; mean_r -0.8122; mean_dsc 0.98; 
Tested 50 episodes at step 68000, reward mean -30.46, std 17.48, time cost 1.199s.
Training steps per second: 179.3.
Step 69000; q_loss 0.009968; mean_q -21.62; min_q -40.48; max_q -0.5573; mean_r -0.8173; mean_dsc 0.98; 
Tested 50 episodes at step 69000, reward mean -26.14, std 19.78, time cost 1.204s.
Training steps per second: 170.
Step 70000; q_loss 0.01855; mean_q -21.66; min_q -40.92; max_q -0.5496; mean_r -0.8488; mean_dsc 0.98; 
Tested 50 episodes at step 70000, reward mean -31.28, std 18.76, time cost 1.209s.
Training steps per second: 178.9.
Step 71000; q_loss 0.01086; mean_q -23.35; min_q -40.92; max_q -0.5101; mean_r -0.8969; mean_dsc 0.98; 
Tested 50 episodes at step 71000, reward mean -28.32, std 19.46, time cost 1.215s.
Training steps per second: 177.9.
Step 72000; q_loss 0.01526; mean_q -21.19; min_q -41.31; max_q -0.493; mean_r -0.8293; mean_dsc 0.98; 
Tested 50 episodes at step 72000, reward mean -27.64, std 18.76, time cost 1.205s.
Training steps per second: 175.8.
Step 73000; q_loss 0.01778; mean_q -22.31; min_q -41.65; max_q -0.4915; mean_r -0.8383; mean_dsc 0.98; 
Tested 50 episodes at step 73000, reward mean -27.56, std 18.88, time cost 1.212s.
Training steps per second: 179.
Step 74000; q_loss 0.02516; mean_q -21.02; min_q -41.96; max_q -0.488; mean_r -0.7915; mean_dsc 0.98; 
Tested 50 episodes at step 74000, reward mean -28.14, std 19.1, time cost 1.351s.
Training steps per second: 174.3.
Step 75000; q_loss 0.03076; mean_q -19.74; min_q -42.25; max_q -0.4715; mean_r -0.8063; mean_dsc 0.98; 
Tested 50 episodes at step 75000, reward mean -28.96, std 18.59, time cost 1.217s.
Training steps per second: 178.1.
Step 76000; q_loss 0.03067; mean_q -21; min_q -42.3; max_q -0.4509; mean_r -0.8022; mean_dsc 0.98; 
Tested 50 episodes at step 76000, reward mean -29.98, std 18.04, time cost 1.218s.
Training steps per second: 177.7.
Step 77000; q_loss 0.01231; mean_q -23.54; min_q -42.48; max_q -0.4408; mean_r -0.8928; mean_dsc 0.98; 
Tested 50 episodes at step 77000, reward mean -34.44, std 19.06, time cost 1.213s.
Training steps per second: 176.7.
Step 78000; q_loss 0.02285; mean_q -19.69; min_q -42.57; max_q -0.4315; mean_r -0.8281; mean_dsc 0.98; 
Tested 50 episodes at step 78000, reward mean -29.72, std 19.09, time cost 1.202s.
Training steps per second: 178.4.
Step 79000; q_loss 0.01055; mean_q -23.03; min_q -43.09; max_q -0.4387; mean_r -0.8696; mean_dsc 0.98; 
Tested 50 episodes at step 79000, reward mean -27.76, std 18.31, time cost 1.227s.
Training steps per second: 158.
Step 80000; q_loss 0.0469; mean_q -21.75; min_q -43.33; max_q -0.4368; mean_r -0.8124; mean_dsc 0.98; 
Tested 50 episodes at step 80000, reward mean -29.66, std 16.51, time cost 1.239s.
Training steps per second: 176.3.
Step 81000; q_loss 0.02621; mean_q -20.18; min_q -43.51; max_q -0.4246; mean_r -0.7726; mean_dsc 0.98; 
Tested 50 episodes at step 81000, reward mean -34.08, std 20.27, time cost 1.269s.
Training steps per second: 166.8.
Step 82000; q_loss 0.0135; mean_q -17.26; min_q -43.8; max_q -0.4168; mean_r -0.7303; mean_dsc 0.98; 
Tested 50 episodes at step 82000, reward mean -28.6, std 18.82, time cost 1.241s.
Training steps per second: 174.3.
Step 83000; q_loss 0.02477; mean_q -18.89; min_q -43.88; max_q -0.39; mean_r -0.7529; mean_dsc 0.98; 
Tested 50 episodes at step 83000, reward mean -25.76, std 18.41, time cost 1.217s.
Training steps per second: 176.8.
Step 84000; q_loss 0.01154; mean_q -21.32; min_q -44.31; max_q -0.379; mean_r -0.7426; mean_dsc 0.98; 
Tested 50 episodes at step 84000, reward mean -26.46, std 18.11, time cost 1.215s.
Training steps per second: 177.8.
Step 85000; q_loss 0.02799; mean_q -20.84; min_q -44.14; max_q -0.3589; mean_r -0.8433; mean_dsc 0.98; 
Tested 50 episodes at step 85000, reward mean -28.72, std 20.96, time cost 1.21s.
Training steps per second: 178.6.
Step 86000; q_loss 0.02821; mean_q -20.31; min_q -44.5; max_q -0.354; mean_r -0.8245; mean_dsc 0.98; 
Tested 50 episodes at step 86000, reward mean -27.54, std 18.13, time cost 1.212s.
Training steps per second: 177.9.
Step 87000; q_loss 0.01915; mean_q -21.97; min_q -44.9; max_q -0.3571; mean_r -0.8067; mean_dsc 0.98; 
Tested 50 episodes at step 87000, reward mean -26.58, std 18.44, time cost 1.213s.
Training steps per second: 178.2.
Step 88000; q_loss 0.03964; mean_q -19.84; min_q -45.16; max_q -0.3445; mean_r -0.8125; mean_dsc 0.98; 
Tested 50 episodes at step 88000, reward mean -27.54, std 17.3, time cost 1.227s.
Training steps per second: 177.3.
Step 89000; q_loss 0.01934; mean_q -19.46; min_q -45.47; max_q -0.3336; mean_r -0.8152; mean_dsc 0.98; 
Tested 50 episodes at step 89000, reward mean -30.7, std 17.92, time cost 1.196s.
Training steps per second: 170.2.
Step 90000; q_loss 0.02357; mean_q -18.84; min_q -45.28; max_q -0.3166; mean_r -0.766; mean_dsc 0.98; 
Tested 50 episodes at step 90000, reward mean -32.08, std 18.36, time cost 1.205s.
Training steps per second: 177.2.
Step 91000; q_loss 0.01857; mean_q -20.8; min_q -45.65; max_q -0.301; mean_r -0.8488; mean_dsc 0.98; 
Tested 50 episodes at step 91000, reward mean -29.98, std 17.52, time cost 1.228s.
Training steps per second: 175.3.
Step 92000; q_loss 0.01795; mean_q -21.46; min_q -45.8; max_q -0.2995; mean_r -0.8435; mean_dsc 0.98; 
Tested 50 episodes at step 92000, reward mean -27.98, std 19.12, time cost 1.185s.
Training steps per second: 179.6.
Step 93000; q_loss 0.01994; mean_q -19.45; min_q -45.91; max_q -0.2968; mean_r -0.8154; mean_dsc 0.98; 
Tested 50 episodes at step 93000, reward mean -32.36, std 17.62, time cost 1.216s.
Training steps per second: 176.9.
Step 94000; q_loss 0.02991; mean_q -20.06; min_q -45.68; max_q -0.2985; mean_r -0.8008; mean_dsc 0.98; 
Tested 50 episodes at step 94000, reward mean -29.92, std 18.04, time cost 1.229s.
Training steps per second: 160.7.
Step 95000; q_loss 0.02269; mean_q -20.08; min_q -46.37; max_q -0.2767; mean_r -0.8125; mean_dsc 0.98; 
Tested 50 episodes at step 95000, reward mean -29.28, std 18.5, time cost 1.571s.
Training steps per second: 167.1.
Step 96000; q_loss 0.0151; mean_q -18.84; min_q -45.9; max_q -0.2684; mean_r -0.8126; mean_dsc 0.98; 
Tested 50 episodes at step 96000, reward mean -29.48, std 17.42, time cost 1.201s.
Training steps per second: 177.3.
Step 97000; q_loss 0.02259; mean_q -20.2; min_q -46.35; max_q -0.2594; mean_r -0.8661; mean_dsc 0.98; 
Tested 50 episodes at step 97000, reward mean -31.58, std 17.64, time cost 1.194s.
Training steps per second: 178.5.
Step 98000; q_loss 0.02561; mean_q -20.79; min_q -46.77; max_q -0.2528; mean_r -0.8331; mean_dsc 0.98; 
Tested 50 episodes at step 98000, reward mean -31, std 18.1, time cost 1.193s.
Training steps per second: 178.7.
Step 99000; q_loss 0.03082; mean_q -20.44; min_q -46.94; max_q -0.2663; mean_r -0.738; mean_dsc 0.98; 
Tested 50 episodes at step 99000, reward mean -28.78, std 17.82, time cost 1.208s.
Training steps per second: 168.1.
Step 100000; q_loss 0.05172; mean_q -22.69; min_q -47.04; max_q -0.2631; mean_r -0.9013; mean_dsc 0.98; 
Tested 50 episodes at step 100000, reward mean -25.56, std 17.37, time cost 1.241s.
Training steps per second: 176.5.
Step 101000; q_loss 0.03042; mean_q -22.18; min_q -47.01; max_q -0.2444; mean_r -0.8322; mean_dsc 0.98; 
Tested 50 episodes at step 101000, reward mean -32.84, std 18.29, time cost 1.198s.
Training steps per second: 178.5.
Step 102000; q_loss 0.01835; mean_q -19.9; min_q -47.29; max_q -0.2355; mean_r -0.8226; mean_dsc 0.98; 
Tested 50 episodes at step 102000, reward mean -33.18, std 16.73, time cost 1.231s.
Training steps per second: 176.7.
Step 103000; q_loss 0.0204; mean_q -19.9; min_q -47.16; max_q -0.2105; mean_r -0.7676; mean_dsc 0.98; 
Tested 50 episodes at step 103000, reward mean -31.24, std 16.62, time cost 1.194s.
Training steps per second: 179.1.
Step 104000; q_loss 0.02816; mean_q -21.69; min_q -47.43; max_q -0.1983; mean_r -0.8624; mean_dsc 0.98; 
Tested 50 episodes at step 104000, reward mean -30.28, std 17.54, time cost 1.227s.
Training steps per second: 177.1.
Step 105000; q_loss 0.01958; mean_q -19.21; min_q -47.43; max_q -0.1993; mean_r -0.7914; mean_dsc 0.98; 
Tested 50 episodes at step 105000, reward mean -24.52, std 18.3, time cost 1.19s.
Training steps per second: 177.6.
Step 106000; q_loss 0.01524; mean_q -19.99; min_q -47.79; max_q -0.1795; mean_r -0.7956; mean_dsc 0.98; 
Tested 50 episodes at step 106000, reward mean -33.48, std 16.55, time cost 1.199s.
Training steps per second: 170.7.
Step 107000; q_loss 0.01701; mean_q -19.56; min_q -47.91; max_q -0.18; mean_r -0.7806; mean_dsc 0.98; 
Tested 50 episodes at step 107000, reward mean -28, std 18.62, time cost 1.184s.
Training steps per second: 178.5.
Step 108000; q_loss 0.01318; mean_q -19.32; min_q -47.84; max_q -0.1584; mean_r -0.7721; mean_dsc 0.98; 
Tested 50 episodes at step 108000, reward mean -30.06, std 18.5, time cost 1.227s.
Training steps per second: 176.1.
Step 109000; q_loss 0.01452; mean_q -17.11; min_q -48.06; max_q -0.1691; mean_r -0.7694; mean_dsc 0.98; 
Tested 50 episodes at step 109000, reward mean -28.14, std 19.29, time cost 1.24s.
Training steps per second: 166.4.
Step 110000; q_loss 0.02085; mean_q -20.1; min_q -48.17; max_q -0.1689; mean_r -0.8836; mean_dsc 0.98; 
Tested 50 episodes at step 110000, reward mean -26.56, std 18.22, time cost 1.21s.
Training steps per second: 177.9.
Step 111000; q_loss 0.02071; mean_q -20.79; min_q -48.18; max_q -0.1479; mean_r -0.8656; mean_dsc 0.98; 
Tested 50 episodes at step 111000, reward mean -26.14, std 18.49, time cost 1.207s.
Training steps per second: 178.3.
Step 112000; q_loss 0.02489; mean_q -20.44; min_q -47.64; max_q -0.1432; mean_r -0.8112; mean_dsc 0.98; 
Tested 50 episodes at step 112000, reward mean -30.58, std 18.37, time cost 1.208s.
Training steps per second: 177.6.
Step 113000; q_loss 0.06736; mean_q -19.49; min_q -47.94; max_q -0.1433; mean_r -0.7814; mean_dsc 0.98; 
Tested 50 episodes at step 113000, reward mean -27.12, std 18.35, time cost 1.208s.
Training steps per second: 177.8.
Step 114000; q_loss 0.01244; mean_q -18.86; min_q -47.79; max_q -0.1415; mean_r -0.8095; mean_dsc 0.98; 
Tested 50 episodes at step 114000, reward mean -27.54, std 19.48, time cost 1.215s.
Training steps per second: 177.3.
Step 115000; q_loss 0.01071; mean_q -20.1; min_q -48.03; max_q -0.1318; mean_r -0.8611; mean_dsc 0.98; 
Tested 50 episodes at step 115000, reward mean -29.72, std 18.58, time cost 1.244s.
Training steps per second: 177.4.
Step 116000; q_loss 0.016; mean_q -17.8; min_q -47.72; max_q -0.1166; mean_r -0.779; mean_dsc 0.98; 
Tested 50 episodes at step 116000, reward mean -29.12, std 18.53, time cost 1.217s.
Training steps per second: 178.3.
Step 117000; q_loss 0.01636; mean_q -16.72; min_q -47.23; max_q -0.1305; mean_r -0.7684; mean_dsc 0.98; 
Tested 50 episodes at step 117000, reward mean -29.3, std 18.87, time cost 1.22s.
Training steps per second: 177.9.
Step 118000; q_loss 0.01239; mean_q -19.33; min_q -43.98; max_q -0.1233; mean_r -0.8364; mean_dsc 0.98; 
Tested 50 episodes at step 118000, reward mean -32.84, std 18.63, time cost 1.254s.
Training steps per second: 174.1.
Step 119000; q_loss 0.018; mean_q -19.83; min_q -45.55; max_q -0.1295; mean_r -0.7823; mean_dsc 0.98; 
Tested 50 episodes at step 119000, reward mean -30.68, std 17.96, time cost 1.214s.
Training steps per second: 168.5.
Step 120000; q_loss 0.01199; mean_q -17.27; min_q -42.61; max_q -0.1432; mean_r -0.7821; mean_dsc 0.98; 
Tested 50 episodes at step 120000, reward mean -31.3, std 19.08, time cost 1.31s.
Training steps per second: 174.2.
Step 121000; q_loss 0.01696; mean_q -16.18; min_q -46.31; max_q -0.1216; mean_r -0.7596; mean_dsc 0.98; 
Tested 50 episodes at step 121000, reward mean -30.86, std 18.9, time cost 1.196s.
Training steps per second: 177.5.
Step 122000; q_loss 0.02233; mean_q -18.68; min_q -44.05; max_q -0.1218; mean_r -0.8135; mean_dsc 0.98; 
Tested 50 episodes at step 122000, reward mean -32.54, std 18.85, time cost 1.25s.
Training steps per second: 175.3.
Step 123000; q_loss 0.01242; mean_q -17.97; min_q -45.55; max_q -0.1148; mean_r -0.7377; mean_dsc 0.98; 
Tested 50 episodes at step 123000, reward mean -31.5, std 18.94, time cost 1.232s.
Training steps per second: 174.8.
Step 124000; q_loss 0.008999; mean_q -17.66; min_q -40.27; max_q -0.1046; mean_r -0.7918; mean_dsc 0.98; 
Tested 50 episodes at step 124000, reward mean -33.98, std 18.3, time cost 1.206s.
Training steps per second: 177.8.
Step 125000; q_loss 0.008834; mean_q -19.41; min_q -42.95; max_q -0.1183; mean_r -0.817; mean_dsc 0.98; 
Tested 50 episodes at step 125000, reward mean -32.38, std 16.94, time cost 1.223s.
Training steps per second: 176.4.
Step 126000; q_loss 0.02315; mean_q -19.03; min_q -43.99; max_q -0.1207; mean_r -0.8147; mean_dsc 0.98; 
Tested 50 episodes at step 126000, reward mean -30.74, std 20.04, time cost 1.195s.
Training steps per second: 178.9.
Step 127000; q_loss 0.01026; mean_q -17.37; min_q -42.44; max_q -0.1166; mean_r -0.7659; mean_dsc 0.98; 
Tested 50 episodes at step 127000, reward mean -29.92, std 18.91, time cost 1.211s.
Training steps per second: 175.7.
Step 128000; q_loss 0.007151; mean_q -18.27; min_q -40.82; max_q -0.1109; mean_r -0.7898; mean_dsc 0.98; 
Tested 50 episodes at step 128000, reward mean -29.46, std 18.13, time cost 1.208s.
Training steps per second: 177.1.
Step 129000; q_loss 0.009453; mean_q -15.39; min_q -41.31; max_q -0.09957; mean_r -0.7233; mean_dsc 0.98; 
Tested 50 episodes at step 129000, reward mean -28.32, std 18.31, time cost 1.211s.
Training steps per second: 167.9.
Step 130000; q_loss 0.008236; mean_q -20.91; min_q -42.26; max_q -0.09555; mean_r -0.8076; mean_dsc 0.98; 
Tested 50 episodes at step 130000, reward mean -32.92, std 18.52, time cost 1.203s.
Training steps per second: 177.5.
Step 131000; q_loss 0.006751; mean_q -20.93; min_q -41.22; max_q -0.09581; mean_r -0.837; mean_dsc 0.98; 
Tested 50 episodes at step 131000, reward mean -29.66, std 19.33, time cost 1.214s.
Training steps per second: 177.8.
Step 132000; q_loss 0.008821; mean_q -19.92; min_q -41.43; max_q -0.08635; mean_r -0.8377; mean_dsc 0.98; 
Tested 50 episodes at step 132000, reward mean -30.96, std 17.8, time cost 1.237s.
Training steps per second: 176.6.
Step 133000; q_loss 0.01535; mean_q -19.76; min_q -41.97; max_q -0.07727; mean_r -0.7926; mean_dsc 0.98; 
Tested 50 episodes at step 133000, reward mean -30.82, std 18.6, time cost 1.214s.
Training steps per second: 177.4.
Step 134000; q_loss 0.01811; mean_q -18.64; min_q -42.03; max_q -0.07882; mean_r -0.75; mean_dsc 0.98; 
Tested 50 episodes at step 134000, reward mean -30.64, std 18.17, time cost 1.227s.
Training steps per second: 174.9.
Step 135000; q_loss 0.005312; mean_q -16.91; min_q -42.27; max_q -0.07313; mean_r -0.7498; mean_dsc 0.98; 
Tested 50 episodes at step 135000, reward mean -29.74, std 18.99, time cost 1.195s.
Training steps per second: 177.5.
Step 136000; q_loss 0.01341; mean_q -20.93; min_q -42.68; max_q -0.05944; mean_r -0.7748; mean_dsc 0.98; 
Tested 50 episodes at step 136000, reward mean -31.36, std 17.82, time cost 1.251s.
Training steps per second: 170.8.
Step 137000; q_loss 0.002734; mean_q -16.24; min_q -43.02; max_q -0.0462; mean_r -0.7505; mean_dsc 0.98; 
Tested 50 episodes at step 137000, reward mean -30.46, std 19.36, time cost 1.246s.
Training steps per second: 172.9.
Step 138000; q_loss 0.004768; mean_q -18.25; min_q -43.08; max_q -0.04193; mean_r -0.8018; mean_dsc 0.98; 
Tested 50 episodes at step 138000, reward mean -29.18, std 17.41, time cost 1.252s.
Training steps per second: 176.5.
Step 139000; q_loss 0.005518; mean_q -20.5; min_q -43.11; max_q -0.03323; mean_r -0.8384; mean_dsc 0.98; 
Tested 50 episodes at step 139000, reward mean -29.48, std 19.79, time cost 1.239s.
Training steps per second: 160.9.
Step 140000; q_loss 0.005053; mean_q -16.36; min_q -43.14; max_q -0.02337; mean_r -0.7425; mean_dsc 0.98; 
Tested 50 episodes at step 140000, reward mean -29.92, std 17.5, time cost 1.217s.
Training steps per second: 177.7.
Step 141000; q_loss 0.005305; mean_q -18.7; min_q -43.64; max_q -0.01909; mean_r -0.7543; mean_dsc 0.98; 
Tested 50 episodes at step 141000, reward mean -28.34, std 18.67, time cost 1.184s.
Training steps per second: 177.4.
Step 142000; q_loss 0.00576; mean_q -16.37; min_q -43.79; max_q -0.02562; mean_r -0.7457; mean_dsc 0.98; 
Tested 50 episodes at step 142000, reward mean -22.56, std 17.4, time cost 1.213s.
Training steps per second: 178.1.
Step 143000; q_loss 0.004268; mean_q -19.05; min_q -44.3; max_q -0.02096; mean_r -0.8573; mean_dsc 0.98; 
Tested 50 episodes at step 143000, reward mean -27.48, std 16.8, time cost 1.214s.
Training steps per second: 177.3.
Step 144000; q_loss 0.01524; mean_q -20.55; min_q -44.24; max_q -0.02498; mean_r -0.8953; mean_dsc 0.98; 
Tested 50 episodes at step 144000, reward mean -33.76, std 16.95, time cost 1.212s.
Training steps per second: 176.9.
Step 145000; q_loss 0.002285; mean_q -16.16; min_q -44.71; max_q -0.02186; mean_r -0.7634; mean_dsc 0.98; 
Tested 50 episodes at step 145000, reward mean -33.1, std 19.36, time cost 1.213s.
Training steps per second: 176.7.
Step 146000; q_loss 0.0103; mean_q -21.08; min_q -43.33; max_q -0.02675; mean_r -0.8336; mean_dsc 0.98; 
Tested 50 episodes at step 146000, reward mean -25.2, std 18.52, time cost 1.202s.
Training steps per second: 177.1.
Step 147000; q_loss 0.00516; mean_q -17.59; min_q -44.37; max_q -0.03035; mean_r -0.7583; mean_dsc 0.98; 
Tested 50 episodes at step 147000, reward mean -28.02, std 16.81, time cost 1.207s.
Training steps per second: 177.4.
Step 148000; q_loss 0.007127; mean_q -18.35; min_q -44.4; max_q -0.03569; mean_r -0.8155; mean_dsc 0.98; 
Tested 50 episodes at step 148000, reward mean -27.56, std 17.67, time cost 1.197s.
Training steps per second: 176.5.
Step 149000; q_loss 0.003564; mean_q -16.26; min_q -44.62; max_q -0.0419; mean_r -0.6966; mean_dsc 0.98; 
Tested 50 episodes at step 149000, reward mean -32.36, std 18.55, time cost 1.201s.
Training steps per second: 169.
Step 150000; q_loss 0.006708; mean_q -19.06; min_q -44.21; max_q -0.04076; mean_r -0.8016; mean_dsc 0.98; 
Tested 50 episodes at step 150000, reward mean -29.2, std 17.45, time cost 1.215s.
Training steps per second: 175.8.
Step 151000; q_loss 0.002094; mean_q -17.27; min_q -44.05; max_q -0.03728; mean_r -0.7267; mean_dsc 0.98; 
Tested 50 episodes at step 151000, reward mean -27.96, std 20.11, time cost 1.228s.
Training steps per second: 176.
Step 152000; q_loss 0.003856; mean_q -18.33; min_q -44.22; max_q -0.0433; mean_r -0.7693; mean_dsc 0.98; 
Tested 50 episodes at step 152000, reward mean -31.12, std 18.3, time cost 1.216s.
Training steps per second: 177.
Step 153000; q_loss 0.003094; mean_q -19.87; min_q -42.89; max_q -0.02619; mean_r -0.8237; mean_dsc 0.98; 
Tested 50 episodes at step 153000, reward mean -26.04, std 17.75, time cost 1.224s.
Training steps per second: 177.5.
Step 154000; q_loss 0.01061; mean_q -19.72; min_q -42.55; max_q -0.02395; mean_r -0.8775; mean_dsc 0.98; 
Tested 50 episodes at step 154000, reward mean -31.82, std 17.91, time cost 1.201s.
Training steps per second: 177.2.
Step 155000; q_loss 0.007143; mean_q -17.66; min_q -43.93; max_q -0.02828; mean_r -0.81; mean_dsc 0.98; 
Tested 50 episodes at step 155000, reward mean -34.04, std 18.21, time cost 1.224s.
Training steps per second: 152.5.
Step 156000; q_loss 0.004932; mean_q -21.08; min_q -44.82; max_q -0.03464; mean_r -0.8272; mean_dsc 0.98; 
Tested 50 episodes at step 156000, reward mean -28.2, std 18.68, time cost 1.21s.
Training steps per second: 177.9.
Step 157000; q_loss 0.002043; mean_q -19.46; min_q -43.87; max_q -0.03548; mean_r -0.7884; mean_dsc 0.98; 
Tested 50 episodes at step 157000, reward mean -24, std 18.13, time cost 1.217s.
Training steps per second: 177.7.
Step 158000; q_loss 0.006559; mean_q -17.6; min_q -44.14; max_q -0.04069; mean_r -0.7803; mean_dsc 0.98; 
Tested 50 episodes at step 158000, reward mean -32.2, std 17.85, time cost 1.213s.
Training steps per second: 177.5.
Step 159000; q_loss 0.001772; mean_q -16.41; min_q -43.56; max_q -0.04291; mean_r -0.7668; mean_dsc 0.98; 
Tested 50 episodes at step 159000, reward mean -24.38, std 17.85, time cost 1.198s.
Training steps per second: 170.3.
Step 160000; q_loss 0.01699; mean_q -17.36; min_q -43.6; max_q -0.03749; mean_r -0.7682; mean_dsc 0.98; 
Tested 50 episodes at step 160000, reward mean -29, std 17.92, time cost 1.234s.
Training steps per second: 177.6.
Step 161000; q_loss 0.005055; mean_q -18.19; min_q -44.44; max_q -0.02667; mean_r -0.769; mean_dsc 0.98; 
Tested 50 episodes at step 161000, reward mean -32.08, std 16.78, time cost 1.201s.
Training steps per second: 177.3.
Step 162000; q_loss 0.006382; mean_q -19.12; min_q -44.4; max_q -0.01915; mean_r -0.8255; mean_dsc 0.98; 
Tested 50 episodes at step 162000, reward mean -30.58, std 18.23, time cost 1.212s.
Training steps per second: 176.
Step 163000; q_loss 0.006351; mean_q -15.32; min_q -42.68; max_q -0.01766; mean_r -0.7349; mean_dsc 0.98; 
Tested 50 episodes at step 163000, reward mean -26.56, std 17.37, time cost 1.214s.
Training steps per second: 176.2.
Step 164000; q_loss 0.001935; mean_q -18.91; min_q -43.67; max_q -0.03025; mean_r -0.7767; mean_dsc 0.98; 
Tested 50 episodes at step 164000, reward mean -32.58, std 17.73, time cost 1.198s.
Training steps per second: 174.7.
Step 165000; q_loss 0.004244; mean_q -16.02; min_q -42.68; max_q -0.03708; mean_r -0.7571; mean_dsc 0.98; 
Tested 50 episodes at step 165000, reward mean -29.76, std 20.4, time cost 1.236s.
Training steps per second: 174.7.
Step 166000; q_loss 0.002728; mean_q -17.31; min_q -43.72; max_q -0.03448; mean_r -0.7694; mean_dsc 0.98; 
Tested 50 episodes at step 166000, reward mean -32.9, std 17.54, time cost 1.249s.
Training steps per second: 176.4.
Step 167000; q_loss 0.001849; mean_q -17.92; min_q -42.61; max_q -0.03552; mean_r -0.8308; mean_dsc 0.98; 
Tested 50 episodes at step 167000, reward mean -31.08, std 18.73, time cost 1.21s.
Training steps per second: 177.6.
Step 168000; q_loss 0.002338; mean_q -17.93; min_q -43.33; max_q -0.03574; mean_r -0.7948; mean_dsc 0.98; 
Tested 50 episodes at step 168000, reward mean -31, std 18.41, time cost 1.186s.
Training steps per second: 179.3.
Step 169000; q_loss 0.003186; mean_q -17.64; min_q -43.38; max_q -0.02948; mean_r -0.8055; mean_dsc 0.98; 
Tested 50 episodes at step 169000, reward mean -27.06, std 18.94, time cost 1.26s.
Training steps per second: 168.8.
Step 170000; q_loss 0.007045; mean_q -19.03; min_q -43.94; max_q -0.03184; mean_r -0.7992; mean_dsc 0.98; 
Tested 50 episodes at step 170000, reward mean -28.68, std 18.9, time cost 1.204s.
Training steps per second: 177.3.
Step 171000; q_loss 0.007421; mean_q -19.51; min_q -43.64; max_q -0.03073; mean_r -0.8368; mean_dsc 0.98; 
Tested 50 episodes at step 171000, reward mean -29.58, std 17.06, time cost 1.195s.
Training steps per second: 177.8.
Step 172000; q_loss 0.003053; mean_q -18.71; min_q -44.57; max_q -0.03455; mean_r -0.783; mean_dsc 0.98; 
Tested 50 episodes at step 172000, reward mean -26.74, std 17.9, time cost 1.21s.
Training steps per second: 177.6.
Step 173000; q_loss 0.001941; mean_q -16.33; min_q -42.4; max_q -0.03774; mean_r -0.777; mean_dsc 0.98; 
Tested 50 episodes at step 173000, reward mean -30.26, std 19.39, time cost 1.214s.
Training steps per second: 175.9.
Step 174000; q_loss 0.004543; mean_q -17.53; min_q -44.55; max_q -0.03491; mean_r -0.7967; mean_dsc 0.98; 
Tested 50 episodes at step 174000, reward mean -37.16, std 17.73, time cost 1.231s.
Training steps per second: 176.7.
Step 175000; q_loss 0.004324; mean_q -16.03; min_q -44.5; max_q -0.03332; mean_r -0.7187; mean_dsc 0.98; 
Tested 50 episodes at step 175000, reward mean -28.52, std 18.84, time cost 1.205s.
Training steps per second: 178.2.
Step 176000; q_loss 0.003859; mean_q -20.85; min_q -42.62; max_q -0.03112; mean_r -0.8592; mean_dsc 0.98; 
Tested 50 episodes at step 176000, reward mean -32.42, std 17.47, time cost 1.224s.
Training steps per second: 176.
Step 177000; q_loss 0.004425; mean_q -17.42; min_q -43.6; max_q -0.0268; mean_r -0.7712; mean_dsc 0.98; 
Tested 50 episodes at step 177000, reward mean -27.8, std 20.76, time cost 1.202s.
Training steps per second: 177.8.
Step 178000; q_loss 0.003876; mean_q -14; min_q -42.99; max_q -0.02345; mean_r -0.6364; mean_dsc 0.98; 
Tested 50 episodes at step 178000, reward mean -30.5, std 18.01, time cost 1.201s.
Training steps per second: 177.1.
Step 179000; q_loss 0.002773; mean_q -15.06; min_q -42.45; max_q -0.02216; mean_r -0.7336; mean_dsc 0.98; 
Tested 50 episodes at step 179000, reward mean -32.74, std 16.98, time cost 1.219s.
Training steps per second: 166.1.
Step 180000; q_loss 0.006129; mean_q -18.65; min_q -43.62; max_q -0.01918; mean_r -0.8358; mean_dsc 0.98; 
Tested 50 episodes at step 180000, reward mean -36.78, std 15.26, time cost 1.223s.
Training steps per second: 165.7.
Step 181000; q_loss 0.006263; mean_q -17.01; min_q -43.35; max_q -0.01348; mean_r -0.7784; mean_dsc 0.98; 
Tested 50 episodes at step 181000, reward mean -28.02, std 17.79, time cost 1.22s.
Training steps per second: 176.7.
Step 182000; q_loss 0.0009269; mean_q -18.89; min_q -43.91; max_q -0.02668; mean_r -0.8041; mean_dsc 0.98; 
Tested 50 episodes at step 182000, reward mean -26.8, std 18.66, time cost 1.206s.
Training steps per second: 175.1.
Step 183000; q_loss 0.002433; mean_q -19.27; min_q -43.66; max_q -0.02632; mean_r -0.809; mean_dsc 0.98; 
Tested 50 episodes at step 183000, reward mean -27.54, std 18.05, time cost 1.205s.
Training steps per second: 178.
Step 184000; q_loss 0.001921; mean_q -18.26; min_q -44.53; max_q -0.02256; mean_r -0.8085; mean_dsc 0.98; 
Tested 50 episodes at step 184000, reward mean -31.8, std 17.04, time cost 1.211s.
Training steps per second: 177.5.
Step 185000; q_loss 0.001738; mean_q -17.2; min_q -43.36; max_q -0.02126; mean_r -0.8061; mean_dsc 0.98; 
Tested 50 episodes at step 185000, reward mean -34.2, std 17.17, time cost 1.205s.
Training steps per second: 177.7.
Step 186000; q_loss 0.002607; mean_q -19.11; min_q -42.63; max_q -0.02249; mean_r -0.8688; mean_dsc 0.98; 
Tested 50 episodes at step 186000, reward mean -29.76, std 18.18, time cost 1.217s.
Training steps per second: 177.5.
Step 187000; q_loss 0.002501; mean_q -17.34; min_q -43.26; max_q -0.02088; mean_r -0.7624; mean_dsc 0.98; 
Tested 50 episodes at step 187000, reward mean -24.86, std 19.28, time cost 1.21s.
Training steps per second: 177.7.
Step 188000; q_loss 0.001554; mean_q -17.61; min_q -43.36; max_q -0.02352; mean_r -0.7601; mean_dsc 0.98; 
Tested 50 episodes at step 188000, reward mean -27.06, std 19.36, time cost 1.206s.
Training steps per second: 177.7.
Step 189000; q_loss 0.004552; mean_q -17.38; min_q -42.6; max_q -0.02592; mean_r -0.749; mean_dsc 0.98; 
Tested 50 episodes at step 189000, reward mean -24.58, std 18.95, time cost 1.201s.
Training steps per second: 168.6.
Step 190000; q_loss 0.003019; mean_q -17.1; min_q -43.33; max_q -0.0216; mean_r -0.8234; mean_dsc 0.98; 
Tested 50 episodes at step 190000, reward mean -30.22, std 17.65, time cost 1.208s.
Training steps per second: 174.9.
Step 191000; q_loss 0.002318; mean_q -19.46; min_q -43.34; max_q -0.009117; mean_r -0.8251; mean_dsc 0.98; 
Tested 50 episodes at step 191000, reward mean -24.24, std 17.56, time cost 1.237s.
Training steps per second: 174.1.
Step 192000; q_loss 0.0006145; mean_q -16.77; min_q -44.58; max_q -0.005247; mean_r -0.713; mean_dsc 0.98; 
Tested 50 episodes at step 192000, reward mean -24.02, std 17.85, time cost 1.221s.
Training steps per second: 175.
Step 193000; q_loss 0.001148; mean_q -16.6; min_q -43.35; max_q -0.003627; mean_r -0.7439; mean_dsc 0.98; 
Tested 50 episodes at step 193000, reward mean -32.54, std 18.13, time cost 1.236s.
Training steps per second: 175.1.
Step 194000; q_loss 0.0008457; mean_q -16.17; min_q -43.35; max_q -0.003167; mean_r -0.779; mean_dsc 0.98; 
Tested 50 episodes at step 194000, reward mean -35.2, std 17.28, time cost 1.22s.
Training steps per second: 176.7.
Step 195000; q_loss 0.001674; mean_q -17.59; min_q -43.3; max_q 0.001771; mean_r -0.803; mean_dsc 0.98; 
Tested 50 episodes at step 195000, reward mean -30.54, std 19.29, time cost 1.2s.
Training steps per second: 177.3.
Step 196000; q_loss 0.009429; mean_q -16.84; min_q -43.3; max_q 0.003129; mean_r -0.7589; mean_dsc 0.98; 
Tested 50 episodes at step 196000, reward mean -32.78, std 18.85, time cost 1.201s.
Training steps per second: 177.
Step 197000; q_loss 0.002802; mean_q -17.6; min_q -44.17; max_q 0.008288; mean_r -0.8305; mean_dsc 0.98; 
Tested 50 episodes at step 197000, reward mean -31.2, std 18, time cost 1.216s.
Training steps per second: 177.6.
Step 198000; q_loss 0.002721; mean_q -16.18; min_q -43.02; max_q 0.001873; mean_r -0.7588; mean_dsc 0.98; 
Tested 50 episodes at step 198000, reward mean -25.38, std 19.46, time cost 1.221s.
Training steps per second: 177.2.
Step 199000; q_loss 0.006283; mean_q -18.39; min_q -44.06; max_q 0.001576; mean_r -0.803; mean_dsc 0.98; 
Tested 50 episodes at step 199000, reward mean -27.3, std 17.47, time cost 1.206s.
Training steps per second: 167.8.
Step 200000; q_loss 0.002051; mean_q -17.81; min_q -43.06; max_q 0.004168; mean_r -0.7925; mean_dsc 0.98; 
Tested 50 episodes at step 200000, reward mean -27.68, std 18.06, time cost 1.223s.
