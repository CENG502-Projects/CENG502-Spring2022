device: cuda.
Representation model loaded from /content/drive/MyDrive/CEng502-AdvancedDeepLearning/ExperimentCodes/Wu_GridRoom_Run1/laplacian_code_Wu/log/laprepr/HardMaze/test/model.ckpt.
Start collecting transitions.
(10000/10000) steps collected.
Replay buffer initialization finished, time cost: 1.6042299270629883s
Training steps per second: 0.
Step 1; q_loss 1.026; mean_q -0.9572; min_q -1.119; max_q -0.4312; mean_r -1.04; mean_dsc 0.98; 
Tested 50 episodes at step 1, reward mean -50, std 0, time cost 1.324s.
Training steps per second: 163.
Step 1000; q_loss 0.004283; mean_q -1.557; min_q -1.814; max_q -0.3521; mean_r -1.021; mean_dsc 0.98; 
Tested 50 episodes at step 1000, reward mean -49.98, std 0.14, time cost 1.322s.
Training steps per second: 159.8.
Step 2000; q_loss 0.006391; mean_q -2.282; min_q -2.706; max_q -0.6358; mean_r -1.014; mean_dsc 0.98; 
Tested 50 episodes at step 2000, reward mean -49.96, std 0.196, time cost 1.323s.
Training steps per second: 162.8.
Step 3000; q_loss 0.003423; mean_q -3.056; min_q -3.61; max_q -1.527; mean_r -1.017; mean_dsc 0.98; 
Tested 50 episodes at step 3000, reward mean -49.96, std 0.196, time cost 1.287s.
Training steps per second: 162.8.
Step 4000; q_loss 0.004842; mean_q -3.898; min_q -4.623; max_q -1.842; mean_r -1.025; mean_dsc 0.98; 
Tested 50 episodes at step 4000, reward mean -49.94, std 0.2375, time cost 1.294s.
Training steps per second: 161.1.
Step 5000; q_loss 0.009131; mean_q -4.752; min_q -5.602; max_q -2.17; mean_r -1.032; mean_dsc 0.98; 
Tested 50 episodes at step 5000, reward mean -39.54, std 19.72, time cost 1.315s.
Training steps per second: 162.9.
Step 6000; q_loss 0.01037; mean_q -5.635; min_q -6.576; max_q -2.68; mean_r -1.034; mean_dsc 0.98; 
Tested 50 episodes at step 6000, reward mean -46.06, std 13.36, time cost 1.307s.
Training steps per second: 162.5.
Step 7000; q_loss 0.01115; mean_q -6.433; min_q -7.501; max_q -2.865; mean_r -1.034; mean_dsc 0.98; 
Tested 50 episodes at step 7000, reward mean -45.62, std 13.17, time cost 1.327s.
Training steps per second: 160.5.
Step 8000; q_loss 0.01167; mean_q -6.87; min_q -8.436; max_q -2.488; mean_r -0.976; mean_dsc 0.98; 
Tested 50 episodes at step 8000, reward mean -42.7, std 16.79, time cost 1.297s.
Training steps per second: 155.6.
Step 9000; q_loss 0.01079; mean_q -7.808; min_q -9.329; max_q -2.585; mean_r -0.9897; mean_dsc 0.98; 
Tested 50 episodes at step 9000, reward mean -40.7, std 18.64, time cost 1.318s.
Training steps per second: 141.8.
Step 10000; q_loss 0.01648; mean_q -8.47; min_q -10.13; max_q -2.598; mean_r -0.9979; mean_dsc 0.98; 
Tested 50 episodes at step 10000, reward mean -42.58, std 17.02, time cost 1.312s.
Training steps per second: 158.2.
Step 11000; q_loss 0.02387; mean_q -9.538; min_q -10.97; max_q -2.562; mean_r -1.031; mean_dsc 0.98; 
Tested 50 episodes at step 11000, reward mean -40.4, std 18.23, time cost 1.266s.
Training steps per second: 160.3.
Step 12000; q_loss 0.01487; mean_q -9.908; min_q -11.59; max_q -2.49; mean_r -0.9983; mean_dsc 0.98; 
Tested 50 episodes at step 12000, reward mean -35.14, std 20.96, time cost 1.325s.
Training steps per second: 159.2.
Step 13000; q_loss 0.01353; mean_q -10.34; min_q -12.5; max_q -2.404; mean_r -0.9821; mean_dsc 0.98; 
Tested 50 episodes at step 13000, reward mean -27.7, std 22.45, time cost 1.272s.
Training steps per second: 160.7.
Step 14000; q_loss 0.02745; mean_q -10.72; min_q -13.45; max_q -2.324; mean_r -0.9574; mean_dsc 0.98; 
Tested 50 episodes at step 14000, reward mean -38.96, std 18.75, time cost 1.326s.
Training steps per second: 157.3.
Step 15000; q_loss 0.02025; mean_q -11.04; min_q -14.25; max_q -2.275; mean_r -0.9467; mean_dsc 0.98; 
Tested 50 episodes at step 15000, reward mean -36.78, std 20.31, time cost 1.319s.
Training steps per second: 159.5.
Step 16000; q_loss 0.04031; mean_q -12.63; min_q -14.95; max_q -2.216; mean_r -1.005; mean_dsc 0.98; 
Tested 50 episodes at step 16000, reward mean -40.92, std 17.19, time cost 1.295s.
Training steps per second: 161.
Step 17000; q_loss 0.04495; mean_q -11.68; min_q -15.64; max_q -2.154; mean_r -0.9217; mean_dsc 0.98; 
Tested 50 episodes at step 17000, reward mean -32.64, std 21.41, time cost 1.28s.
Training steps per second: 163.2.
Step 18000; q_loss 0.0288; mean_q -12.5; min_q -16.56; max_q -2.098; mean_r -0.9278; mean_dsc 0.98; 
Tested 50 episodes at step 18000, reward mean -40.18, std 18.55, time cost 1.273s.
Training steps per second: 162.
Step 19000; q_loss 0.04363; mean_q -12.1; min_q -17.08; max_q -2.011; mean_r -0.8765; mean_dsc 0.98; 
Tested 50 episodes at step 19000, reward mean -34.92, std 21.2, time cost 1.344s.
Training steps per second: 148.8.
Step 20000; q_loss 0.0313; mean_q -13.83; min_q -17.79; max_q -1.963; mean_r -0.9583; mean_dsc 0.98; 
Tested 50 episodes at step 20000, reward mean -38.16, std 19.16, time cost 1.369s.
Training steps per second: 156.2.
Step 21000; q_loss 0.03134; mean_q -13.45; min_q -18.5; max_q -1.905; mean_r -0.9016; mean_dsc 0.98; 
Tested 50 episodes at step 21000, reward mean -37.66, std 19.95, time cost 1.328s.
Training steps per second: 159.
Step 22000; q_loss 0.02755; mean_q -14.27; min_q -19.14; max_q -1.829; mean_r -0.9076; mean_dsc 0.98; 
Tested 50 episodes at step 22000, reward mean -36.82, std 20.39, time cost 1.31s.
Training steps per second: 160.
Step 23000; q_loss 0.0478; mean_q -14.22; min_q -19.79; max_q -1.801; mean_r -0.9038; mean_dsc 0.98; 
Tested 50 episodes at step 23000, reward mean -38.6, std 18.39, time cost 1.301s.
Training steps per second: 159.8.
Step 24000; q_loss 0.02537; mean_q -15.13; min_q -20.43; max_q -1.751; mean_r -0.9412; mean_dsc 0.98; 
Tested 50 episodes at step 24000, reward mean -35.32, std 20.75, time cost 1.322s.
Training steps per second: 157.6.
Step 25000; q_loss 0.02734; mean_q -15.92; min_q -21.04; max_q -1.68; mean_r -0.933; mean_dsc 0.98; 
Tested 50 episodes at step 25000, reward mean -31.08, std 21.62, time cost 1.303s.
Training steps per second: 160.4.
Step 26000; q_loss 0.07083; mean_q -14.72; min_q -21.54; max_q -1.639; mean_r -0.8789; mean_dsc 0.98; 
Tested 50 episodes at step 26000, reward mean -35.48, std 19.85, time cost 1.324s.
Training steps per second: 152.7.
Step 27000; q_loss 0.03986; mean_q -15.83; min_q -21.88; max_q -1.6; mean_r -0.9368; mean_dsc 0.98; 
Tested 50 episodes at step 27000, reward mean -39.24, std 19.28, time cost 1.867s.
Training steps per second: 143.2.
Step 28000; q_loss 0.03152; mean_q -15.16; min_q -22.73; max_q -1.568; mean_r -0.883; mean_dsc 0.98; 
Tested 50 episodes at step 28000, reward mean -28.6, std 20.88, time cost 1.328s.
Training steps per second: 156.5.
Step 29000; q_loss 0.01911; mean_q -17.76; min_q -23.31; max_q -1.542; mean_r -0.9494; mean_dsc 0.98; 
Tested 50 episodes at step 29000, reward mean -30.88, std 21.16, time cost 1.313s.
Training steps per second: 146.3.
Step 30000; q_loss 0.0201; mean_q -17.08; min_q -23.81; max_q -1.515; mean_r -0.9269; mean_dsc 0.98; 
Tested 50 episodes at step 30000, reward mean -32.16, std 21.26, time cost 1.306s.
Training steps per second: 159.5.
Step 31000; q_loss 0.05188; mean_q -16.71; min_q -24.41; max_q -1.469; mean_r -0.9239; mean_dsc 0.98; 
Tested 50 episodes at step 31000, reward mean -32.16, std 20.56, time cost 1.298s.
Training steps per second: 158.4.
Step 32000; q_loss 0.03658; mean_q -16.61; min_q -25.24; max_q -1.417; mean_r -0.9109; mean_dsc 0.98; 
Tested 50 episodes at step 32000, reward mean -32.92, std 18.94, time cost 1.318s.
Training steps per second: 159.4.
Step 33000; q_loss 0.01694; mean_q -16.45; min_q -25.53; max_q -1.385; mean_r -0.8502; mean_dsc 0.98; 
Tested 50 episodes at step 33000, reward mean -29.66, std 20.88, time cost 1.294s.
Training steps per second: 159.2.
Step 34000; q_loss 0.02489; mean_q -16.99; min_q -26.2; max_q -1.353; mean_r -0.8766; mean_dsc 0.98; 
Tested 50 episodes at step 34000, reward mean -32.98, std 21.21, time cost 1.321s.
Training steps per second: 158.1.
Step 35000; q_loss 0.0118; mean_q -18.23; min_q -26.84; max_q -1.32; mean_r -0.8624; mean_dsc 0.98; 
Tested 50 episodes at step 35000, reward mean -28.6, std 19.74, time cost 1.323s.
Training steps per second: 158.4.
Step 36000; q_loss 0.02048; mean_q -16.68; min_q -27.35; max_q -1.284; mean_r -0.8675; mean_dsc 0.98; 
Tested 50 episodes at step 36000, reward mean -31.08, std 20.31, time cost 1.313s.
Training steps per second: 157.8.
Step 37000; q_loss 0.02752; mean_q -18.11; min_q -27.35; max_q -1.252; mean_r -0.8788; mean_dsc 0.98; 
Tested 50 episodes at step 37000, reward mean -31.72, std 20.54, time cost 1.3s.
Training steps per second: 157.1.
Step 38000; q_loss 0.1279; mean_q -16.44; min_q -27.72; max_q -1.222; mean_r -0.8022; mean_dsc 0.98; 
Tested 50 episodes at step 38000, reward mean -32.2, std 19.95, time cost 1.298s.
Training steps per second: 157.9.
Step 39000; q_loss 0.01905; mean_q -17.77; min_q -28.17; max_q -1.197; mean_r -0.857; mean_dsc 0.98; 
Tested 50 episodes at step 39000, reward mean -33.92, std 19.53, time cost 1.318s.
Training steps per second: 156.7.
Step 40000; q_loss 0.02431; mean_q -17.57; min_q -28.4; max_q -1.18; mean_r -0.8376; mean_dsc 0.98; 
Tested 50 episodes at step 40000, reward mean -29.02, std 19.99, time cost 1.346s.
Training steps per second: 155.2.
Step 41000; q_loss 0.03712; mean_q -17.77; min_q -29.42; max_q -1.155; mean_r -0.8117; mean_dsc 0.98; 
Tested 50 episodes at step 41000, reward mean -31.24, std 19.87, time cost 1.316s.
Training steps per second: 158.7.
Step 42000; q_loss 0.01929; mean_q -19.61; min_q -29.85; max_q -1.134; mean_r -0.8986; mean_dsc 0.98; 
Tested 50 episodes at step 42000, reward mean -33.44, std 20.34, time cost 1.283s.
Training steps per second: 160.1.
Step 43000; q_loss 0.04427; mean_q -18.97; min_q -30.23; max_q -1.097; mean_r -0.8099; mean_dsc 0.98; 
Tested 50 episodes at step 43000, reward mean -34.66, std 19.65, time cost 1.317s.
Training steps per second: 156.1.
Step 44000; q_loss 0.03925; mean_q -20.95; min_q -30.5; max_q -1.072; mean_r -0.9033; mean_dsc 0.98; 
Tested 50 episodes at step 44000, reward mean -27.54, std 21.38, time cost 1.31s.
Training steps per second: 153.9.
Step 45000; q_loss 0.02454; mean_q -17.34; min_q -30.77; max_q -1.043; mean_r -0.8301; mean_dsc 0.98; 
Tested 50 episodes at step 45000, reward mean -34.76, std 19.77, time cost 1.311s.
Training steps per second: 158.
Step 46000; q_loss 0.0324; mean_q -20.29; min_q -31.58; max_q -1.025; mean_r -0.8347; mean_dsc 0.98; 
Tested 50 episodes at step 46000, reward mean -34.48, std 20.96, time cost 1.287s.
Training steps per second: 158.
Step 47000; q_loss 0.02563; mean_q -17.72; min_q -31.41; max_q -0.999; mean_r -0.7536; mean_dsc 0.98; 
Tested 50 episodes at step 47000, reward mean -33.22, std 20.8, time cost 1.355s.
Training steps per second: 157.3.
Step 48000; q_loss 0.03549; mean_q -19.57; min_q -31.9; max_q -0.9718; mean_r -0.8627; mean_dsc 0.98; 
Tested 50 episodes at step 48000, reward mean -37.58, std 19.28, time cost 1.348s.
Training steps per second: 155.8.
Step 49000; q_loss 0.02283; mean_q -17.45; min_q -32.3; max_q -0.9482; mean_r -0.7616; mean_dsc 0.98; 
Tested 50 episodes at step 49000, reward mean -32.44, std 20.96, time cost 1.324s.
Training steps per second: 158.1.
Step 50000; q_loss 0.02094; mean_q -18.44; min_q -32.71; max_q -0.9243; mean_r -0.8052; mean_dsc 0.98; 
Tested 50 episodes at step 50000, reward mean -29.76, std 21.45, time cost 1.346s.
Training steps per second: 155.3.
Step 51000; q_loss 0.0468; mean_q -19.87; min_q -33.11; max_q -0.9038; mean_r -0.8368; mean_dsc 0.98; 
Tested 50 episodes at step 51000, reward mean -33.52, std 21.3, time cost 1.325s.
Training steps per second: 157.
Step 52000; q_loss 0.02631; mean_q -20.85; min_q -33.41; max_q -0.8921; mean_r -0.8443; mean_dsc 0.98; 
Tested 50 episodes at step 52000, reward mean -34.14, std 19.68, time cost 1.3s.
Training steps per second: 158.9.
Step 53000; q_loss 0.02149; mean_q -18.89; min_q -34.01; max_q -0.8756; mean_r -0.7752; mean_dsc 0.98; 
Tested 50 episodes at step 53000, reward mean -37.6, std 18.52, time cost 1.323s.
Training steps per second: 158.
Step 54000; q_loss 0.03582; mean_q -19.49; min_q -34.08; max_q -0.8516; mean_r -0.8086; mean_dsc 0.98; 
Tested 50 episodes at step 54000, reward mean -32.72, std 20.34, time cost 1.331s.
Training steps per second: 159.1.
Step 55000; q_loss 0.0435; mean_q -22.95; min_q -34.58; max_q -0.831; mean_r -0.9166; mean_dsc 0.98; 
Tested 50 episodes at step 55000, reward mean -37.08, std 17.68, time cost 1.298s.
Training steps per second: 155.7.
Step 56000; q_loss 0.03514; mean_q -21.2; min_q -35.04; max_q -0.815; mean_r -0.838; mean_dsc 0.98; 
Tested 50 episodes at step 56000, reward mean -30.46, std 19.57, time cost 1.284s.
Training steps per second: 159.2.
Step 57000; q_loss 0.02836; mean_q -21.57; min_q -35.3; max_q -0.7922; mean_r -0.8722; mean_dsc 0.98; 
Tested 50 episodes at step 57000, reward mean -32.64, std 19.3, time cost 1.3s.
Training steps per second: 157.2.
Step 58000; q_loss 0.02266; mean_q -18.39; min_q -35.44; max_q -0.7699; mean_r -0.7311; mean_dsc 0.98; 
Tested 50 episodes at step 58000, reward mean -28.92, std 18.97, time cost 1.279s.
Training steps per second: 158.9.
Step 59000; q_loss 0.02468; mean_q -21.79; min_q -35.81; max_q -0.7606; mean_r -0.8429; mean_dsc 0.98; 
Tested 50 episodes at step 59000, reward mean -29.42, std 18.7, time cost 1.341s.
Training steps per second: 158.2.
Step 60000; q_loss 0.01667; mean_q -18.54; min_q -36.19; max_q -0.7392; mean_r -0.7369; mean_dsc 0.98; 
Tested 50 episodes at step 60000, reward mean -34.04, std 19.07, time cost 1.292s.
Training steps per second: 156.6.
Step 61000; q_loss 0.03184; mean_q -20.95; min_q -36.84; max_q -0.7228; mean_r -0.7901; mean_dsc 0.98; 
Tested 50 episodes at step 61000, reward mean -34.52, std 18.72, time cost 1.298s.
Training steps per second: 159.3.
Step 62000; q_loss 0.02253; mean_q -17.88; min_q -36.97; max_q -0.7034; mean_r -0.7585; mean_dsc 0.98; 
Tested 50 episodes at step 62000, reward mean -33.3, std 18.61, time cost 1.256s.
Training steps per second: 159.1.
Step 63000; q_loss 0.03069; mean_q -21.97; min_q -37.26; max_q -0.6921; mean_r -0.8366; mean_dsc 0.98; 
Tested 50 episodes at step 63000, reward mean -28.58, std 19.66, time cost 1.242s.
Training steps per second: 160.
Step 64000; q_loss 0.0269; mean_q -22.19; min_q -37.71; max_q -0.6833; mean_r -0.8364; mean_dsc 0.98; 
Tested 50 episodes at step 64000, reward mean -27.04, std 19.63, time cost 1.291s.
Training steps per second: 159.6.
Step 65000; q_loss 0.03277; mean_q -21.25; min_q -37.86; max_q -0.6703; mean_r -0.8393; mean_dsc 0.98; 
Tested 50 episodes at step 65000, reward mean -32.06, std 18.1, time cost 1.283s.
Training steps per second: 159.8.
Step 66000; q_loss 0.01651; mean_q -24.34; min_q -38.32; max_q -0.6518; mean_r -0.8392; mean_dsc 0.98; 
Tested 50 episodes at step 66000, reward mean -30.52, std 18.47, time cost 1.359s.
Training steps per second: 158.
Step 67000; q_loss 0.04493; mean_q -18.63; min_q -38.7; max_q -0.6379; mean_r -0.7431; mean_dsc 0.98; 
Tested 50 episodes at step 67000, reward mean -33.52, std 17.3, time cost 1.297s.
Training steps per second: 159.1.
Step 68000; q_loss 0.03715; mean_q -22.59; min_q -38.84; max_q -0.6223; mean_r -0.799; mean_dsc 0.98; 
Tested 50 episodes at step 68000, reward mean -29.44, std 17.79, time cost 1.283s.
Training steps per second: 160.3.
Step 69000; q_loss 0.01901; mean_q -19.18; min_q -38.94; max_q -0.6109; mean_r -0.7409; mean_dsc 0.98; 
Tested 50 episodes at step 69000, reward mean -29.6, std 19.45, time cost 1.29s.
Training steps per second: 159.
Step 70000; q_loss 0.01714; mean_q -20.79; min_q -39.16; max_q -0.5957; mean_r -0.8198; mean_dsc 0.98; 
Tested 50 episodes at step 70000, reward mean -25.84, std 18.78, time cost 1.288s.
Training steps per second: 160.6.
Step 71000; q_loss 0.01343; mean_q -20.32; min_q -39.48; max_q -0.5806; mean_r -0.7607; mean_dsc 0.98; 
Tested 50 episodes at step 71000, reward mean -26.74, std 19.87, time cost 1.301s.
Training steps per second: 159.6.
Step 72000; q_loss 0.05375; mean_q -20.36; min_q -39.54; max_q -0.5673; mean_r -0.7737; mean_dsc 0.98; 
Tested 50 episodes at step 72000, reward mean -28.14, std 19.62, time cost 1.286s.
Training steps per second: 158.6.
Step 73000; q_loss 0.01096; mean_q -23.46; min_q -39.87; max_q -0.5596; mean_r -0.8768; mean_dsc 0.98; 
Tested 50 episodes at step 73000, reward mean -35.34, std 16.59, time cost 1.305s.
Training steps per second: 160.9.
Step 74000; q_loss 0.01957; mean_q -20.02; min_q -40.18; max_q -0.545; mean_r -0.719; mean_dsc 0.98; 
Tested 50 episodes at step 74000, reward mean -27.02, std 18.48, time cost 1.279s.
Training steps per second: 160.3.
Step 75000; q_loss 0.02529; mean_q -22.22; min_q -40.56; max_q -0.5333; mean_r -0.8495; mean_dsc 0.98; 
Tested 50 episodes at step 75000, reward mean -30.5, std 19.74, time cost 1.308s.
Training steps per second: 158.7.
Step 76000; q_loss 0.03524; mean_q -22; min_q -40.78; max_q -0.5134; mean_r -0.8196; mean_dsc 0.98; 
Tested 50 episodes at step 76000, reward mean -29.64, std 20.2, time cost 1.286s.
Training steps per second: 160.
Step 77000; q_loss 0.0236; mean_q -21.09; min_q -41.03; max_q -0.4942; mean_r -0.8025; mean_dsc 0.98; 
Tested 50 episodes at step 77000, reward mean -31.64, std 19.67, time cost 1.309s.
Training steps per second: 138.5.
Step 78000; q_loss 0.04549; mean_q -19.6; min_q -41.13; max_q -0.4786; mean_r -0.7616; mean_dsc 0.98; 
Tested 50 episodes at step 78000, reward mean -22.72, std 18.77, time cost 1.297s.
Training steps per second: 159.3.
Step 79000; q_loss 0.05621; mean_q -17.52; min_q -41.28; max_q -0.4666; mean_r -0.7148; mean_dsc 0.98; 
Tested 50 episodes at step 79000, reward mean -25.1, std 18.99, time cost 1.303s.
Training steps per second: 158.6.
Step 80000; q_loss 0.03098; mean_q -19.8; min_q -41.35; max_q -0.4474; mean_r -0.7585; mean_dsc 0.98; 
Tested 50 episodes at step 80000, reward mean -24.6, std 20.1, time cost 1.302s.
Training steps per second: 159.1.
Step 81000; q_loss 0.05706; mean_q -23.44; min_q -41.7; max_q -0.434; mean_r -0.8149; mean_dsc 0.98; 
Tested 50 episodes at step 81000, reward mean -28.52, std 19.44, time cost 1.281s.
Training steps per second: 155.7.
Step 82000; q_loss 0.05517; mean_q -23.58; min_q -42.15; max_q -0.4202; mean_r -0.8446; mean_dsc 0.98; 
Tested 50 episodes at step 82000, reward mean -24.82, std 20.33, time cost 1.321s.
Training steps per second: 157.5.
Step 83000; q_loss 0.01833; mean_q -19.39; min_q -42.48; max_q -0.4141; mean_r -0.7578; mean_dsc 0.98; 
Tested 50 episodes at step 83000, reward mean -26.34, std 19.13, time cost 1.32s.
Training steps per second: 159.6.
Step 84000; q_loss 0.03061; mean_q -18.51; min_q -42.13; max_q -0.4082; mean_r -0.791; mean_dsc 0.98; 
Tested 50 episodes at step 84000, reward mean -28.36, std 18.53, time cost 1.288s.
Training steps per second: 160.5.
Step 85000; q_loss 0.02969; mean_q -20.77; min_q -42.53; max_q -0.4066; mean_r -0.7855; mean_dsc 0.98; 
Tested 50 episodes at step 85000, reward mean -29.12, std 19.85, time cost 1.34s.
Training steps per second: 160.
Step 86000; q_loss 0.03329; mean_q -22.1; min_q -42.56; max_q -0.3948; mean_r -0.8554; mean_dsc 0.98; 
Tested 50 episodes at step 86000, reward mean -29.12, std 18.8, time cost 1.273s.
Training steps per second: 159.5.
Step 87000; q_loss 0.02402; mean_q -22.57; min_q -43.31; max_q -0.3836; mean_r -0.8298; mean_dsc 0.98; 
Tested 50 episodes at step 87000, reward mean -29.62, std 18.8, time cost 1.339s.
Training steps per second: 158.9.
Step 88000; q_loss 0.02519; mean_q -21.15; min_q -43.37; max_q -0.375; mean_r -0.7992; mean_dsc 0.98; 
Tested 50 episodes at step 88000, reward mean -32.28, std 19.31, time cost 1.306s.
Training steps per second: 158.2.
Step 89000; q_loss 0.103; mean_q -21.72; min_q -43.41; max_q -0.3647; mean_r -0.8331; mean_dsc 0.98; 
Tested 50 episodes at step 89000, reward mean -22.56, std 18.44, time cost 1.275s.
Training steps per second: 160.4.
Step 90000; q_loss 0.01805; mean_q -18.37; min_q -43.45; max_q -0.3556; mean_r -0.7516; mean_dsc 0.98; 
Tested 50 episodes at step 90000, reward mean -27.2, std 18.86, time cost 1.288s.
Training steps per second: 160.2.
Step 91000; q_loss 0.02404; mean_q -21.63; min_q -43.71; max_q -0.3491; mean_r -0.8058; mean_dsc 0.98; 
Tested 50 episodes at step 91000, reward mean -28.84, std 19.57, time cost 1.307s.
Training steps per second: 158.8.
Step 92000; q_loss 0.0265; mean_q -18.32; min_q -44.04; max_q -0.346; mean_r -0.7371; mean_dsc 0.98; 
Tested 50 episodes at step 92000, reward mean -34.26, std 18.97, time cost 1.338s.
Training steps per second: 159.4.
Step 93000; q_loss 0.01831; mean_q -20; min_q -44.02; max_q -0.3382; mean_r -0.7455; mean_dsc 0.98; 
Tested 50 episodes at step 93000, reward mean -31.62, std 18.71, time cost 1.291s.
Training steps per second: 159.6.
Step 94000; q_loss 0.01479; mean_q -19.47; min_q -44.46; max_q -0.3299; mean_r -0.7775; mean_dsc 0.98; 
Tested 50 episodes at step 94000, reward mean -27.76, std 17.98, time cost 1.301s.
Training steps per second: 159.1.
Step 95000; q_loss 0.02502; mean_q -19.24; min_q -44.21; max_q -0.3278; mean_r -0.7956; mean_dsc 0.98; 
Tested 50 episodes at step 95000, reward mean -33.58, std 18.92, time cost 1.279s.
Training steps per second: 161.4.
Step 96000; q_loss 0.02682; mean_q -19.52; min_q -44.38; max_q -0.3169; mean_r -0.7992; mean_dsc 0.98; 
Tested 50 episodes at step 96000, reward mean -31.68, std 18.2, time cost 1.321s.
Training steps per second: 157.4.
Step 97000; q_loss 0.01798; mean_q -19.47; min_q -44.53; max_q -0.3071; mean_r -0.7675; mean_dsc 0.98; 
Tested 50 episodes at step 97000, reward mean -31.74, std 19.47, time cost 1.288s.
Training steps per second: 160.2.
Step 98000; q_loss 0.02037; mean_q -19.9; min_q -45.09; max_q -0.2983; mean_r -0.7591; mean_dsc 0.98; 
Tested 50 episodes at step 98000, reward mean -31.04, std 19.17, time cost 1.285s.
Training steps per second: 159.5.
Step 99000; q_loss 0.01147; mean_q -19.77; min_q -44.94; max_q -0.289; mean_r -0.7724; mean_dsc 0.98; 
Tested 50 episodes at step 99000, reward mean -33.84, std 19.06, time cost 1.287s.
Training steps per second: 158.9.
Step 100000; q_loss 0.01326; mean_q -20.67; min_q -44.85; max_q -0.282; mean_r -0.8066; mean_dsc 0.98; 
Tested 50 episodes at step 100000, reward mean -29.08, std 18.55, time cost 1.287s.
Training steps per second: 159.7.
Step 101000; q_loss 0.0265; mean_q -19.04; min_q -44.98; max_q -0.2713; mean_r -0.7814; mean_dsc 0.98; 
Tested 50 episodes at step 101000, reward mean -28.38, std 19.06, time cost 1.277s.
Training steps per second: 158.4.
Step 102000; q_loss 0.01851; mean_q -18.94; min_q -45.17; max_q -0.2626; mean_r -0.7638; mean_dsc 0.98; 
Tested 50 episodes at step 102000, reward mean -32.62, std 17.22, time cost 1.279s.
Training steps per second: 161.3.
Step 103000; q_loss 0.01814; mean_q -20.72; min_q -45.23; max_q -0.2568; mean_r -0.7342; mean_dsc 0.98; 
Tested 50 episodes at step 103000, reward mean -29.96, std 18.52, time cost 1.287s.
Training steps per second: 156.3.
Step 104000; q_loss 0.01451; mean_q -19.8; min_q -45.5; max_q -0.2478; mean_r -0.7379; mean_dsc 0.98; 
Tested 50 episodes at step 104000, reward mean -31.72, std 19.17, time cost 1.268s.
Training steps per second: 160.5.
Step 105000; q_loss 0.0261; mean_q -20.73; min_q -45.52; max_q -0.2417; mean_r -0.8029; mean_dsc 0.98; 
Tested 50 episodes at step 105000, reward mean -28.4, std 18.14, time cost 1.275s.
Training steps per second: 158.5.
Step 106000; q_loss 0.02075; mean_q -20.12; min_q -45.92; max_q -0.229; mean_r -0.8374; mean_dsc 0.98; 
Tested 50 episodes at step 106000, reward mean -31.56, std 19.23, time cost 1.322s.
Training steps per second: 150.4.
Step 107000; q_loss 0.02345; mean_q -20.04; min_q -45.82; max_q -0.2196; mean_r -0.7802; mean_dsc 0.98; 
Tested 50 episodes at step 107000, reward mean -32.3, std 17.91, time cost 1.284s.
Training steps per second: 157.9.
Step 108000; q_loss 0.01754; mean_q -20.13; min_q -45.87; max_q -0.213; mean_r -0.7825; mean_dsc 0.98; 
Tested 50 episodes at step 108000, reward mean -31.96, std 17.29, time cost 1.303s.
Training steps per second: 158.
Step 109000; q_loss 0.01544; mean_q -19.06; min_q -46.36; max_q -0.2114; mean_r -0.7662; mean_dsc 0.98; 
Tested 50 episodes at step 109000, reward mean -28.96, std 19.18, time cost 1.316s.
Training steps per second: 159.3.
Step 110000; q_loss 0.01113; mean_q -19.69; min_q -45.87; max_q -0.2042; mean_r -0.7702; mean_dsc 0.98; 
Tested 50 episodes at step 110000, reward mean -30.22, std 18.44, time cost 1.347s.
Training steps per second: 156.6.
Step 111000; q_loss 0.02092; mean_q -18.87; min_q -46.35; max_q -0.1989; mean_r -0.7407; mean_dsc 0.98; 
Tested 50 episodes at step 111000, reward mean -31.38, std 19.14, time cost 1.267s.
Training steps per second: 160.8.
Step 112000; q_loss 0.01754; mean_q -20.34; min_q -46.23; max_q -0.1925; mean_r -0.7861; mean_dsc 0.98; 
Tested 50 episodes at step 112000, reward mean -32, std 18.19, time cost 1.278s.
Training steps per second: 159.6.
Step 113000; q_loss 0.01625; mean_q -17.3; min_q -46.08; max_q -0.1874; mean_r -0.7385; mean_dsc 0.98; 
Tested 50 episodes at step 113000, reward mean -27.2, std 18.74, time cost 1.259s.
Training steps per second: 159.4.
Step 114000; q_loss 0.01825; mean_q -19.62; min_q -46.71; max_q -0.1857; mean_r -0.7791; mean_dsc 0.98; 
Tested 50 episodes at step 114000, reward mean -31.56, std 18, time cost 1.263s.
Training steps per second: 161.3.
Step 115000; q_loss 0.006491; mean_q -16.5; min_q -46.15; max_q -0.1795; mean_r -0.7213; mean_dsc 0.98; 
Tested 50 episodes at step 115000, reward mean -26.98, std 19.89, time cost 1.301s.
Training steps per second: 159.1.
Step 116000; q_loss 0.009263; mean_q -17.19; min_q -45.37; max_q -0.1764; mean_r -0.7303; mean_dsc 0.98; 
Tested 50 episodes at step 116000, reward mean -33.38, std 17.63, time cost 1.294s.
Training steps per second: 159.1.
Step 117000; q_loss 0.009816; mean_q -18.31; min_q -46.79; max_q -0.174; mean_r -0.7651; mean_dsc 0.98; 
Tested 50 episodes at step 117000, reward mean -28.36, std 19.83, time cost 1.273s.
Training steps per second: 160.
Step 118000; q_loss 0.01286; mean_q -19.27; min_q -46.5; max_q -0.1717; mean_r -0.7958; mean_dsc 0.98; 
Tested 50 episodes at step 118000, reward mean -27.48, std 18.43, time cost 1.281s.
Training steps per second: 160.9.
Step 119000; q_loss 0.01511; mean_q -16.74; min_q -44.65; max_q -0.1693; mean_r -0.7502; mean_dsc 0.98; 
Tested 50 episodes at step 119000, reward mean -28.52, std 17.8, time cost 1.259s.
Training steps per second: 162.4.
Step 120000; q_loss 0.007834; mean_q -16.79; min_q -44.92; max_q -0.1666; mean_r -0.7584; mean_dsc 0.98; 
Tested 50 episodes at step 120000, reward mean -28.02, std 18.53, time cost 1.3s.
Training steps per second: 157.7.
Step 121000; q_loss 0.01064; mean_q -16.21; min_q -46.13; max_q -0.1629; mean_r -0.7098; mean_dsc 0.98; 
Tested 50 episodes at step 121000, reward mean -27.86, std 18.93, time cost 1.329s.
Training steps per second: 159.1.
Step 122000; q_loss 0.01157; mean_q -16.87; min_q -43.97; max_q -0.1599; mean_r -0.7014; mean_dsc 0.98; 
Tested 50 episodes at step 122000, reward mean -26.64, std 18.81, time cost 1.297s.
Training steps per second: 160.1.
Step 123000; q_loss 0.01062; mean_q -16.07; min_q -44.35; max_q -0.1542; mean_r -0.718; mean_dsc 0.98; 
Tested 50 episodes at step 123000, reward mean -26.66, std 18.61, time cost 1.29s.
Training steps per second: 158.9.
Step 124000; q_loss 0.005614; mean_q -19.17; min_q -40.56; max_q -0.1528; mean_r -0.8136; mean_dsc 0.98; 
Tested 50 episodes at step 124000, reward mean -29.94, std 17.43, time cost 1.298s.
Training steps per second: 159.3.
Step 125000; q_loss 0.005927; mean_q -15.65; min_q -44.75; max_q -0.1504; mean_r -0.7; mean_dsc 0.98; 
Tested 50 episodes at step 125000, reward mean -33.04, std 19.44, time cost 1.276s.
Training steps per second: 160.4.
Step 126000; q_loss 0.007069; mean_q -20.61; min_q -43.64; max_q -0.1459; mean_r -0.8665; mean_dsc 0.98; 
Tested 50 episodes at step 126000, reward mean -31, std 17.36, time cost 1.333s.
Training steps per second: 159.5.
Step 127000; q_loss 0.01039; mean_q -17.78; min_q -39.48; max_q -0.143; mean_r -0.749; mean_dsc 0.98; 
Tested 50 episodes at step 127000, reward mean -30.74, std 17.2, time cost 1.285s.
Training steps per second: 161.1.
Step 128000; q_loss 0.00719; mean_q -20.5; min_q -41; max_q -0.1389; mean_r -0.7951; mean_dsc 0.98; 
Tested 50 episodes at step 128000, reward mean -28.8, std 18.82, time cost 1.27s.
Training steps per second: 142.1.
Step 129000; q_loss 0.007085; mean_q -17.57; min_q -39.25; max_q -0.1357; mean_r -0.767; mean_dsc 0.98; 
Tested 50 episodes at step 129000, reward mean -29.74, std 17.6, time cost 1.332s.
Training steps per second: 158.5.
Step 130000; q_loss 0.006706; mean_q -17.85; min_q -43.07; max_q -0.1324; mean_r -0.7672; mean_dsc 0.98; 
Tested 50 episodes at step 130000, reward mean -27.8, std 18.14, time cost 1.349s.
Training steps per second: 156.9.
Step 131000; q_loss 0.00309; mean_q -16.86; min_q -41.78; max_q -0.1302; mean_r -0.7487; mean_dsc 0.98; 
Tested 50 episodes at step 131000, reward mean -25.26, std 17.06, time cost 1.29s.
Training steps per second: 160.8.
Step 132000; q_loss 0.003182; mean_q -16.63; min_q -40.51; max_q -0.1275; mean_r -0.7198; mean_dsc 0.98; 
Tested 50 episodes at step 132000, reward mean -32.8, std 18.96, time cost 1.283s.
Training steps per second: 159.4.
Step 133000; q_loss 0.012; mean_q -19.05; min_q -40.62; max_q -0.1245; mean_r -0.8092; mean_dsc 0.98; 
Tested 50 episodes at step 133000, reward mean -34.96, std 18.48, time cost 1.272s.
Training steps per second: 161.9.
Step 134000; q_loss 0.006792; mean_q -18.16; min_q -40.27; max_q -0.1217; mean_r -0.7418; mean_dsc 0.98; 
Tested 50 episodes at step 134000, reward mean -31, std 19.27, time cost 1.294s.
Training steps per second: 160.4.
Step 135000; q_loss 0.01329; mean_q -18.19; min_q -41.24; max_q -0.1171; mean_r -0.7274; mean_dsc 0.98; 
Tested 50 episodes at step 135000, reward mean -30.54, std 18.85, time cost 1.289s.
Training steps per second: 158.4.
Step 136000; q_loss 0.008112; mean_q -17.28; min_q -40.97; max_q -0.1124; mean_r -0.7309; mean_dsc 0.98; 
Tested 50 episodes at step 136000, reward mean -27.78, std 18.11, time cost 1.261s.
Training steps per second: 161.
Step 137000; q_loss 0.003998; mean_q -20.3; min_q -41.25; max_q -0.1103; mean_r -0.8033; mean_dsc 0.98; 
Tested 50 episodes at step 137000, reward mean -31.9, std 16.97, time cost 1.316s.
Training steps per second: 158.7.
Step 138000; q_loss 0.008828; mean_q -20.45; min_q -40.86; max_q -0.1089; mean_r -0.8087; mean_dsc 0.98; 
Tested 50 episodes at step 138000, reward mean -27.38, std 18.91, time cost 1.276s.
Training steps per second: 160.8.
Step 139000; q_loss 0.004055; mean_q -16.66; min_q -41.42; max_q -0.1056; mean_r -0.729; mean_dsc 0.98; 
Tested 50 episodes at step 139000, reward mean -25.76, std 18.71, time cost 1.288s.
Training steps per second: 151.6.
Step 140000; q_loss 0.007295; mean_q -19.93; min_q -41.77; max_q -0.1037; mean_r -0.8101; mean_dsc 0.98; 
Tested 50 episodes at step 140000, reward mean -23.88, std 18.62, time cost 1.287s.
Training steps per second: 158.2.
Step 141000; q_loss 0.005033; mean_q -17.75; min_q -42.16; max_q -0.1031; mean_r -0.7603; mean_dsc 0.98; 
Tested 50 episodes at step 141000, reward mean -27.88, std 17.37, time cost 1.249s.
Training steps per second: 160.3.
Step 142000; q_loss 0.004402; mean_q -14.92; min_q -42.16; max_q -0.1029; mean_r -0.6798; mean_dsc 0.98; 
Tested 50 episodes at step 142000, reward mean -28.98, std 18.11, time cost 1.288s.
Training steps per second: 160.1.
Step 143000; q_loss 0.002077; mean_q -16.92; min_q -40.86; max_q -0.1011; mean_r -0.7937; mean_dsc 0.98; 
Tested 50 episodes at step 143000, reward mean -32.4, std 19.47, time cost 1.314s.
Training steps per second: 158.8.
Step 144000; q_loss 0.01145; mean_q -16.72; min_q -42.31; max_q -0.09857; mean_r -0.7335; mean_dsc 0.98; 
Tested 50 episodes at step 144000, reward mean -30.08, std 17.84, time cost 1.289s.
Training steps per second: 161.4.
Step 145000; q_loss 0.003839; mean_q -16.8; min_q -41.23; max_q -0.09527; mean_r -0.7392; mean_dsc 0.98; 
Tested 50 episodes at step 145000, reward mean -32.22, std 18.28, time cost 1.258s.
Training steps per second: 160.7.
Step 146000; q_loss 0.003172; mean_q -17.41; min_q -42.6; max_q -0.0947; mean_r -0.7217; mean_dsc 0.98; 
Tested 50 episodes at step 146000, reward mean -37.68, std 15.8, time cost 1.306s.
Training steps per second: 159.8.
Step 147000; q_loss 0.003107; mean_q -18.09; min_q -42.26; max_q -0.09109; mean_r -0.7638; mean_dsc 0.98; 
Tested 50 episodes at step 147000, reward mean -29.16, std 17.98, time cost 1.288s.
Training steps per second: 159.8.
Step 148000; q_loss 0.00232; mean_q -18.87; min_q -41.62; max_q -0.08921; mean_r -0.7621; mean_dsc 0.98; 
Tested 50 episodes at step 148000, reward mean -29.56, std 18.83, time cost 1.284s.
Training steps per second: 160.3.
Step 149000; q_loss 0.003469; mean_q -17.05; min_q -42.58; max_q -0.08881; mean_r -0.7481; mean_dsc 0.98; 
Tested 50 episodes at step 149000, reward mean -28.56, std 19.41, time cost 1.28s.
Training steps per second: 158.4.
Step 150000; q_loss 0.003591; mean_q -18.02; min_q -42.29; max_q -0.08554; mean_r -0.7467; mean_dsc 0.98; 
Tested 50 episodes at step 150000, reward mean -27.62, std 18.84, time cost 1.281s.
Training steps per second: 157.5.
Step 151000; q_loss 0.002486; mean_q -20.24; min_q -42.62; max_q -0.08502; mean_r -0.8083; mean_dsc 0.98; 
Tested 50 episodes at step 151000, reward mean -31.48, std 17.95, time cost 1.407s.
Training steps per second: 156.5.
Step 152000; q_loss 0.00357; mean_q -17.17; min_q -42.39; max_q -0.08313; mean_r -0.7618; mean_dsc 0.98; 
Tested 50 episodes at step 152000, reward mean -30.36, std 18.75, time cost 1.28s.
Training steps per second: 161.5.
Step 153000; q_loss 0.003311; mean_q -16.38; min_q -40.96; max_q -0.08132; mean_r -0.7138; mean_dsc 0.98; 
Tested 50 episodes at step 153000, reward mean -33.32, std 17.35, time cost 1.294s.
Training steps per second: 159.5.
Step 154000; q_loss 0.002628; mean_q -17.68; min_q -42.41; max_q -0.07886; mean_r -0.7579; mean_dsc 0.98; 
Tested 50 episodes at step 154000, reward mean -32.02, std 18.78, time cost 1.266s.
Training steps per second: 161.3.
Step 155000; q_loss 0.005192; mean_q -17; min_q -40.97; max_q -0.07665; mean_r -0.744; mean_dsc 0.98; 
Tested 50 episodes at step 155000, reward mean -27.78, std 18.81, time cost 1.319s.
Training steps per second: 158.2.
Step 156000; q_loss 0.004211; mean_q -15.95; min_q -41.74; max_q -0.07451; mean_r -0.7013; mean_dsc 0.98; 
Tested 50 episodes at step 156000, reward mean -30.26, std 19.03, time cost 1.243s.
Training steps per second: 160.6.
Step 157000; q_loss 0.002485; mean_q -16.06; min_q -42.07; max_q -0.0724; mean_r -0.7484; mean_dsc 0.98; 
Tested 50 episodes at step 157000, reward mean -28.86, std 19.01, time cost 1.301s.
Training steps per second: 160.8.
Step 158000; q_loss 0.002444; mean_q -18.14; min_q -42.75; max_q -0.07114; mean_r -0.7758; mean_dsc 0.98; 
Tested 50 episodes at step 158000, reward mean -29.98, std 18.16, time cost 1.249s.
Training steps per second: 161.4.
Step 159000; q_loss 0.002295; mean_q -21.2; min_q -42.74; max_q -0.06993; mean_r -0.8416; mean_dsc 0.98; 
Tested 50 episodes at step 159000, reward mean -25.36, std 18.59, time cost 1.283s.
Training steps per second: 160.8.
Step 160000; q_loss 0.002833; mean_q -16.94; min_q -41.99; max_q -0.06998; mean_r -0.6741; mean_dsc 0.98; 
Tested 50 episodes at step 160000, reward mean -27.62, std 18, time cost 1.346s.
Training steps per second: 160.2.
Step 161000; q_loss 0.002312; mean_q -17.65; min_q -40.75; max_q -0.06844; mean_r -0.8147; mean_dsc 0.98; 
Tested 50 episodes at step 161000, reward mean -26.88, std 17.99, time cost 1.259s.
Training steps per second: 160.2.
Step 162000; q_loss 0.002961; mean_q -14.22; min_q -41.96; max_q -0.0687; mean_r -0.6909; mean_dsc 0.98; 
Tested 50 episodes at step 162000, reward mean -30.16, std 17.84, time cost 1.244s.
Training steps per second: 161.5.
Step 163000; q_loss 0.002254; mean_q -16.41; min_q -41.34; max_q -0.07045; mean_r -0.7304; mean_dsc 0.98; 
Tested 50 episodes at step 163000, reward mean -30.74, std 18.3, time cost 1.289s.
Training steps per second: 160.5.
Step 164000; q_loss 0.002628; mean_q -16.85; min_q -42.37; max_q -0.0685; mean_r -0.7089; mean_dsc 0.98; 
Tested 50 episodes at step 164000, reward mean -28.46, std 18.47, time cost 1.289s.
Training steps per second: 159.6.
Step 165000; q_loss 0.003681; mean_q -17.14; min_q -42.75; max_q -0.06664; mean_r -0.7194; mean_dsc 0.98; 
Tested 50 episodes at step 165000, reward mean -30.66, std 18.82, time cost 1.244s.
Training steps per second: 162.7.
Step 166000; q_loss 0.00632; mean_q -17.29; min_q -42.03; max_q -0.06557; mean_r -0.7715; mean_dsc 0.98; 
Tested 50 episodes at step 166000, reward mean -32.5, std 16.16, time cost 1.236s.
Training steps per second: 162.4.
Step 167000; q_loss 0.001722; mean_q -19.61; min_q -42.39; max_q -0.06521; mean_r -0.8095; mean_dsc 0.98; 
Tested 50 episodes at step 167000, reward mean -29.8, std 18.08, time cost 1.274s.
Training steps per second: 160.9.
Step 168000; q_loss 0.003864; mean_q -20.52; min_q -42; max_q -0.06354; mean_r -0.8441; mean_dsc 0.98; 
Tested 50 episodes at step 168000, reward mean -28.76, std 19.41, time cost 1.286s.
Training steps per second: 159.6.
Step 169000; q_loss 0.004746; mean_q -18.99; min_q -42; max_q -0.0614; mean_r -0.7509; mean_dsc 0.98; 
Tested 50 episodes at step 169000, reward mean -24.74, std 18.58, time cost 1.257s.
Training steps per second: 161.7.
Step 170000; q_loss 0.001755; mean_q -18.46; min_q -41.99; max_q -0.05947; mean_r -0.7902; mean_dsc 0.98; 
Tested 50 episodes at step 170000, reward mean -26.42, std 18.19, time cost 1.249s.
Training steps per second: 161.1.
Step 171000; q_loss 0.004277; mean_q -17.01; min_q -41.99; max_q -0.05567; mean_r -0.7403; mean_dsc 0.98; 
Tested 50 episodes at step 171000, reward mean -35.24, std 18.85, time cost 1.336s.
Training steps per second: 160.5.
Step 172000; q_loss 0.00303; mean_q -16.15; min_q -42.73; max_q -0.0537; mean_r -0.759; mean_dsc 0.98; 
Tested 50 episodes at step 172000, reward mean -25.74, std 19.13, time cost 1.359s.
Training steps per second: 159.7.
Step 173000; q_loss 0.00222; mean_q -16.13; min_q -41.63; max_q -0.05117; mean_r -0.7311; mean_dsc 0.98; 
Tested 50 episodes at step 173000, reward mean -30.78, std 18.41, time cost 1.286s.
Training steps per second: 160.9.
Step 174000; q_loss 0.01189; mean_q -16.78; min_q -41.95; max_q -0.04765; mean_r -0.7371; mean_dsc 0.98; 
Tested 50 episodes at step 174000, reward mean -30.16, std 19.16, time cost 1.261s.
Training steps per second: 162.6.
Step 175000; q_loss 0.008035; mean_q -19.36; min_q -42.26; max_q -0.04663; mean_r -0.8202; mean_dsc 0.98; 
Tested 50 episodes at step 175000, reward mean -29.94, std 18.54, time cost 1.325s.
Training steps per second: 158.9.
Step 176000; q_loss 0.003324; mean_q -17.38; min_q -42.23; max_q -0.04085; mean_r -0.7248; mean_dsc 0.98; 
Tested 50 episodes at step 176000, reward mean -32.44, std 18.46, time cost 1.284s.
Training steps per second: 161.3.
Step 177000; q_loss 0.002857; mean_q -18.22; min_q -41.96; max_q -0.03839; mean_r -0.7458; mean_dsc 0.98; 
Tested 50 episodes at step 177000, reward mean -26.04, std 17.61, time cost 1.262s.
Training steps per second: 162.4.
Step 178000; q_loss 0.003183; mean_q -16.72; min_q -41.94; max_q -0.03777; mean_r -0.7388; mean_dsc 0.98; 
Tested 50 episodes at step 178000, reward mean -33.24, std 18.28, time cost 1.257s.
Training steps per second: 160.6.
Step 179000; q_loss 0.003798; mean_q -19.3; min_q -42.3; max_q -0.036; mean_r -0.8286; mean_dsc 0.98; 
Tested 50 episodes at step 179000, reward mean -28.82, std 18.19, time cost 1.246s.
Training steps per second: 140.2.
Step 180000; q_loss 0.001701; mean_q -16.85; min_q -40.89; max_q -0.03375; mean_r -0.7512; mean_dsc 0.98; 
Tested 50 episodes at step 180000, reward mean -29.12, std 19.4, time cost 1.304s.
Training steps per second: 150.8.
Step 181000; q_loss 0.00296; mean_q -16.96; min_q -42.62; max_q -0.03385; mean_r -0.7077; mean_dsc 0.98; 
Tested 50 episodes at step 181000, reward mean -26.52, std 19.02, time cost 1.248s.
Training steps per second: 161.2.
Step 182000; q_loss 0.005145; mean_q -16.45; min_q -40.97; max_q -0.03272; mean_r -0.7487; mean_dsc 0.98; 
Tested 50 episodes at step 182000, reward mean -33.6, std 18.23, time cost 1.276s.
Training steps per second: 160.5.
Step 183000; q_loss 0.002135; mean_q -17.67; min_q -42.35; max_q -0.03349; mean_r -0.7652; mean_dsc 0.98; 
Tested 50 episodes at step 183000, reward mean -28.08, std 18.48, time cost 1.259s.
Training steps per second: 162.3.
Step 184000; q_loss 0.002562; mean_q -17.79; min_q -42.41; max_q -0.03123; mean_r -0.7555; mean_dsc 0.98; 
Tested 50 episodes at step 184000, reward mean -32.26, std 18.11, time cost 1.272s.
Training steps per second: 161.
Step 185000; q_loss 0.002357; mean_q -15.59; min_q -42.08; max_q -0.02913; mean_r -0.693; mean_dsc 0.98; 
Tested 50 episodes at step 185000, reward mean -27.54, std 16.78, time cost 1.258s.
Training steps per second: 162.7.
Step 186000; q_loss 0.001874; mean_q -18.23; min_q -42.46; max_q -0.0263; mean_r -0.7654; mean_dsc 0.98; 
Tested 50 episodes at step 186000, reward mean -26.74, std 17.78, time cost 1.247s.
Training steps per second: 161.7.
Step 187000; q_loss 0.003797; mean_q -16.46; min_q -42.09; max_q -0.02418; mean_r -0.7475; mean_dsc 0.98; 
Tested 50 episodes at step 187000, reward mean -30.78, std 20.04, time cost 1.272s.
Training steps per second: 161.7.
Step 188000; q_loss 0.001743; mean_q -16.44; min_q -42.73; max_q -0.02264; mean_r -0.7527; mean_dsc 0.98; 
Tested 50 episodes at step 188000, reward mean -31.38, std 19.96, time cost 1.249s.
Training steps per second: 162.5.
Step 189000; q_loss 0.002745; mean_q -17.14; min_q -40.96; max_q -0.02218; mean_r -0.7406; mean_dsc 0.98; 
Tested 50 episodes at step 189000, reward mean -27.66, std 17.02, time cost 1.223s.
Training steps per second: 161.8.
Step 190000; q_loss 0.002891; mean_q -17.8; min_q -42.07; max_q -0.02241; mean_r -0.7437; mean_dsc 0.98; 
Tested 50 episodes at step 190000, reward mean -27.28, std 18.38, time cost 1.254s.
Training steps per second: 161.3.
Step 191000; q_loss 0.004501; mean_q -16.78; min_q -42.71; max_q -0.02244; mean_r -0.7226; mean_dsc 0.98; 
Tested 50 episodes at step 191000, reward mean -35.24, std 17.46, time cost 1.271s.
Training steps per second: 161.2.
Step 192000; q_loss 0.002591; mean_q -14.72; min_q -40.95; max_q -0.02155; mean_r -0.7242; mean_dsc 0.98; 
Tested 50 episodes at step 192000, reward mean -27.88, std 18.76, time cost 1.287s.
Training steps per second: 162.
Step 193000; q_loss 0.003178; mean_q -17.51; min_q -42.01; max_q -0.01827; mean_r -0.7357; mean_dsc 0.98; 
Tested 50 episodes at step 193000, reward mean -31.76, std 18.73, time cost 1.275s.
Training steps per second: 160.5.
Step 194000; q_loss 0.001994; mean_q -15.99; min_q -41.28; max_q -0.01682; mean_r -0.758; mean_dsc 0.98; 
Tested 50 episodes at step 194000, reward mean -28.88, std 18.17, time cost 1.284s.
Training steps per second: 160.2.
Step 195000; q_loss 0.002496; mean_q -14.95; min_q -42.31; max_q -0.01688; mean_r -0.7091; mean_dsc 0.98; 
Tested 50 episodes at step 195000, reward mean -29.54, std 17.85, time cost 1.269s.
Training steps per second: 161.5.
Step 196000; q_loss 0.0008651; mean_q -16.67; min_q -41.63; max_q -0.01679; mean_r -0.7096; mean_dsc 0.98; 
Tested 50 episodes at step 196000, reward mean -32.5, std 17.77, time cost 1.289s.
Training steps per second: 161.3.
Step 197000; q_loss 0.00172; mean_q -17.78; min_q -41.28; max_q -0.01509; mean_r -0.7718; mean_dsc 0.98; 
Tested 50 episodes at step 197000, reward mean -28.74, std 19.63, time cost 1.275s.
Training steps per second: 161.
Step 198000; q_loss 0.001319; mean_q -16.04; min_q -42.31; max_q -0.01134; mean_r -0.7435; mean_dsc 0.98; 
Tested 50 episodes at step 198000, reward mean -31.68, std 18.78, time cost 1.26s.
Training steps per second: 162.3.
Step 199000; q_loss 0.003497; mean_q -18.32; min_q -42.29; max_q -0.009809; mean_r -0.8193; mean_dsc 0.98; 
Tested 50 episodes at step 199000, reward mean -29.22, std 18.05, time cost 1.376s.
Training steps per second: 154.7.
Step 200000; q_loss 0.002392; mean_q -18.84; min_q -41.98; max_q -0.0119; mean_r -0.7714; mean_dsc 0.98; 
Tested 50 episodes at step 200000, reward mean -28.02, std 18.54, time cost 1.277s.
