device: cuda.
Representation model loaded from /content/drive/MyDrive/CEng502-AdvancedDeepLearning/ExperimentCodes/Wu_GridRoom_Run2/laplacian_code_Wu/log/laprepr/HardMaze/test/model.ckpt.
Start collecting transitions.
(10000/10000) steps collected.
Replay buffer initialization finished, time cost: 1.5102362632751465s
Training steps per second: 0.
Step 1; q_loss 1.046; mean_q -1; min_q -1.204; max_q -0.4518; mean_r -1.056; mean_dsc 0.98; 
Tested 50 episodes at step 1, reward mean -50, std 0, time cost 1.312s.
Training steps per second: 161.
Step 1000; q_loss 0.001628; mean_q -1.594; min_q -1.897; max_q -0.886; mean_r -1.032; mean_dsc 0.98; 
Tested 50 episodes at step 1000, reward mean -49.92, std 0.2713, time cost 1.304s.
Training steps per second: 161.2.
Step 2000; q_loss 0.003246; mean_q -2.35; min_q -2.893; max_q -0.6664; mean_r -1.024; mean_dsc 0.98; 
Tested 50 episodes at step 2000, reward mean -49.94, std 0.2375, time cost 1.322s.
Training steps per second: 160.1.
Step 3000; q_loss 0.00552; mean_q -3.206; min_q -3.866; max_q -1.569; mean_r -1.036; mean_dsc 0.98; 
Tested 50 episodes at step 3000, reward mean -49.98, std 0.14, time cost 1.312s.
Training steps per second: 160.6.
Step 4000; q_loss 0.0114; mean_q -3.886; min_q -4.828; max_q -1.283; mean_r -0.9971; mean_dsc 0.98; 
Tested 50 episodes at step 4000, reward mean -47.58, std 7.264, time cost 1.271s.
Training steps per second: 166.
Step 5000; q_loss 0.01732; mean_q -4.738; min_q -5.695; max_q -2.165; mean_r -1.02; mean_dsc 0.98; 
Tested 50 episodes at step 5000, reward mean -48.02, std 6.716, time cost 1.298s.
Training steps per second: 163.
Step 6000; q_loss 0.01928; mean_q -5.424; min_q -6.745; max_q -1.899; mean_r -1.002; mean_dsc 0.98; 
Tested 50 episodes at step 6000, reward mean -48.06, std 9.509, time cost 1.327s.
Training steps per second: 161.
Step 7000; q_loss 0.008399; mean_q -6.231; min_q -7.701; max_q -2.127; mean_r -0.989; mean_dsc 0.98; 
Tested 50 episodes at step 7000, reward mean -44.26, std 15.55, time cost 1.279s.
Training steps per second: 162.5.
Step 8000; q_loss 0.008262; mean_q -7.261; min_q -8.503; max_q -2.262; mean_r -1.034; mean_dsc 0.98; 
Tested 50 episodes at step 8000, reward mean -46.58, std 11.62, time cost 1.307s.
Training steps per second: 160.8.
Step 9000; q_loss 0.01631; mean_q -7.895; min_q -9.471; max_q -2.339; mean_r -1.012; mean_dsc 0.98; 
Tested 50 episodes at step 9000, reward mean -46.36, std 12.38, time cost 1.276s.
Training steps per second: 135.3.
Step 10000; q_loss 0.02273; mean_q -7.978; min_q -10.26; max_q -2.368; mean_r -0.944; mean_dsc 0.98; 
Tested 50 episodes at step 10000, reward mean -43.4, std 16.38, time cost 1.285s.
Training steps per second: 160.8.
Step 11000; q_loss 0.03818; mean_q -9.064; min_q -10.98; max_q -2.367; mean_r -0.9888; mean_dsc 0.98; 
Tested 50 episodes at step 11000, reward mean -39.74, std 19.35, time cost 1.318s.
Training steps per second: 158.7.
Step 12000; q_loss 0.04542; mean_q -9.479; min_q -11.96; max_q -2.307; mean_r -0.9596; mean_dsc 0.98; 
Tested 50 episodes at step 12000, reward mean -39.42, std 18.9, time cost 1.315s.
Training steps per second: 159.8.
Step 13000; q_loss 0.02216; mean_q -10.25; min_q -12.72; max_q -2.24; mean_r -0.9847; mean_dsc 0.98; 
Tested 50 episodes at step 13000, reward mean -32.14, std 21.21, time cost 1.318s.
Training steps per second: 159.8.
Step 14000; q_loss 0.0567; mean_q -10.33; min_q -13.25; max_q -2.156; mean_r -0.9519; mean_dsc 0.98; 
Tested 50 episodes at step 14000, reward mean -36.04, std 19.57, time cost 1.283s.
Training steps per second: 159.6.
Step 15000; q_loss 0.02682; mean_q -11.48; min_q -14.33; max_q -2.098; mean_r -0.9984; mean_dsc 0.98; 
Tested 50 episodes at step 15000, reward mean -35.82, std 20.81, time cost 1.3s.
Training steps per second: 158.3.
Step 16000; q_loss 0.0221; mean_q -11.61; min_q -14.88; max_q -2.001; mean_r -0.9721; mean_dsc 0.98; 
Tested 50 episodes at step 16000, reward mean -35.12, std 20.84, time cost 1.313s.
Training steps per second: 159.1.
Step 17000; q_loss 0.04932; mean_q -11.45; min_q -15.7; max_q -1.929; mean_r -0.9467; mean_dsc 0.98; 
Tested 50 episodes at step 17000, reward mean -36.24, std 19.52, time cost 1.264s.
Training steps per second: 161.5.
Step 18000; q_loss 0.04126; mean_q -11.98; min_q -16.42; max_q -1.904; mean_r -0.9115; mean_dsc 0.98; 
Tested 50 episodes at step 18000, reward mean -43.78, std 15.47, time cost 1.294s.
Training steps per second: 160.
Step 19000; q_loss 0.03446; mean_q -12; min_q -16.51; max_q -1.861; mean_r -0.9159; mean_dsc 0.98; 
Tested 50 episodes at step 19000, reward mean -32.84, std 21.24, time cost 1.334s.
Training steps per second: 146.9.
Step 20000; q_loss 0.04078; mean_q -12.15; min_q -17.81; max_q -1.825; mean_r -0.9127; mean_dsc 0.98; 
Tested 50 episodes at step 20000, reward mean -30.38, std 21.5, time cost 1.289s.
Training steps per second: 159.7.
Step 21000; q_loss 0.02143; mean_q -14.01; min_q -18.13; max_q -1.792; mean_r -0.9696; mean_dsc 0.98; 
Tested 50 episodes at step 21000, reward mean -34.16, std 21.26, time cost 1.315s.
Training steps per second: 157.8.
Step 22000; q_loss 0.01755; mean_q -13.03; min_q -19.17; max_q -1.759; mean_r -0.8485; mean_dsc 0.98; 
Tested 50 episodes at step 22000, reward mean -37.68, std 18.96, time cost 1.319s.
Training steps per second: 158.
Step 23000; q_loss 0.01543; mean_q -14.69; min_q -19.85; max_q -1.726; mean_r -0.9237; mean_dsc 0.98; 
Tested 50 episodes at step 23000, reward mean -38.46, std 18.65, time cost 1.301s.
Training steps per second: 158.8.
Step 24000; q_loss 0.07011; mean_q -13.95; min_q -20.19; max_q -1.693; mean_r -0.8997; mean_dsc 0.98; 
Tested 50 episodes at step 24000, reward mean -36.28, std 20.16, time cost 1.296s.
Training steps per second: 141.3.
Step 25000; q_loss 0.02473; mean_q -14.21; min_q -20.79; max_q -1.667; mean_r -0.8626; mean_dsc 0.98; 
Tested 50 episodes at step 25000, reward mean -37.56, std 19.22, time cost 1.274s.
Training steps per second: 161.2.
Step 26000; q_loss 0.02904; mean_q -15.59; min_q -21.52; max_q -1.631; mean_r -0.9403; mean_dsc 0.98; 
Tested 50 episodes at step 26000, reward mean -34.58, std 19.92, time cost 1.293s.
Training steps per second: 160.9.
Step 27000; q_loss 0.02921; mean_q -14.94; min_q -22.05; max_q -1.611; mean_r -0.9086; mean_dsc 0.98; 
Tested 50 episodes at step 27000, reward mean -34.8, std 19.25, time cost 1.302s.
Training steps per second: 160.6.
Step 28000; q_loss 0.03094; mean_q -15.17; min_q -22.68; max_q -1.585; mean_r -0.9084; mean_dsc 0.98; 
Tested 50 episodes at step 28000, reward mean -37.76, std 18.84, time cost 1.253s.
Training steps per second: 162.3.
Step 29000; q_loss 0.02481; mean_q -15.34; min_q -22.95; max_q -1.569; mean_r -0.8716; mean_dsc 0.98; 
Tested 50 episodes at step 29000, reward mean -31.58, std 20.36, time cost 1.271s.
Training steps per second: 131.
Step 30000; q_loss 0.02279; mean_q -16.65; min_q -23.9; max_q -1.541; mean_r -0.9089; mean_dsc 0.98; 
Tested 50 episodes at step 30000, reward mean -26.66, std 21.19, time cost 1.312s.
Training steps per second: 158.1.
Step 31000; q_loss 0.0518; mean_q -16.28; min_q -24.45; max_q -1.522; mean_r -0.9066; mean_dsc 0.98; 
Tested 50 episodes at step 31000, reward mean -37.04, std 19.96, time cost 1.265s.
Training steps per second: 158.4.
Step 32000; q_loss 0.01982; mean_q -16.6; min_q -25.1; max_q -1.498; mean_r -0.8598; mean_dsc 0.98; 
Tested 50 episodes at step 32000, reward mean -37.92, std 19.46, time cost 1.273s.
Training steps per second: 162.1.
Step 33000; q_loss 0.052; mean_q -16.86; min_q -25.68; max_q -1.451; mean_r -0.8882; mean_dsc 0.98; 
Tested 50 episodes at step 33000, reward mean -33.82, std 20.17, time cost 1.281s.
Training steps per second: 161.5.
Step 34000; q_loss 0.02929; mean_q -17.85; min_q -26.24; max_q -1.425; mean_r -0.9202; mean_dsc 0.98; 
Tested 50 episodes at step 34000, reward mean -31.84, std 18.93, time cost 1.311s.
Training steps per second: 159.
Step 35000; q_loss 0.03365; mean_q -16.8; min_q -26.61; max_q -1.385; mean_r -0.8494; mean_dsc 0.98; 
Tested 50 episodes at step 35000, reward mean -35.28, std 19.3, time cost 1.351s.
Training steps per second: 158.2.
Step 36000; q_loss 0.02843; mean_q -18.78; min_q -27.23; max_q -1.356; mean_r -0.9148; mean_dsc 0.98; 
Tested 50 episodes at step 36000, reward mean -27.4, std 20.16, time cost 1.275s.
Training steps per second: 159.
Step 37000; q_loss 0.03586; mean_q -17.97; min_q -27.69; max_q -1.324; mean_r -0.8633; mean_dsc 0.98; 
Tested 50 episodes at step 37000, reward mean -28.8, std 20.49, time cost 1.312s.
Training steps per second: 158.2.
Step 38000; q_loss 0.01155; mean_q -16.81; min_q -28.08; max_q -1.305; mean_r -0.8052; mean_dsc 0.98; 
Tested 50 episodes at step 38000, reward mean -33.06, std 19.33, time cost 1.303s.
Training steps per second: 160.3.
Step 39000; q_loss 0.04138; mean_q -19.27; min_q -28.64; max_q -1.269; mean_r -0.8981; mean_dsc 0.98; 
Tested 50 episodes at step 39000, reward mean -34.7, std 18.77, time cost 1.269s.
Training steps per second: 160.6.
Step 40000; q_loss 0.04981; mean_q -19.22; min_q -28.7; max_q -1.252; mean_r -0.8839; mean_dsc 0.98; 
Tested 50 episodes at step 40000, reward mean -32.06, std 20.1, time cost 1.281s.
Training steps per second: 161.3.
Step 41000; q_loss 0.03132; mean_q -19.87; min_q -29.53; max_q -1.234; mean_r -0.8993; mean_dsc 0.98; 
Tested 50 episodes at step 41000, reward mean -35.52, std 17.83, time cost 1.291s.
Training steps per second: 158.9.
Step 42000; q_loss 0.03409; mean_q -20.5; min_q -29.47; max_q -1.219; mean_r -0.8667; mean_dsc 0.98; 
Tested 50 episodes at step 42000, reward mean -30.44, std 19.35, time cost 1.294s.
Training steps per second: 158.9.
Step 43000; q_loss 0.01305; mean_q -19.71; min_q -30.18; max_q -1.191; mean_r -0.8895; mean_dsc 0.98; 
Tested 50 episodes at step 43000, reward mean -27.8, std 19.71, time cost 1.278s.
Training steps per second: 160.8.
Step 44000; q_loss 0.02356; mean_q -17.33; min_q -30.6; max_q -1.183; mean_r -0.8248; mean_dsc 0.98; 
Tested 50 episodes at step 44000, reward mean -28.64, std 20.17, time cost 1.276s.
Training steps per second: 155.4.
Step 45000; q_loss 0.03447; mean_q -18.79; min_q -30.74; max_q -1.17; mean_r -0.8322; mean_dsc 0.98; 
Tested 50 episodes at step 45000, reward mean -38.54, std 16.37, time cost 1.255s.
Training steps per second: 161.5.
Step 46000; q_loss 0.02096; mean_q -19.3; min_q -31.44; max_q -1.138; mean_r -0.8457; mean_dsc 0.98; 
Tested 50 episodes at step 46000, reward mean -35.52, std 17.62, time cost 1.292s.
Training steps per second: 160.8.
Step 47000; q_loss 0.02229; mean_q -19.31; min_q -31.95; max_q -1.108; mean_r -0.8627; mean_dsc 0.98; 
Tested 50 episodes at step 47000, reward mean -27.96, std 19.01, time cost 1.272s.
Training steps per second: 160.
Step 48000; q_loss 0.01541; mean_q -18.28; min_q -32.15; max_q -1.1; mean_r -0.8133; mean_dsc 0.98; 
Tested 50 episodes at step 48000, reward mean -33.1, std 18.99, time cost 1.27s.
Training steps per second: 162.
Step 49000; q_loss 0.02352; mean_q -16.53; min_q -32.5; max_q -1.089; mean_r -0.8312; mean_dsc 0.98; 
Tested 50 episodes at step 49000, reward mean -28.6, std 18.49, time cost 1.296s.
Training steps per second: 158.1.
Step 50000; q_loss 0.02775; mean_q -19.31; min_q -32.94; max_q -1.079; mean_r -0.7985; mean_dsc 0.98; 
Tested 50 episodes at step 50000, reward mean -31.12, std 19.84, time cost 1.242s.
Training steps per second: 160.6.
Step 51000; q_loss 0.02083; mean_q -18.11; min_q -33.44; max_q -1.101; mean_r -0.8313; mean_dsc 0.98; 
Tested 50 episodes at step 51000, reward mean -30, std 20.56, time cost 1.253s.
Training steps per second: 161.
Step 52000; q_loss 0.02506; mean_q -21.33; min_q -33.77; max_q -1.097; mean_r -0.868; mean_dsc 0.98; 
Tested 50 episodes at step 52000, reward mean -30.56, std 18.86, time cost 1.29s.
Training steps per second: 160.
Step 53000; q_loss 0.05755; mean_q -20.1; min_q -34.11; max_q -1.089; mean_r -0.8092; mean_dsc 0.98; 
Tested 50 episodes at step 53000, reward mean -32.62, std 19.75, time cost 1.313s.
Training steps per second: 159.2.
Step 54000; q_loss 0.01455; mean_q -21.02; min_q -34.59; max_q -1.076; mean_r -0.8997; mean_dsc 0.98; 
Tested 50 episodes at step 54000, reward mean -29.88, std 19.03, time cost 1.293s.
Training steps per second: 158.9.
Step 55000; q_loss 0.0229; mean_q -18.08; min_q -34.88; max_q -1.051; mean_r -0.7401; mean_dsc 0.98; 
Tested 50 episodes at step 55000, reward mean -27.18, std 21.52, time cost 1.279s.
Training steps per second: 160.9.
Step 56000; q_loss 0.02304; mean_q -19.79; min_q -35.58; max_q -1.02; mean_r -0.8213; mean_dsc 0.98; 
Tested 50 episodes at step 56000, reward mean -33.1, std 19.83, time cost 1.265s.
Training steps per second: 160.3.
Step 57000; q_loss 0.01358; mean_q -19.89; min_q -35.66; max_q -0.9969; mean_r -0.8299; mean_dsc 0.98; 
Tested 50 episodes at step 57000, reward mean -30.52, std 18.46, time cost 1.319s.
Training steps per second: 157.6.
Step 58000; q_loss 0.01422; mean_q -16.67; min_q -36.05; max_q -0.9671; mean_r -0.7642; mean_dsc 0.98; 
Tested 50 episodes at step 58000, reward mean -33.76, std 18.36, time cost 1.275s.
Training steps per second: 161.4.
Step 59000; q_loss 0.02718; mean_q -21.73; min_q -36.4; max_q -0.9509; mean_r -0.877; mean_dsc 0.98; 
Tested 50 episodes at step 59000, reward mean -28.48, std 19.04, time cost 1.257s.
Training steps per second: 158.7.
Step 60000; q_loss 0.0086; mean_q -18.54; min_q -36.72; max_q -0.9389; mean_r -0.7481; mean_dsc 0.98; 
Tested 50 episodes at step 60000, reward mean -24.36, std 20.18, time cost 1.272s.
Training steps per second: 155.4.
Step 61000; q_loss 0.01505; mean_q -20.24; min_q -37.07; max_q -0.913; mean_r -0.8522; mean_dsc 0.98; 
Tested 50 episodes at step 61000, reward mean -34.5, std 18.78, time cost 1.274s.
Training steps per second: 160.7.
Step 62000; q_loss 0.03079; mean_q -20.11; min_q -37.33; max_q -0.8959; mean_r -0.8352; mean_dsc 0.98; 
Tested 50 episodes at step 62000, reward mean -32.24, std 18.55, time cost 1.294s.
Training steps per second: 161.5.
Step 63000; q_loss 0.02126; mean_q -18.59; min_q -37.64; max_q -0.8554; mean_r -0.7895; mean_dsc 0.98; 
Tested 50 episodes at step 63000, reward mean -27.64, std 16.97, time cost 1.277s.
Training steps per second: 161.
Step 64000; q_loss 0.01069; mean_q -19.62; min_q -37.89; max_q -0.8366; mean_r -0.7747; mean_dsc 0.98; 
Tested 50 episodes at step 64000, reward mean -33.6, std 18.55, time cost 1.355s.
Training steps per second: 157.3.
Step 65000; q_loss 0.03789; mean_q -19.4; min_q -38.41; max_q -0.8117; mean_r -0.7775; mean_dsc 0.98; 
Tested 50 episodes at step 65000, reward mean -25.2, std 19.3, time cost 1.306s.
Training steps per second: 159.6.
Step 66000; q_loss 0.01701; mean_q -20.02; min_q -38.49; max_q -0.8016; mean_r -0.7877; mean_dsc 0.98; 
Tested 50 episodes at step 66000, reward mean -32.38, std 18.67, time cost 1.282s.
Training steps per second: 161.1.
Step 67000; q_loss 0.02041; mean_q -20.3; min_q -38.82; max_q -0.801; mean_r -0.807; mean_dsc 0.98; 
Tested 50 episodes at step 67000, reward mean -29.8, std 20.53, time cost 1.305s.
Training steps per second: 160.4.
Step 68000; q_loss 0.01488; mean_q -22.44; min_q -39.02; max_q -0.7902; mean_r -0.8742; mean_dsc 0.98; 
Tested 50 episodes at step 68000, reward mean -25.92, std 18.81, time cost 1.291s.
Training steps per second: 158.2.
Step 69000; q_loss 0.0425; mean_q -18.19; min_q -39.64; max_q -0.7793; mean_r -0.73; mean_dsc 0.98; 
Tested 50 episodes at step 69000, reward mean -28.56, std 18.99, time cost 1.287s.
Training steps per second: 158.3.
Step 70000; q_loss 0.01199; mean_q -18.76; min_q -39.61; max_q -0.7576; mean_r -0.7727; mean_dsc 0.98; 
Tested 50 episodes at step 70000, reward mean -26.98, std 17.41, time cost 1.337s.
Training steps per second: 157.7.
Step 71000; q_loss 0.01668; mean_q -19.36; min_q -39.86; max_q -0.7492; mean_r -0.7494; mean_dsc 0.98; 
Tested 50 episodes at step 71000, reward mean -30.2, std 19.16, time cost 1.291s.
Training steps per second: 157.8.
Step 72000; q_loss 0.03831; mean_q -19.5; min_q -40.05; max_q -0.729; mean_r -0.7799; mean_dsc 0.98; 
Tested 50 episodes at step 72000, reward mean -30.46, std 18.53, time cost 1.3s.
Training steps per second: 159.3.
Step 73000; q_loss 0.03856; mean_q -19.43; min_q -40.4; max_q -0.7224; mean_r -0.8024; mean_dsc 0.98; 
Tested 50 episodes at step 73000, reward mean -30.66, std 19.1, time cost 1.287s.
Training steps per second: 157.8.
Step 74000; q_loss 0.03639; mean_q -20.2; min_q -40.56; max_q -0.7143; mean_r -0.812; mean_dsc 0.98; 
Tested 50 episodes at step 74000, reward mean -26.94, std 19.03, time cost 1.278s.
Training steps per second: 160.2.
Step 75000; q_loss 0.05726; mean_q -17.74; min_q -40.84; max_q -0.7039; mean_r -0.78; mean_dsc 0.98; 
Tested 50 episodes at step 75000, reward mean -30.2, std 18.27, time cost 1.536s.
Training steps per second: 138.8.
Step 76000; q_loss 0.04264; mean_q -19.18; min_q -40.98; max_q -0.6865; mean_r -0.775; mean_dsc 0.98; 
Tested 50 episodes at step 76000, reward mean -30.42, std 18.71, time cost 1.304s.
Training steps per second: 158.7.
Step 77000; q_loss 0.01978; mean_q -19.07; min_q -41.64; max_q -0.6849; mean_r -0.7896; mean_dsc 0.98; 
Tested 50 episodes at step 77000, reward mean -28.8, std 19.87, time cost 1.299s.
Training steps per second: 159.4.
Step 78000; q_loss 0.04966; mean_q -18.05; min_q -41.61; max_q -0.6701; mean_r -0.766; mean_dsc 0.98; 
Tested 50 episodes at step 78000, reward mean -32.46, std 20.91, time cost 1.28s.
Training steps per second: 159.1.
Step 79000; q_loss 0.04242; mean_q -19.74; min_q -41.84; max_q -0.6577; mean_r -0.8539; mean_dsc 0.98; 
Tested 50 episodes at step 79000, reward mean -30.46, std 18.52, time cost 1.303s.
Training steps per second: 157.9.
Step 80000; q_loss 0.0208; mean_q -19.37; min_q -42.08; max_q -0.6608; mean_r -0.7416; mean_dsc 0.98; 
Tested 50 episodes at step 80000, reward mean -32.86, std 18.88, time cost 1.282s.
Training steps per second: 158.6.
Step 81000; q_loss 0.02182; mean_q -19.46; min_q -42.6; max_q -0.6171; mean_r -0.7982; mean_dsc 0.98; 
Tested 50 episodes at step 81000, reward mean -28.54, std 18.98, time cost 1.289s.
Training steps per second: 154.4.
Step 82000; q_loss 0.03521; mean_q -20.28; min_q -42.43; max_q -0.5991; mean_r -0.7939; mean_dsc 0.98; 
Tested 50 episodes at step 82000, reward mean -30.74, std 19.09, time cost 1.299s.
Training steps per second: 158.3.
Step 83000; q_loss 0.02871; mean_q -20.18; min_q -43.03; max_q -0.585; mean_r -0.817; mean_dsc 0.98; 
Tested 50 episodes at step 83000, reward mean -27.78, std 17.9, time cost 1.309s.
Training steps per second: 156.8.
Step 84000; q_loss 0.01674; mean_q -20.29; min_q -43.24; max_q -0.5678; mean_r -0.7792; mean_dsc 0.98; 
Tested 50 episodes at step 84000, reward mean -31.96, std 19.53, time cost 1.325s.
Training steps per second: 155.5.
Step 85000; q_loss 0.03033; mean_q -21.45; min_q -43.21; max_q -0.5362; mean_r -0.7889; mean_dsc 0.98; 
Tested 50 episodes at step 85000, reward mean -27.58, std 18.58, time cost 1.296s.
Training steps per second: 157.7.
Step 86000; q_loss 0.02518; mean_q -21.13; min_q -43.67; max_q -0.5231; mean_r -0.7768; mean_dsc 0.98; 
Tested 50 episodes at step 86000, reward mean -27.84, std 19.02, time cost 1.3s.
Training steps per second: 159.9.
Step 87000; q_loss 0.03676; mean_q -19.61; min_q -43.77; max_q -0.5259; mean_r -0.7709; mean_dsc 0.98; 
Tested 50 episodes at step 87000, reward mean -28.68, std 19.96, time cost 1.334s.
Training steps per second: 159.5.
Step 88000; q_loss 0.03145; mean_q -21.77; min_q -43.84; max_q -0.5227; mean_r -0.8517; mean_dsc 0.98; 
Tested 50 episodes at step 88000, reward mean -22.56, std 17.73, time cost 1.335s.
Training steps per second: 158.3.
Step 89000; q_loss 0.0667; mean_q -18.68; min_q -43.66; max_q -0.5127; mean_r -0.7174; mean_dsc 0.98; 
Tested 50 episodes at step 89000, reward mean -32.64, std 19.42, time cost 1.29s.
Training steps per second: 158.3.
Step 90000; q_loss 0.06749; mean_q -19.54; min_q -44.04; max_q -0.499; mean_r -0.7867; mean_dsc 0.98; 
Tested 50 episodes at step 90000, reward mean -33.94, std 17.08, time cost 1.293s.
Training steps per second: 159.7.
Step 91000; q_loss 0.03237; mean_q -18.54; min_q -44.11; max_q -0.4898; mean_r -0.7749; mean_dsc 0.98; 
Tested 50 episodes at step 91000, reward mean -33.12, std 18.1, time cost 1.305s.
Training steps per second: 158.2.
Step 92000; q_loss 0.02261; mean_q -21.32; min_q -44.42; max_q -0.4791; mean_r -0.8191; mean_dsc 0.98; 
Tested 50 episodes at step 92000, reward mean -28.6, std 17, time cost 1.301s.
Training steps per second: 156.6.
Step 93000; q_loss 0.01979; mean_q -17.89; min_q -44.18; max_q -0.4471; mean_r -0.6957; mean_dsc 0.98; 
Tested 50 episodes at step 93000, reward mean -27.74, std 17.79, time cost 1.259s.
Training steps per second: 159.9.
Step 94000; q_loss 0.04845; mean_q -22.33; min_q -44.6; max_q -0.4334; mean_r -0.8118; mean_dsc 0.98; 
Tested 50 episodes at step 94000, reward mean -29.16, std 18.85, time cost 1.281s.
Training steps per second: 158.9.
Step 95000; q_loss 0.03526; mean_q -19.18; min_q -45.18; max_q -0.4158; mean_r -0.7702; mean_dsc 0.98; 
Tested 50 episodes at step 95000, reward mean -26.24, std 19.17, time cost 1.318s.
Training steps per second: 159.5.
Step 96000; q_loss 0.01423; mean_q -20.75; min_q -45.33; max_q -0.4137; mean_r -0.7846; mean_dsc 0.98; 
Tested 50 episodes at step 96000, reward mean -31.36, std 18.26, time cost 1.261s.
Training steps per second: 161.6.
Step 97000; q_loss 0.03475; mean_q -17.94; min_q -45.4; max_q -0.3896; mean_r -0.6875; mean_dsc 0.98; 
Tested 50 episodes at step 97000, reward mean -30.36, std 17.81, time cost 1.304s.
Training steps per second: 158.3.
Step 98000; q_loss 0.01287; mean_q -22.6; min_q -45.16; max_q -0.3812; mean_r -0.8188; mean_dsc 0.98; 
Tested 50 episodes at step 98000, reward mean -27.62, std 19.35, time cost 1.28s.
Training steps per second: 158.8.
Step 99000; q_loss 0.02017; mean_q -19.43; min_q -45.61; max_q -0.3662; mean_r -0.7878; mean_dsc 0.98; 
Tested 50 episodes at step 99000, reward mean -30.14, std 18.15, time cost 1.3s.
Training steps per second: 159.8.
Step 100000; q_loss 0.02981; mean_q -18.49; min_q -45.65; max_q -0.3449; mean_r -0.7393; mean_dsc 0.98; 
Tested 50 episodes at step 100000, reward mean -28.32, std 17.73, time cost 1.265s.
Training steps per second: 162.1.
Step 101000; q_loss 0.006182; mean_q -17.55; min_q -45.29; max_q -0.3268; mean_r -0.7148; mean_dsc 0.98; 
Tested 50 episodes at step 101000, reward mean -31.42, std 18.58, time cost 1.267s.
Training steps per second: 161.1.
Step 102000; q_loss 0.02161; mean_q -19.03; min_q -45.85; max_q -0.3194; mean_r -0.75; mean_dsc 0.98; 
Tested 50 episodes at step 102000, reward mean -32.04, std 18.28, time cost 1.26s.
Training steps per second: 161.3.
Step 103000; q_loss 0.02947; mean_q -20.4; min_q -45.85; max_q -0.3044; mean_r -0.807; mean_dsc 0.98; 
Tested 50 episodes at step 103000, reward mean -30.64, std 17.85, time cost 1.32s.
Training steps per second: 160.
Step 104000; q_loss 0.01758; mean_q -21.14; min_q -46.14; max_q -0.2869; mean_r -0.8011; mean_dsc 0.98; 
Tested 50 episodes at step 104000, reward mean -24.42, std 18.55, time cost 1.244s.
Training steps per second: 160.9.
Step 105000; q_loss 0.01139; mean_q -22.75; min_q -46.27; max_q -0.2691; mean_r -0.8745; mean_dsc 0.98; 
Tested 50 episodes at step 105000, reward mean -29.74, std 17.7, time cost 1.263s.
Training steps per second: 161.5.
Step 106000; q_loss 0.02567; mean_q -20.33; min_q -46.27; max_q -0.2756; mean_r -0.8134; mean_dsc 0.98; 
Tested 50 episodes at step 106000, reward mean -28.5, std 18.11, time cost 1.257s.
Training steps per second: 154.9.
Step 107000; q_loss 0.02549; mean_q -17.78; min_q -46.59; max_q -0.2696; mean_r -0.7728; mean_dsc 0.98; 
Tested 50 episodes at step 107000, reward mean -28.96, std 17.24, time cost 1.238s.
Training steps per second: 162.
Step 108000; q_loss 0.01775; mean_q -19.61; min_q -46.33; max_q -0.2568; mean_r -0.8296; mean_dsc 0.98; 
Tested 50 episodes at step 108000, reward mean -23.3, std 17.78, time cost 1.329s.
Training steps per second: 159.4.
Step 109000; q_loss 0.009489; mean_q -18.87; min_q -46.84; max_q -0.2603; mean_r -0.7549; mean_dsc 0.98; 
Tested 50 episodes at step 109000, reward mean -30.84, std 19.3, time cost 1.234s.
Training steps per second: 162.9.
Step 110000; q_loss 0.01166; mean_q -20.26; min_q -46.16; max_q -0.2601; mean_r -0.7661; mean_dsc 0.98; 
Tested 50 episodes at step 110000, reward mean -30.52, std 18.82, time cost 1.231s.
Training steps per second: 162.4.
Step 111000; q_loss 0.01329; mean_q -22.44; min_q -46.87; max_q -0.2742; mean_r -0.8476; mean_dsc 0.98; 
Tested 50 episodes at step 111000, reward mean -26.08, std 17.76, time cost 1.295s.
Training steps per second: 159.3.
Step 112000; q_loss 0.03819; mean_q -19.97; min_q -46.94; max_q -0.2547; mean_r -0.8178; mean_dsc 0.98; 
Tested 50 episodes at step 112000, reward mean -29.5, std 18.7, time cost 1.288s.
Training steps per second: 159.5.
Step 113000; q_loss 0.01115; mean_q -17.18; min_q -45.89; max_q -0.2495; mean_r -0.7054; mean_dsc 0.98; 
Tested 50 episodes at step 113000, reward mean -25.64, std 19.57, time cost 1.315s.
Training steps per second: 160.2.
Step 114000; q_loss 0.01786; mean_q -17.97; min_q -47.06; max_q -0.2476; mean_r -0.7793; mean_dsc 0.98; 
Tested 50 episodes at step 114000, reward mean -31.28, std 17.58, time cost 1.303s.
Training steps per second: 160.8.
Step 115000; q_loss 0.0142; mean_q -19.25; min_q -47.52; max_q -0.2325; mean_r -0.8025; mean_dsc 0.98; 
Tested 50 episodes at step 115000, reward mean -30.52, std 18.56, time cost 1.259s.
Training steps per second: 162.7.
Step 116000; q_loss 0.02443; mean_q -20.49; min_q -47; max_q -0.2126; mean_r -0.7487; mean_dsc 0.98; 
Tested 50 episodes at step 116000, reward mean -31.58, std 16.82, time cost 1.278s.
Training steps per second: 162.4.
Step 117000; q_loss 0.01314; mean_q -20.52; min_q -46.93; max_q -0.2128; mean_r -0.8273; mean_dsc 0.98; 
Tested 50 episodes at step 117000, reward mean -32.1, std 17.09, time cost 1.28s.
Training steps per second: 159.2.
Step 118000; q_loss 0.009033; mean_q -18.36; min_q -46.89; max_q -0.2132; mean_r -0.7739; mean_dsc 0.98; 
Tested 50 episodes at step 118000, reward mean -28.66, std 17.69, time cost 1.296s.
Training steps per second: 159.1.
Step 119000; q_loss 0.02274; mean_q -17.98; min_q -46.98; max_q -0.2064; mean_r -0.7436; mean_dsc 0.98; 
Tested 50 episodes at step 119000, reward mean -31.44, std 18.42, time cost 1.303s.
Training steps per second: 159.6.
Step 120000; q_loss 0.007783; mean_q -20.89; min_q -46.25; max_q -0.2039; mean_r -0.7794; mean_dsc 0.98; 
Tested 50 episodes at step 120000, reward mean -26.08, std 18.69, time cost 1.263s.
Training steps per second: 160.
Step 121000; q_loss 0.005794; mean_q -16.39; min_q -44.51; max_q -0.1833; mean_r -0.7748; mean_dsc 0.98; 
Tested 50 episodes at step 121000, reward mean -30.2, std 18.25, time cost 1.325s.
Training steps per second: 158.6.
Step 122000; q_loss 0.01064; mean_q -19.44; min_q -47.24; max_q -0.1682; mean_r -0.81; mean_dsc 0.98; 
Tested 50 episodes at step 122000, reward mean -28.5, std 18.9, time cost 1.253s.
Training steps per second: 159.2.
Step 123000; q_loss 0.008274; mean_q -16.94; min_q -45.53; max_q -0.1657; mean_r -0.7085; mean_dsc 0.98; 
Tested 50 episodes at step 123000, reward mean -26.9, std 18.67, time cost 1.266s.
Training steps per second: 160.2.
Step 124000; q_loss 0.02058; mean_q -17.09; min_q -46.39; max_q -0.1579; mean_r -0.7825; mean_dsc 0.98; 
Tested 50 episodes at step 124000, reward mean -26.04, std 20.16, time cost 1.284s.
Training steps per second: 162.3.
Step 125000; q_loss 0.007749; mean_q -21.25; min_q -43.93; max_q -0.1426; mean_r -0.8445; mean_dsc 0.98; 
Tested 50 episodes at step 125000, reward mean -26.66, std 19.22, time cost 1.267s.
Training steps per second: 144.2.
Step 126000; q_loss 0.005832; mean_q -19.25; min_q -42.76; max_q -0.1368; mean_r -0.7906; mean_dsc 0.98; 
Tested 50 episodes at step 126000, reward mean -27.88, std 19.21, time cost 1.462s.
Training steps per second: 154.2.
Step 127000; q_loss 0.01231; mean_q -19.47; min_q -44.66; max_q -0.1233; mean_r -0.8652; mean_dsc 0.98; 
Tested 50 episodes at step 127000, reward mean -28.58, std 17.85, time cost 1.265s.
Training steps per second: 160.7.
Step 128000; q_loss 0.009872; mean_q -17.13; min_q -45.63; max_q -0.1149; mean_r -0.7756; mean_dsc 0.98; 
Tested 50 episodes at step 128000, reward mean -27.08, std 17.99, time cost 1.241s.
Training steps per second: 162.1.
Step 129000; q_loss 0.04444; mean_q -18.19; min_q -44.59; max_q -0.1092; mean_r -0.7413; mean_dsc 0.98; 
Tested 50 episodes at step 129000, reward mean -25.12, std 19.32, time cost 1.26s.
Training steps per second: 161.6.
Step 130000; q_loss 0.005993; mean_q -16.19; min_q -41.81; max_q -0.09042; mean_r -0.7361; mean_dsc 0.98; 
Tested 50 episodes at step 130000, reward mean -30.94, std 19.51, time cost 1.268s.
Training steps per second: 162.3.
Step 131000; q_loss 0.009792; mean_q -16.63; min_q -43.36; max_q -0.09679; mean_r -0.7743; mean_dsc 0.98; 
Tested 50 episodes at step 131000, reward mean -29.94, std 19.66, time cost 1.276s.
Training steps per second: 161.6.
Step 132000; q_loss 0.003129; mean_q -18; min_q -44.2; max_q -0.09685; mean_r -0.7671; mean_dsc 0.98; 
Tested 50 episodes at step 132000, reward mean -29.22, std 17.02, time cost 1.259s.
Training steps per second: 159.3.
Step 133000; q_loss 0.0189; mean_q -18.15; min_q -42.42; max_q -0.09218; mean_r -0.7618; mean_dsc 0.98; 
Tested 50 episodes at step 133000, reward mean -26.22, std 19.37, time cost 1.255s.
Training steps per second: 159.9.
Step 134000; q_loss 0.006214; mean_q -16.04; min_q -41.48; max_q -0.09526; mean_r -0.7282; mean_dsc 0.98; 
Tested 50 episodes at step 134000, reward mean -26.92, std 18.1, time cost 1.264s.
Training steps per second: 159.7.
Step 135000; q_loss 0.003842; mean_q -18.43; min_q -42.16; max_q -0.08006; mean_r -0.7444; mean_dsc 0.98; 
Tested 50 episodes at step 135000, reward mean -29.88, std 19.12, time cost 1.299s.
Training steps per second: 160.4.
Step 136000; q_loss 0.004447; mean_q -17.61; min_q -42.1; max_q -0.08377; mean_r -0.7534; mean_dsc 0.98; 
Tested 50 episodes at step 136000, reward mean -28.8, std 18.83, time cost 1.281s.
Training steps per second: 160.9.
Step 137000; q_loss 0.009278; mean_q -17.46; min_q -42.91; max_q -0.08007; mean_r -0.738; mean_dsc 0.98; 
Tested 50 episodes at step 137000, reward mean -33.8, std 16.12, time cost 1.258s.
Training steps per second: 159.5.
Step 138000; q_loss 0.003685; mean_q -18.23; min_q -42.43; max_q -0.08908; mean_r -0.7742; mean_dsc 0.98; 
Tested 50 episodes at step 138000, reward mean -26.46, std 19.65, time cost 1.268s.
Training steps per second: 159.6.
Step 139000; q_loss 0.002929; mean_q -15.75; min_q -43.12; max_q -0.08147; mean_r -0.655; mean_dsc 0.98; 
Tested 50 episodes at step 139000, reward mean -28.68, std 18.09, time cost 1.307s.
Training steps per second: 153.1.
Step 140000; q_loss 0.007809; mean_q -18.38; min_q -42.71; max_q -0.06946; mean_r -0.79; mean_dsc 0.98; 
Tested 50 episodes at step 140000, reward mean -31.38, std 17.86, time cost 1.241s.
Training steps per second: 160.8.
Step 141000; q_loss 0.005112; mean_q -17.55; min_q -41.44; max_q -0.07289; mean_r -0.7753; mean_dsc 0.98; 
Tested 50 episodes at step 141000, reward mean -26.22, std 19.89, time cost 1.279s.
Training steps per second: 159.7.
Step 142000; q_loss 0.006469; mean_q -15.91; min_q -42.85; max_q -0.06983; mean_r -0.6882; mean_dsc 0.98; 
Tested 50 episodes at step 142000, reward mean -30.14, std 20.07, time cost 1.289s.
Training steps per second: 158.4.
Step 143000; q_loss 0.004074; mean_q -17.2; min_q -42.06; max_q -0.07033; mean_r -0.7645; mean_dsc 0.98; 
Tested 50 episodes at step 143000, reward mean -27.84, std 18.99, time cost 1.266s.
Training steps per second: 160.
Step 144000; q_loss 0.006778; mean_q -15.88; min_q -41.29; max_q -0.06355; mean_r -0.7137; mean_dsc 0.98; 
Tested 50 episodes at step 144000, reward mean -29.94, std 16.64, time cost 1.26s.
Training steps per second: 161.
Step 145000; q_loss 0.004928; mean_q -16.26; min_q -42.5; max_q -0.06081; mean_r -0.7359; mean_dsc 0.98; 
Tested 50 episodes at step 145000, reward mean -32.34, std 17.79, time cost 1.271s.
Training steps per second: 160.3.
Step 146000; q_loss 0.00387; mean_q -18.57; min_q -42.72; max_q -0.05801; mean_r -0.772; mean_dsc 0.98; 
Tested 50 episodes at step 146000, reward mean -27.72, std 19.65, time cost 1.285s.
Training steps per second: 159.2.
Step 147000; q_loss 0.002392; mean_q -16.29; min_q -41.94; max_q -0.07129; mean_r -0.7392; mean_dsc 0.98; 
Tested 50 episodes at step 147000, reward mean -30.9, std 17.72, time cost 1.308s.
Training steps per second: 158.5.
Step 148000; q_loss 0.01344; mean_q -18.12; min_q -42.25; max_q -0.0715; mean_r -0.7607; mean_dsc 0.98; 
Tested 50 episodes at step 148000, reward mean -25.38, std 18.81, time cost 1.275s.
Training steps per second: 159.8.
Step 149000; q_loss 0.00643; mean_q -18.8; min_q -43.15; max_q -0.07174; mean_r -0.7858; mean_dsc 0.98; 
Tested 50 episodes at step 149000, reward mean -24.98, std 18.98, time cost 1.292s.
Training steps per second: 159.5.
Step 150000; q_loss 0.006615; mean_q -17.03; min_q -42.59; max_q -0.06319; mean_r -0.7776; mean_dsc 0.98; 
Tested 50 episodes at step 150000, reward mean -27.9, std 19.66, time cost 1.282s.
Training steps per second: 160.2.
Step 151000; q_loss 0.009095; mean_q -16.76; min_q -42.67; max_q -0.05992; mean_r -0.7282; mean_dsc 0.98; 
Tested 50 episodes at step 151000, reward mean -28.64, std 19.02, time cost 1.278s.
Training steps per second: 160.
Step 152000; q_loss 0.002214; mean_q -16.3; min_q -42.63; max_q -0.06129; mean_r -0.7671; mean_dsc 0.98; 
Tested 50 episodes at step 152000, reward mean -26.08, std 19.31, time cost 1.28s.
Training steps per second: 160.6.
Step 153000; q_loss 0.01237; mean_q -16.47; min_q -43.06; max_q -0.04946; mean_r -0.7377; mean_dsc 0.98; 
Tested 50 episodes at step 153000, reward mean -26.74, std 18.95, time cost 1.263s.
Training steps per second: 161.3.
Step 154000; q_loss 0.004659; mean_q -19.23; min_q -42.31; max_q -0.05361; mean_r -0.7965; mean_dsc 0.98; 
Tested 50 episodes at step 154000, reward mean -30.28, std 18.99, time cost 1.286s.
Training steps per second: 160.2.
Step 155000; q_loss 0.003399; mean_q -16; min_q -42.33; max_q -0.05355; mean_r -0.6959; mean_dsc 0.98; 
Tested 50 episodes at step 155000, reward mean -29.76, std 18.21, time cost 1.271s.
Training steps per second: 159.6.
Step 156000; q_loss 0.004142; mean_q -15.07; min_q -40.89; max_q -0.04708; mean_r -0.7546; mean_dsc 0.98; 
Tested 50 episodes at step 156000, reward mean -33.24, std 17.39, time cost 1.254s.
Training steps per second: 159.6.
Step 157000; q_loss 0.004896; mean_q -19.48; min_q -42.36; max_q -0.0474; mean_r -0.8005; mean_dsc 0.98; 
Tested 50 episodes at step 157000, reward mean -32.1, std 17.18, time cost 1.285s.
Training steps per second: 161.
Step 158000; q_loss 0.005022; mean_q -14.55; min_q -41.62; max_q -0.04702; mean_r -0.6615; mean_dsc 0.98; 
Tested 50 episodes at step 158000, reward mean -27.72, std 18.68, time cost 1.296s.
Training steps per second: 160.4.
Step 159000; q_loss 0.005789; mean_q -16.71; min_q -42.36; max_q -0.0441; mean_r -0.7733; mean_dsc 0.98; 
Tested 50 episodes at step 159000, reward mean -26.8, std 17.77, time cost 1.255s.
Training steps per second: 160.9.
Step 160000; q_loss 0.006276; mean_q -18.09; min_q -42.36; max_q -0.04552; mean_r -0.7852; mean_dsc 0.98; 
Tested 50 episodes at step 160000, reward mean -28.1, std 19.25, time cost 1.255s.
Training steps per second: 159.7.
Step 161000; q_loss 0.003282; mean_q -19.07; min_q -42.34; max_q -0.04272; mean_r -0.8067; mean_dsc 0.98; 
Tested 50 episodes at step 161000, reward mean -26.88, std 19.42, time cost 1.291s.
Training steps per second: 157.7.
Step 162000; q_loss 0.00436; mean_q -15.54; min_q -42.71; max_q -0.0412; mean_r -0.7119; mean_dsc 0.98; 
Tested 50 episodes at step 162000, reward mean -30.9, std 18.77, time cost 1.328s.
Training steps per second: 158.9.
Step 163000; q_loss 0.003992; mean_q -15.94; min_q -43.37; max_q -0.04067; mean_r -0.7363; mean_dsc 0.98; 
Tested 50 episodes at step 163000, reward mean -25.88, std 17.89, time cost 1.272s.
Training steps per second: 160.3.
Step 164000; q_loss 0.003515; mean_q -18.41; min_q -41.65; max_q -0.0433; mean_r -0.7891; mean_dsc 0.98; 
Tested 50 episodes at step 164000, reward mean -28.96, std 18.09, time cost 1.278s.
Training steps per second: 159.6.
Step 165000; q_loss 0.002283; mean_q -15.36; min_q -41.92; max_q -0.03508; mean_r -0.6895; mean_dsc 0.98; 
Tested 50 episodes at step 165000, reward mean -28.4, std 18.91, time cost 1.345s.
Training steps per second: 156.9.
Step 166000; q_loss 0.003987; mean_q -16.9; min_q -41.92; max_q -0.035; mean_r -0.7956; mean_dsc 0.98; 
Tested 50 episodes at step 166000, reward mean -31.74, std 17.73, time cost 1.254s.
Training steps per second: 161.5.
Step 167000; q_loss 0.002242; mean_q -18.98; min_q -42.61; max_q -0.04048; mean_r -0.8064; mean_dsc 0.98; 
Tested 50 episodes at step 167000, reward mean -28, std 19.33, time cost 1.307s.
Training steps per second: 160.7.
Step 168000; q_loss 0.008708; mean_q -17.26; min_q -41.6; max_q -0.04008; mean_r -0.7796; mean_dsc 0.98; 
Tested 50 episodes at step 168000, reward mean -33.4, std 17.33, time cost 1.286s.
Training steps per second: 162.5.
Step 169000; q_loss 0.0142; mean_q -17.93; min_q -42.24; max_q -0.03843; mean_r -0.7533; mean_dsc 0.98; 
Tested 50 episodes at step 169000, reward mean -26.4, std 19.14, time cost 1.348s.
Training steps per second: 159.4.
Step 170000; q_loss 0.005091; mean_q -17.05; min_q -43.04; max_q -0.03618; mean_r -0.7437; mean_dsc 0.98; 
Tested 50 episodes at step 170000, reward mean -31.88, std 18.81, time cost 1.249s.
Training steps per second: 161.
Step 171000; q_loss 0.004832; mean_q -20.25; min_q -41.88; max_q -0.04453; mean_r -0.8022; mean_dsc 0.98; 
Tested 50 episodes at step 171000, reward mean -29.42, std 19.69, time cost 1.275s.
Training steps per second: 158.9.
Step 172000; q_loss 0.005843; mean_q -15.59; min_q -41.85; max_q -0.04757; mean_r -0.7678; mean_dsc 0.98; 
Tested 50 episodes at step 172000, reward mean -32.44, std 18.54, time cost 1.262s.
Training steps per second: 161.5.
Step 173000; q_loss 0.003198; mean_q -16.01; min_q -42.55; max_q -0.03798; mean_r -0.714; mean_dsc 0.98; 
Tested 50 episodes at step 173000, reward mean -25.78, std 20.19, time cost 1.279s.
Training steps per second: 159.7.
Step 174000; q_loss 0.009447; mean_q -17.43; min_q -42.51; max_q -0.03292; mean_r -0.7751; mean_dsc 0.98; 
Tested 50 episodes at step 174000, reward mean -29.16, std 18.57, time cost 1.253s.
Training steps per second: 161.6.
Step 175000; q_loss 0.002194; mean_q -17.65; min_q -42.52; max_q -0.03545; mean_r -0.7773; mean_dsc 0.98; 
Tested 50 episodes at step 175000, reward mean -32.42, std 17.85, time cost 1.33s.
Training steps per second: 160.
Step 176000; q_loss 0.002668; mean_q -16.75; min_q -41.84; max_q -0.03902; mean_r -0.783; mean_dsc 0.98; 
Tested 50 episodes at step 176000, reward mean -31.34, std 18.65, time cost 1.264s.
Training steps per second: 160.3.
Step 177000; q_loss 0.004418; mean_q -15.24; min_q -43.1; max_q -0.02868; mean_r -0.6814; mean_dsc 0.98; 
Tested 50 episodes at step 177000, reward mean -30.9, std 19.25, time cost 1.278s.
Training steps per second: 144.7.
Step 178000; q_loss 0.004357; mean_q -15.99; min_q -42.14; max_q -0.0329; mean_r -0.7696; mean_dsc 0.98; 
Tested 50 episodes at step 178000, reward mean -31.7, std 17.79, time cost 1.292s.
Training steps per second: 160.1.
Step 179000; q_loss 0.002288; mean_q -17.22; min_q -42.4; max_q -0.02803; mean_r -0.7823; mean_dsc 0.98; 
Tested 50 episodes at step 179000, reward mean -27.22, std 18.49, time cost 1.294s.
Training steps per second: 161.2.
Step 180000; q_loss 0.008797; mean_q -16.69; min_q -42.44; max_q -0.02617; mean_r -0.7612; mean_dsc 0.98; 
Tested 50 episodes at step 180000, reward mean -34.64, std 16.65, time cost 1.275s.
Training steps per second: 148.6.
Step 181000; q_loss 0.002953; mean_q -17.55; min_q -42.52; max_q -0.01931; mean_r -0.7419; mean_dsc 0.98; 
Tested 50 episodes at step 181000, reward mean -31.96, std 18.87, time cost 1.341s.
Training steps per second: 157.9.
Step 182000; q_loss 0.002963; mean_q -17.48; min_q -42.8; max_q -0.01772; mean_r -0.7236; mean_dsc 0.98; 
Tested 50 episodes at step 182000, reward mean -28.04, std 17.97, time cost 1.252s.
Training steps per second: 162.1.
Step 183000; q_loss 0.005037; mean_q -15.06; min_q -43.44; max_q -0.02378; mean_r -0.6876; mean_dsc 0.98; 
Tested 50 episodes at step 183000, reward mean -30.26, std 18.66, time cost 1.308s.
Training steps per second: 158.7.
Step 184000; q_loss 0.009114; mean_q -15.56; min_q -41.79; max_q -0.02303; mean_r -0.6634; mean_dsc 0.98; 
Tested 50 episodes at step 184000, reward mean -28.86, std 18.17, time cost 1.29s.
Training steps per second: 158.6.
Step 185000; q_loss 0.00143; mean_q -16.49; min_q -42.45; max_q -0.01723; mean_r -0.6962; mean_dsc 0.98; 
Tested 50 episodes at step 185000, reward mean -30.06, std 17.49, time cost 1.288s.
Training steps per second: 161.
Step 186000; q_loss 0.001745; mean_q -16.5; min_q -41.09; max_q -0.01596; mean_r -0.7446; mean_dsc 0.98; 
Tested 50 episodes at step 186000, reward mean -31.96, std 19.36, time cost 1.362s.
Training steps per second: 157.3.
Step 187000; q_loss 0.008076; mean_q -17.85; min_q -42.42; max_q -0.01593; mean_r -0.7896; mean_dsc 0.98; 
Tested 50 episodes at step 187000, reward mean -35.04, std 17.75, time cost 1.264s.
Training steps per second: 159.7.
Step 188000; q_loss 0.01361; mean_q -18.94; min_q -42.08; max_q -0.007087; mean_r -0.8456; mean_dsc 0.98; 
Tested 50 episodes at step 188000, reward mean -28.64, std 18.24, time cost 1.278s.
Training steps per second: 159.
Step 189000; q_loss 0.002503; mean_q -17.39; min_q -41.13; max_q -0.004391; mean_r -0.7787; mean_dsc 0.98; 
Tested 50 episodes at step 189000, reward mean -30.8, std 18.41, time cost 1.29s.
Training steps per second: 157.9.
Step 190000; q_loss 0.002535; mean_q -14.81; min_q -41.4; max_q -0.0051; mean_r -0.7479; mean_dsc 0.98; 
Tested 50 episodes at step 190000, reward mean -29.24, std 18.48, time cost 1.296s.
Training steps per second: 156.8.
Step 191000; q_loss 0.006922; mean_q -15.66; min_q -41.42; max_q -0.002405; mean_r -0.7354; mean_dsc 0.98; 
Tested 50 episodes at step 191000, reward mean -26.58, std 20.3, time cost 1.269s.
Training steps per second: 159.5.
Step 192000; q_loss 0.001806; mean_q -17.32; min_q -43.13; max_q -0.001693; mean_r -0.7951; mean_dsc 0.98; 
Tested 50 episodes at step 192000, reward mean -29.96, std 18.75, time cost 1.271s.
Training steps per second: 160.6.
Step 193000; q_loss 0.002297; mean_q -15.02; min_q -42.82; max_q -0.005183; mean_r -0.715; mean_dsc 0.98; 
Tested 50 episodes at step 193000, reward mean -29.36, std 20.57, time cost 1.246s.
Training steps per second: 161.9.
Step 194000; q_loss 0.002551; mean_q -15.38; min_q -42.88; max_q -0.004734; mean_r -0.7191; mean_dsc 0.98; 
Tested 50 episodes at step 194000, reward mean -30.22, std 17.97, time cost 1.298s.
Training steps per second: 160.6.
Step 195000; q_loss 0.003306; mean_q -15.99; min_q -42.78; max_q -0.001941; mean_r -0.7152; mean_dsc 0.98; 
Tested 50 episodes at step 195000, reward mean -32.46, std 18.27, time cost 1.268s.
Training steps per second: 160.1.
Step 196000; q_loss 0.001713; mean_q -16.19; min_q -42.11; max_q -0.002559; mean_r -0.7053; mean_dsc 0.98; 
Tested 50 episodes at step 196000, reward mean -32.7, std 17.38, time cost 1.3s.
Training steps per second: 160.6.
Step 197000; q_loss 0.002947; mean_q -16.37; min_q -42.45; max_q 0.003884; mean_r -0.7289; mean_dsc 0.98; 
Tested 50 episodes at step 197000, reward mean -31.22, std 17.79, time cost 1.285s.
Training steps per second: 161.3.
Step 198000; q_loss 0.002425; mean_q -15.88; min_q -42.44; max_q 0.01187; mean_r -0.7338; mean_dsc 0.98; 
Tested 50 episodes at step 198000, reward mean -27.92, std 18.51, time cost 1.282s.
Training steps per second: 160.5.
Step 199000; q_loss 0.007975; mean_q -15.76; min_q -42.44; max_q 0.01706; mean_r -0.7612; mean_dsc 0.98; 
Tested 50 episodes at step 199000, reward mean -31.48, std 18.68, time cost 1.282s.
Training steps per second: 161.4.
Step 200000; q_loss 0.002565; mean_q -17.41; min_q -41.78; max_q 0.02068; mean_r -0.8163; mean_dsc 0.98; 
Tested 50 episodes at step 200000, reward mean -29.42, std 18.67, time cost 1.222s.
