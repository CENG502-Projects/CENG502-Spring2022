device: cuda.
Representation model loaded from /content/drive/MyDrive/Laplacian/laplacian_code_Wu_run2_maze/log/laprepr/HardMaze/test/model.ckpt.
Start collecting transitions.
(10000/10000) steps collected.
Replay buffer initialization finished, time cost: 1.5043182373046875s
Training steps per second: 0.
Step 1; q_loss 0.9936; mean_q -0.9905; min_q -1.235; max_q -0.5166; mean_r -1.029; mean_dsc 0.98; 
Tested 50 episodes at step 1, reward mean -48.2, std 8.82, time cost 1.299s.
Training steps per second: 173.7.
Step 1000; q_loss 0.002893; mean_q -1.595; min_q -1.884; max_q -0.336; mean_r -1.024; mean_dsc 0.98; 
Tested 50 episodes at step 1000, reward mean -45.56, std 13.37, time cost 1.341s.
Training steps per second: 171.9.
Step 2000; q_loss 0.003916; mean_q -2.327; min_q -2.798; max_q -0.4584; mean_r -1.009; mean_dsc 0.98; 
Tested 50 episodes at step 2000, reward mean -40.86, std 18.38, time cost 1.3s.
Training steps per second: 172.2.
Step 3000; q_loss 0.00264; mean_q -3.111; min_q -3.82; max_q -0.4292; mean_r -1.003; mean_dsc 0.98; 
Tested 50 episodes at step 3000, reward mean -42.06, std 16.99, time cost 1.32s.
Training steps per second: 170.8.
Step 4000; q_loss 0.002065; mean_q -3.866; min_q -4.84; max_q -0.3489; mean_r -0.9932; mean_dsc 0.98; 
Tested 50 episodes at step 4000, reward mean -42.1, std 16.92, time cost 1.326s.
Training steps per second: 170.7.
Step 5000; q_loss 0.005166; mean_q -4.901; min_q -5.9; max_q -0.2517; mean_r -1.042; mean_dsc 0.98; 
Tested 50 episodes at step 5000, reward mean -41.94, std 17.28, time cost 1.299s.
Training steps per second: 172.4.
Step 6000; q_loss 0.003817; mean_q -5.767; min_q -6.593; max_q -0.195; mean_r -1.049; mean_dsc 0.98; 
Tested 50 episodes at step 6000, reward mean -41.58, std 16.9, time cost 1.318s.
Training steps per second: 172.7.
Step 7000; q_loss 0.008146; mean_q -6.162; min_q -7.914; max_q -0.1627; mean_r -0.9881; mean_dsc 0.98; 
Tested 50 episodes at step 7000, reward mean -42.28, std 16.53, time cost 1.351s.
Training steps per second: 171.3.
Step 8000; q_loss 0.005976; mean_q -6.518; min_q -8.831; max_q -0.1526; mean_r -0.9508; mean_dsc 0.98; 
Tested 50 episodes at step 8000, reward mean -43.9, std 15.17, time cost 1.315s.
Training steps per second: 170.6.
Step 9000; q_loss 0.01099; mean_q -7.178; min_q -9.743; max_q -0.1297; mean_r -0.9688; mean_dsc 0.98; 
Tested 50 episodes at step 9000, reward mean -38.8, std 19.06, time cost 1.3s.
Training steps per second: 165.7.
Step 10000; q_loss 0.01591; mean_q -7.916; min_q -10.84; max_q -0.1204; mean_r -0.9611; mean_dsc 0.98; 
Tested 50 episodes at step 10000, reward mean -38.62, std 19.32, time cost 1.5s.
Training steps per second: 156.5.
Step 11000; q_loss 0.02807; mean_q -7.991; min_q -11.76; max_q -0.09886; mean_r -0.9183; mean_dsc 0.98; 
Tested 50 episodes at step 11000, reward mean -40.32, std 18.32, time cost 1.715s.
Training steps per second: 148.9.
Step 12000; q_loss 0.0219; mean_q -8.852; min_q -12.4; max_q -0.06148; mean_r -0.9315; mean_dsc 0.98; 
Tested 50 episodes at step 12000, reward mean -41.86, std 17.45, time cost 1.375s.
Training steps per second: 163.5.
Step 13000; q_loss 0.01071; mean_q -9.167; min_q -13.5; max_q -0.02943; mean_r -0.9032; mean_dsc 0.98; 
Tested 50 episodes at step 13000, reward mean -37.5, std 19.17, time cost 1.292s.
Training steps per second: 172.2.
Step 14000; q_loss 0.03064; mean_q -9.496; min_q -14.45; max_q 0.03404; mean_r -0.8912; mean_dsc 0.98; 
Tested 50 episodes at step 14000, reward mean -33.94, std 20.72, time cost 1.315s.
Training steps per second: 168.5.
Step 15000; q_loss 0.02425; mean_q -10.32; min_q -14.25; max_q 0.0136; mean_r -0.9262; mean_dsc 0.98; 
Tested 50 episodes at step 15000, reward mean -37.46, std 18.41, time cost 1.289s.
Training steps per second: 172.8.
Step 16000; q_loss 0.007681; mean_q -9.693; min_q -15.29; max_q 0.01776; mean_r -0.8278; mean_dsc 0.98; 
Tested 50 episodes at step 16000, reward mean -36.46, std 19.02, time cost 1.418s.
Training steps per second: 167.5.
Step 17000; q_loss 0.02625; mean_q -11.14; min_q -16.88; max_q 0.06022; mean_r -0.9007; mean_dsc 0.98; 
Tested 50 episodes at step 17000, reward mean -39.06, std 17.66, time cost 1.298s.
Training steps per second: 169.6.
Step 18000; q_loss 0.009714; mean_q -11.16; min_q -17.37; max_q 0.0732; mean_r -0.8922; mean_dsc 0.98; 
Tested 50 episodes at step 18000, reward mean -37.92, std 18.59, time cost 1.357s.
Training steps per second: 164.6.
Step 19000; q_loss 0.01341; mean_q -10.35; min_q -17.59; max_q 0.06438; mean_r -0.8153; mean_dsc 0.98; 
Tested 50 episodes at step 19000, reward mean -42.9, std 15.36, time cost 1.316s.
Training steps per second: 163.3.
Step 20000; q_loss 0.0535; mean_q -12.71; min_q -17.97; max_q 0.05939; mean_r -0.9589; mean_dsc 0.98; 
Tested 50 episodes at step 20000, reward mean -35.7, std 19.32, time cost 1.326s.
Training steps per second: 170.8.
Step 21000; q_loss 0.01767; mean_q -12.51; min_q -19.53; max_q 0.0668; mean_r -0.8603; mean_dsc 0.98; 
Tested 50 episodes at step 21000, reward mean -35.64, std 20.24, time cost 1.282s.
Training steps per second: 162.6.
Step 22000; q_loss 0.02909; mean_q -12.43; min_q -20.52; max_q 0.06563; mean_r -0.8578; mean_dsc 0.98; 
Tested 50 episodes at step 22000, reward mean -29.82, std 20.63, time cost 1.281s.
Training steps per second: 172.2.
Step 23000; q_loss 0.02355; mean_q -12.21; min_q -20.72; max_q 0.07569; mean_r -0.8297; mean_dsc 0.98; 
Tested 50 episodes at step 23000, reward mean -36.42, std 19.11, time cost 1.303s.
Training steps per second: 171.9.
Step 24000; q_loss 0.05511; mean_q -13.58; min_q -20.79; max_q 0.08606; mean_r -0.882; mean_dsc 0.98; 
Tested 50 episodes at step 24000, reward mean -31.8, std 20.04, time cost 1.306s.
Training steps per second: 167.2.
Step 25000; q_loss 0.02485; mean_q -14.09; min_q -21.85; max_q 0.08955; mean_r -0.8723; mean_dsc 0.98; 
Tested 50 episodes at step 25000, reward mean -37.74, std 18.35, time cost 1.309s.
Training steps per second: 171.3.
Step 26000; q_loss 0.07763; mean_q -14.21; min_q -22.08; max_q 0.1004; mean_r -0.8903; mean_dsc 0.98; 
Tested 50 episodes at step 26000, reward mean -35.26, std 20.01, time cost 1.305s.
Training steps per second: 170.1.
Step 27000; q_loss 0.03536; mean_q -14.01; min_q -22.74; max_q 0.1127; mean_r -0.873; mean_dsc 0.98; 
Tested 50 episodes at step 27000, reward mean -36.36, std 19.3, time cost 1.331s.
Training steps per second: 169.1.
Step 28000; q_loss 0.05021; mean_q -15.03; min_q -23.31; max_q 0.1296; mean_r -0.8802; mean_dsc 0.98; 
Tested 50 episodes at step 28000, reward mean -35.12, std 18.78, time cost 1.389s.
Training steps per second: 168.4.
Step 29000; q_loss 0.1034; mean_q -15.41; min_q -24.39; max_q 0.1308; mean_r -0.8749; mean_dsc 0.98; 
Tested 50 episodes at step 29000, reward mean -31.56, std 20.54, time cost 1.264s.
Training steps per second: 162.1.
Step 30000; q_loss 0.02452; mean_q -15.08; min_q -24.84; max_q 0.1307; mean_r -0.9057; mean_dsc 0.98; 
Tested 50 episodes at step 30000, reward mean -33.78, std 19.82, time cost 1.284s.
Training steps per second: 173.1.
Step 31000; q_loss 0.25; mean_q -15.3; min_q -25.17; max_q 0.1374; mean_r -0.8571; mean_dsc 0.98; 
Tested 50 episodes at step 31000, reward mean -34.02, std 18.65, time cost 1.337s.
Training steps per second: 169.
Step 32000; q_loss 0.1129; mean_q -15; min_q -25.83; max_q 0.1459; mean_r -0.8281; mean_dsc 0.98; 
Tested 50 episodes at step 32000, reward mean -33.22, std 20.25, time cost 1.306s.
Training steps per second: 166.3.
Step 33000; q_loss 0.08332; mean_q -13.6; min_q -26.72; max_q 0.1477; mean_r -0.75; mean_dsc 0.98; 
Tested 50 episodes at step 33000, reward mean -31.44, std 18.35, time cost 1.328s.
Training steps per second: 165.9.
Step 34000; q_loss 0.05406; mean_q -15.6; min_q -26.43; max_q 0.1582; mean_r -0.8652; mean_dsc 0.98; 
Tested 50 episodes at step 34000, reward mean -30.54, std 18.36, time cost 1.323s.
Training steps per second: 171.9.
Step 35000; q_loss 0.04082; mean_q -15.2; min_q -27.06; max_q 0.1532; mean_r -0.8467; mean_dsc 0.98; 
Tested 50 episodes at step 35000, reward mean -30.48, std 19.42, time cost 1.303s.
Training steps per second: 171.
Step 36000; q_loss 0.1281; mean_q -17.1; min_q -27.43; max_q 0.1546; mean_r -0.879; mean_dsc 0.98; 
Tested 50 episodes at step 36000, reward mean -31.92, std 19.7, time cost 1.392s.
Training steps per second: 169.3.
Step 37000; q_loss 0.07632; mean_q -15.43; min_q -28.77; max_q 0.152; mean_r -0.8179; mean_dsc 0.98; 
Tested 50 episodes at step 37000, reward mean -28.16, std 17.69, time cost 1.272s.
Training steps per second: 173.5.
Step 38000; q_loss 0.09381; mean_q -16.83; min_q -28.85; max_q 0.1513; mean_r -0.9001; mean_dsc 0.98; 
Tested 50 episodes at step 38000, reward mean -28.28, std 17.95, time cost 1.28s.
Training steps per second: 173.5.
Step 39000; q_loss 0.07223; mean_q -16.07; min_q -28.66; max_q 0.1754; mean_r -0.8657; mean_dsc 0.98; 
Tested 50 episodes at step 39000, reward mean -31.06, std 18.36, time cost 1.311s.
Training steps per second: 161.1.
Step 40000; q_loss 0.07889; mean_q -15.34; min_q -29.2; max_q 0.1699; mean_r -0.8281; mean_dsc 0.98; 
Tested 50 episodes at step 40000, reward mean -27.6, std 17.65, time cost 1.316s.
Training steps per second: 170.9.
Step 41000; q_loss 0.05828; mean_q -16.06; min_q -29.41; max_q 0.1671; mean_r -0.8534; mean_dsc 0.98; 
Tested 50 episodes at step 41000, reward mean -27.72, std 18.37, time cost 1.261s.
Training steps per second: 174.1.
Step 42000; q_loss 0.03178; mean_q -14.78; min_q -30.08; max_q 0.1706; mean_r -0.7968; mean_dsc 0.98; 
Tested 50 episodes at step 42000, reward mean -30.26, std 17.57, time cost 1.32s.
Training steps per second: 167.2.
Step 43000; q_loss 0.07902; mean_q -15.16; min_q -31.07; max_q 0.161; mean_r -0.8173; mean_dsc 0.98; 
Tested 50 episodes at step 43000, reward mean -25.16, std 16.81, time cost 1.393s.
Training steps per second: 169.6.
Step 44000; q_loss 0.04413; mean_q -14.4; min_q -30.25; max_q 0.16; mean_r -0.8058; mean_dsc 0.98; 
Tested 50 episodes at step 44000, reward mean -27.22, std 18.21, time cost 1.333s.
Training steps per second: 167.6.
Step 45000; q_loss 0.03935; mean_q -12.98; min_q -30.88; max_q 0.1691; mean_r -0.782; mean_dsc 0.98; 
Tested 50 episodes at step 45000, reward mean -29.66, std 19.38, time cost 1.286s.
Training steps per second: 168.4.
Step 46000; q_loss 0.053; mean_q -15.74; min_q -30.6; max_q 0.1721; mean_r -0.8273; mean_dsc 0.98; 
Tested 50 episodes at step 46000, reward mean -28.9, std 17.58, time cost 1.775s.
Training steps per second: 156.5.
Step 47000; q_loss 0.08889; mean_q -15.21; min_q -30.97; max_q 0.1628; mean_r -0.7851; mean_dsc 0.98; 
Tested 50 episodes at step 47000, reward mean -27.62, std 16.34, time cost 1.248s.
Training steps per second: 177.4.
Step 48000; q_loss 0.03538; mean_q -15.15; min_q -32.66; max_q 0.1649; mean_r -0.8511; mean_dsc 0.98; 
Tested 50 episodes at step 48000, reward mean -22.98, std 14.96, time cost 1.238s.
Training steps per second: 178.
Step 49000; q_loss 0.05063; mean_q -15.75; min_q -32.08; max_q 0.1619; mean_r -0.7675; mean_dsc 0.98; 
Tested 50 episodes at step 49000, reward mean -22.38, std 15.23, time cost 1.269s.
Training steps per second: 169.7.
Step 50000; q_loss 0.03465; mean_q -13.3; min_q -31.39; max_q 0.1628; mean_r -0.78; mean_dsc 0.98; 
Tested 50 episodes at step 50000, reward mean -28.54, std 16.7, time cost 1.309s.
Training steps per second: 175.1.
Step 51000; q_loss 0.02878; mean_q -14.3; min_q -33.34; max_q 0.1591; mean_r -0.8364; mean_dsc 0.98; 
Tested 50 episodes at step 51000, reward mean -22.34, std 15.11, time cost 1.301s.
Training steps per second: 170.6.
Step 52000; q_loss 0.02955; mean_q -13.98; min_q -31.16; max_q 0.1651; mean_r -0.8045; mean_dsc 0.98; 
Tested 50 episodes at step 52000, reward mean -25.4, std 16.7, time cost 1.282s.
Training steps per second: 174.6.
Step 53000; q_loss 0.01416; mean_q -16.88; min_q -31.62; max_q 0.1757; mean_r -0.9123; mean_dsc 0.98; 
Tested 50 episodes at step 53000, reward mean -27.32, std 14.22, time cost 1.412s.
Training steps per second: 164.2.
Step 54000; q_loss 0.01565; mean_q -15.97; min_q -32.29; max_q 0.18; mean_r -0.8349; mean_dsc 0.98; 
Tested 50 episodes at step 54000, reward mean -22.58, std 13.73, time cost 1.263s.
Training steps per second: 174.4.
Step 55000; q_loss 0.0213; mean_q -14.92; min_q -32.51; max_q 0.1792; mean_r -0.8017; mean_dsc 0.98; 
Tested 50 episodes at step 55000, reward mean -23.8, std 14.47, time cost 1.266s.
Training steps per second: 172.
Step 56000; q_loss 0.01788; mean_q -13.97; min_q -32.71; max_q 0.1797; mean_r -0.784; mean_dsc 0.98; 
Tested 50 episodes at step 56000, reward mean -26.4, std 15, time cost 1.256s.
Training steps per second: 169.3.
Step 57000; q_loss 0.02551; mean_q -15.54; min_q -33.94; max_q 0.1787; mean_r -0.8359; mean_dsc 0.98; 
Tested 50 episodes at step 57000, reward mean -24.16, std 14.79, time cost 1.297s.
Training steps per second: 169.8.
Step 58000; q_loss 0.02949; mean_q -14.07; min_q -33.14; max_q 0.1815; mean_r -0.7669; mean_dsc 0.98; 
Tested 50 episodes at step 58000, reward mean -25.54, std 16.08, time cost 1.269s.
Training steps per second: 170.
Step 59000; q_loss 0.02369; mean_q -14.22; min_q -33.33; max_q 0.1797; mean_r -0.8038; mean_dsc 0.98; 
Tested 50 episodes at step 59000, reward mean -27.1, std 14.66, time cost 1.281s.
Training steps per second: 163.9.
Step 60000; q_loss 0.03003; mean_q -14.63; min_q -33.98; max_q 0.1802; mean_r -0.7776; mean_dsc 0.98; 
Tested 50 episodes at step 60000, reward mean -22, std 14.13, time cost 1.299s.
Training steps per second: 166.5.
Step 61000; q_loss 0.02418; mean_q -13.44; min_q -34.17; max_q 0.1737; mean_r -0.761; mean_dsc 0.98; 
Tested 50 episodes at step 61000, reward mean -29.74, std 16.44, time cost 1.239s.
Training steps per second: 172.7.
Step 62000; q_loss 0.006157; mean_q -16.29; min_q -35.72; max_q 0.1753; mean_r -0.8513; mean_dsc 0.98; 
Tested 50 episodes at step 62000, reward mean -30.24, std 14.24, time cost 1.29s.
Training steps per second: 171.1.
Step 63000; q_loss 0.01078; mean_q -13.97; min_q -34.97; max_q 0.1723; mean_r -0.7774; mean_dsc 0.98; 
Tested 50 episodes at step 63000, reward mean -22.22, std 13.04, time cost 1.333s.
Training steps per second: 172.8.
Step 64000; q_loss 0.005451; mean_q -12.88; min_q -34.88; max_q 0.1701; mean_r -0.7212; mean_dsc 0.98; 
Tested 50 episodes at step 64000, reward mean -23.34, std 15.36, time cost 1.311s.
Training steps per second: 168.2.
Step 65000; q_loss 0.006637; mean_q -14.95; min_q -35.66; max_q 0.1698; mean_r -0.8168; mean_dsc 0.98; 
Tested 50 episodes at step 65000, reward mean -22.64, std 14.57, time cost 1.288s.
Training steps per second: 173.7.
Step 66000; q_loss 0.004182; mean_q -14.06; min_q -35.97; max_q 0.1727; mean_r -0.7284; mean_dsc 0.98; 
Tested 50 episodes at step 66000, reward mean -22.14, std 13.38, time cost 1.311s.
Training steps per second: 170.4.
Step 67000; q_loss 0.006478; mean_q -12.24; min_q -36.43; max_q 0.1748; mean_r -0.6945; mean_dsc 0.98; 
Tested 50 episodes at step 67000, reward mean -27.22, std 15.09, time cost 1.259s.
Training steps per second: 171.3.
Step 68000; q_loss 0.004723; mean_q -14.16; min_q -36.76; max_q 0.1771; mean_r -0.7592; mean_dsc 0.98; 
Tested 50 episodes at step 68000, reward mean -23.66, std 15.67, time cost 1.372s.
Training steps per second: 162.6.
Step 69000; q_loss 0.002698; mean_q -11.95; min_q -37.3; max_q 0.1788; mean_r -0.6986; mean_dsc 0.98; 
Tested 50 episodes at step 69000, reward mean -28.16, std 15.6, time cost 1.267s.
Training steps per second: 170.9.
Step 70000; q_loss 0.003425; mean_q -13.77; min_q -37.24; max_q 0.1734; mean_r -0.7303; mean_dsc 0.98; 
Tested 50 episodes at step 70000, reward mean -26.8, std 14.93, time cost 1.362s.
Training steps per second: 167.9.
Step 71000; q_loss 0.003398; mean_q -12.6; min_q -37.47; max_q 0.1717; mean_r -0.7007; mean_dsc 0.98; 
Tested 50 episodes at step 71000, reward mean -29.22, std 15.72, time cost 1.392s.
Training steps per second: 166.8.
Step 72000; q_loss 0.00651; mean_q -14.38; min_q -38.65; max_q 0.1738; mean_r -0.7519; mean_dsc 0.98; 
Tested 50 episodes at step 72000, reward mean -23.86, std 14.42, time cost 1.271s.
Training steps per second: 170.6.
Step 73000; q_loss 0.02158; mean_q -14.73; min_q -38.84; max_q 0.1715; mean_r -0.7858; mean_dsc 0.98; 
Tested 50 episodes at step 73000, reward mean -26.28, std 15.36, time cost 1.326s.
Training steps per second: 169.9.
Step 74000; q_loss 0.01242; mean_q -13.25; min_q -37.79; max_q 0.171; mean_r -0.7879; mean_dsc 0.98; 
Tested 50 episodes at step 74000, reward mean -26.22, std 14.1, time cost 1.248s.
Training steps per second: 174.1.
Step 75000; q_loss 0.005066; mean_q -12.73; min_q -39.3; max_q 0.1707; mean_r -0.7274; mean_dsc 0.98; 
Tested 50 episodes at step 75000, reward mean -25.36, std 14, time cost 1.294s.
Training steps per second: 172.8.
Step 76000; q_loss 0.003495; mean_q -13.49; min_q -39.2; max_q 0.1732; mean_r -0.7326; mean_dsc 0.98; 
Tested 50 episodes at step 76000, reward mean -26.66, std 15.2, time cost 1.543s.
Training steps per second: 156.7.
Step 77000; q_loss 0.005338; mean_q -13.55; min_q -38.83; max_q 0.1721; mean_r -0.7226; mean_dsc 0.98; 
Tested 50 episodes at step 77000, reward mean -21.64, std 14.91, time cost 1.357s.
Training steps per second: 170.4.
Step 78000; q_loss 0.0122; mean_q -12.79; min_q -40.14; max_q 0.171; mean_r -0.7495; mean_dsc 0.98; 
Tested 50 episodes at step 78000, reward mean -26.78, std 16.51, time cost 1.254s.
Training steps per second: 173.4.
Step 79000; q_loss 0.006417; mean_q -13.91; min_q -38.49; max_q 0.172; mean_r -0.7406; mean_dsc 0.98; 
Tested 50 episodes at step 79000, reward mean -23.14, std 13.09, time cost 1.278s.
Training steps per second: 163.7.
Step 80000; q_loss 0.005302; mean_q -11.17; min_q -39.49; max_q 0.1711; mean_r -0.6848; mean_dsc 0.98; 
Tested 50 episodes at step 80000, reward mean -26.42, std 15.98, time cost 1.257s.
Training steps per second: 177.
Step 81000; q_loss 0.01257; mean_q -12.95; min_q -40.48; max_q 0.1729; mean_r -0.7656; mean_dsc 0.98; 
Tested 50 episodes at step 81000, reward mean -27.14, std 14.52, time cost 1.23s.
Training steps per second: 168.8.
Step 82000; q_loss 0.01369; mean_q -11.94; min_q -40.88; max_q 0.1711; mean_r -0.6691; mean_dsc 0.98; 
Tested 50 episodes at step 82000, reward mean -26.34, std 11.8, time cost 1.258s.
Training steps per second: 171.8.
Step 83000; q_loss 0.006927; mean_q -11.23; min_q -39.96; max_q 0.171; mean_r -0.6767; mean_dsc 0.98; 
Tested 50 episodes at step 83000, reward mean -23.42, std 15.84, time cost 1.252s.
Training steps per second: 173.6.
Step 84000; q_loss 0.0226; mean_q -14.94; min_q -40.14; max_q 0.1725; mean_r -0.7392; mean_dsc 0.98; 
Tested 50 episodes at step 84000, reward mean -23.76, std 16.24, time cost 1.264s.
Training steps per second: 172.4.
Step 85000; q_loss 0.02002; mean_q -14.21; min_q -40.16; max_q 0.1775; mean_r -0.7473; mean_dsc 0.98; 
Tested 50 episodes at step 85000, reward mean -26.08, std 14.31, time cost 1.256s.
Training steps per second: 174.6.
Step 86000; q_loss 0.00777; mean_q -12.3; min_q -41.3; max_q 0.173; mean_r -0.7068; mean_dsc 0.98; 
Tested 50 episodes at step 86000, reward mean -26.42, std 13.58, time cost 1.288s.
Training steps per second: 169.8.
Step 87000; q_loss 0.007967; mean_q -12.93; min_q -41.07; max_q 0.1735; mean_r -0.7011; mean_dsc 0.98; 
Tested 50 episodes at step 87000, reward mean -24.34, std 13.45, time cost 1.263s.
Training steps per second: 172.8.
Step 88000; q_loss 0.008481; mean_q -11.83; min_q -40.7; max_q 0.1694; mean_r -0.6687; mean_dsc 0.98; 
Tested 50 episodes at step 88000, reward mean -24.98, std 13.49, time cost 1.247s.
Training steps per second: 175.7.
Step 89000; q_loss 0.003856; mean_q -13.99; min_q -41.75; max_q 0.1701; mean_r -0.7496; mean_dsc 0.98; 
Tested 50 episodes at step 89000, reward mean -22.54, std 15.66, time cost 1.239s.
Training steps per second: 171.8.
Step 90000; q_loss 0.02065; mean_q -14.05; min_q -42.49; max_q 0.169; mean_r -0.7299; mean_dsc 0.98; 
Tested 50 episodes at step 90000, reward mean -25.62, std 14.22, time cost 1.239s.
Training steps per second: 175.5.
Step 91000; q_loss 0.006626; mean_q -13.77; min_q -41.93; max_q 0.1693; mean_r -0.7308; mean_dsc 0.98; 
Tested 50 episodes at step 91000, reward mean -24.38, std 14.71, time cost 1.228s.
Training steps per second: 175.6.
Step 92000; q_loss 0.004609; mean_q -11.59; min_q -40.45; max_q 0.168; mean_r -0.7055; mean_dsc 0.98; 
Tested 50 episodes at step 92000, reward mean -27.04, std 14.78, time cost 1.248s.
Training steps per second: 174.8.
Step 93000; q_loss 0.007849; mean_q -13.94; min_q -41.86; max_q 0.1668; mean_r -0.7613; mean_dsc 0.98; 
Tested 50 episodes at step 93000, reward mean -25.42, std 15.07, time cost 1.288s.
Training steps per second: 173.4.
Step 94000; q_loss 0.001169; mean_q -13.3; min_q -39.2; max_q 0.1665; mean_r -0.7518; mean_dsc 0.98; 
Tested 50 episodes at step 94000, reward mean -22.8, std 13.47, time cost 1.314s.
Training steps per second: 173.5.
Step 95000; q_loss 0.00163; mean_q -13.55; min_q -39.56; max_q 0.1669; mean_r -0.741; mean_dsc 0.98; 
Tested 50 episodes at step 95000, reward mean -28.02, std 13.19, time cost 1.251s.
Training steps per second: 172.6.
Step 96000; q_loss 0.00632; mean_q -13.79; min_q -40.09; max_q 0.1691; mean_r -0.7248; mean_dsc 0.98; 
Tested 50 episodes at step 96000, reward mean -24, std 14.46, time cost 1.214s.
Training steps per second: 175.2.
Step 97000; q_loss 0.002874; mean_q -11.34; min_q -36.8; max_q 0.1641; mean_r -0.6844; mean_dsc 0.98; 
Tested 50 episodes at step 97000, reward mean -27.68, std 15.35, time cost 1.249s.
Training steps per second: 174.4.
Step 98000; q_loss 0.003557; mean_q -14.1; min_q -38.29; max_q 0.1601; mean_r -0.8036; mean_dsc 0.98; 
Tested 50 episodes at step 98000, reward mean -25.44, std 13.69, time cost 1.257s.
Training steps per second: 173.2.
Step 99000; q_loss 0.002869; mean_q -14.77; min_q -38.74; max_q 0.1552; mean_r -0.7463; mean_dsc 0.98; 
Tested 50 episodes at step 99000, reward mean -25.16, std 14.75, time cost 1.216s.
Training steps per second: 166.2.
Step 100000; q_loss 0.001182; mean_q -13; min_q -36.99; max_q 0.1542; mean_r -0.7397; mean_dsc 0.98; 
Tested 50 episodes at step 100000, reward mean -25.74, std 12.64, time cost 1.385s.
Training steps per second: 170.2.
Step 101000; q_loss 0.004658; mean_q -12.18; min_q -37.97; max_q 0.1545; mean_r -0.6933; mean_dsc 0.98; 
Tested 50 episodes at step 101000, reward mean -21.9, std 13.49, time cost 1.28s.
Training steps per second: 172.1.
Step 102000; q_loss 0.001628; mean_q -11.87; min_q -38.78; max_q 0.152; mean_r -0.6685; mean_dsc 0.98; 
Tested 50 episodes at step 102000, reward mean -24.38, std 15.85, time cost 1.23s.
Training steps per second: 172.8.
Step 103000; q_loss 0.001236; mean_q -11.75; min_q -39.21; max_q 0.1508; mean_r -0.7034; mean_dsc 0.98; 
Tested 50 episodes at step 103000, reward mean -22.2, std 11.26, time cost 1.313s.
Training steps per second: 170.4.
Step 104000; q_loss 0.002167; mean_q -11.16; min_q -37.35; max_q 0.1511; mean_r -0.675; mean_dsc 0.98; 
Tested 50 episodes at step 104000, reward mean -26.6, std 14.67, time cost 1.318s.
Training steps per second: 171.9.
Step 105000; q_loss 0.001569; mean_q -11.87; min_q -38.42; max_q 0.149; mean_r -0.7168; mean_dsc 0.98; 
Tested 50 episodes at step 105000, reward mean -25.36, std 12.77, time cost 1.253s.
Training steps per second: 170.3.
Step 106000; q_loss 0.008081; mean_q -11.23; min_q -38.83; max_q 0.1468; mean_r -0.6923; mean_dsc 0.98; 
Tested 50 episodes at step 106000, reward mean -27.12, std 14.78, time cost 1.286s.
Training steps per second: 146.1.
Step 107000; q_loss 0.00107; mean_q -12.44; min_q -40.86; max_q 0.1464; mean_r -0.7226; mean_dsc 0.98; 
Tested 50 episodes at step 107000, reward mean -26.38, std 14.22, time cost 1.479s.
Training steps per second: 163.6.
Step 108000; q_loss 0.009014; mean_q -11.14; min_q -39.98; max_q 0.1466; mean_r -0.7048; mean_dsc 0.98; 
Tested 50 episodes at step 108000, reward mean -24.56, std 15.44, time cost 1.247s.
Training steps per second: 172.3.
Step 109000; q_loss 0.002271; mean_q -12.79; min_q -37.31; max_q 0.1466; mean_r -0.7485; mean_dsc 0.98; 
Tested 50 episodes at step 109000, reward mean -26.88, std 14.5, time cost 1.277s.
Training steps per second: 161.
Step 110000; q_loss 0.00283; mean_q -12.26; min_q -39.69; max_q 0.1427; mean_r -0.6643; mean_dsc 0.98; 
Tested 50 episodes at step 110000, reward mean -26.76, std 16.22, time cost 1.248s.
Training steps per second: 169.6.
Step 111000; q_loss 0.00133; mean_q -12.51; min_q -36.59; max_q 0.1414; mean_r -0.71; mean_dsc 0.98; 
Tested 50 episodes at step 111000, reward mean -28.46, std 15.56, time cost 1.39s.
Training steps per second: 169.
Step 112000; q_loss 0.0013; mean_q -14.68; min_q -39.37; max_q 0.1428; mean_r -0.757; mean_dsc 0.98; 
Tested 50 episodes at step 112000, reward mean -27.72, std 13.88, time cost 1.272s.
Training steps per second: 172.5.
Step 113000; q_loss 0.0008721; mean_q -12.1; min_q -40.91; max_q 0.1405; mean_r -0.7046; mean_dsc 0.98; 
Tested 50 episodes at step 113000, reward mean -23.1, std 14.73, time cost 1.246s.
Training steps per second: 175.6.
Step 114000; q_loss 0.0009096; mean_q -12.02; min_q -39.4; max_q 0.1391; mean_r -0.6547; mean_dsc 0.98; 
Tested 50 episodes at step 114000, reward mean -25.06, std 15.29, time cost 1.307s.
Training steps per second: 170.5.
Step 115000; q_loss 0.002534; mean_q -11.6; min_q -38.61; max_q 0.1384; mean_r -0.7245; mean_dsc 0.98; 
Tested 50 episodes at step 115000, reward mean -25.06, std 13.16, time cost 1.273s.
Training steps per second: 172.
Step 116000; q_loss 0.01484; mean_q -12.51; min_q -41.07; max_q 0.1419; mean_r -0.6993; mean_dsc 0.98; 
Tested 50 episodes at step 116000, reward mean -24.34, std 15.01, time cost 1.234s.
Training steps per second: 171.8.
Step 117000; q_loss 0.008743; mean_q -10.8; min_q -36.64; max_q 0.1389; mean_r -0.6467; mean_dsc 0.98; 
Tested 50 episodes at step 117000, reward mean -25.64, std 13.57, time cost 1.243s.
Training steps per second: 172.7.
Step 118000; q_loss 0.01631; mean_q -12.52; min_q -40.28; max_q 0.1364; mean_r -0.7033; mean_dsc 0.98; 
Tested 50 episodes at step 118000, reward mean -26.26, std 15.09, time cost 1.268s.
Training steps per second: 169.9.
Step 119000; q_loss 0.004961; mean_q -11.51; min_q -36.28; max_q 0.1343; mean_r -0.7254; mean_dsc 0.98; 
Tested 50 episodes at step 119000, reward mean -26.6, std 13.95, time cost 1.217s.
Training steps per second: 168.8.
Step 120000; q_loss 0.00159; mean_q -12.25; min_q -38.18; max_q 0.1357; mean_r -0.7071; mean_dsc 0.98; 
Tested 50 episodes at step 120000, reward mean -27.64, std 14.63, time cost 1.294s.
Training steps per second: 169.8.
Step 121000; q_loss 0.0009768; mean_q -12.68; min_q -37.78; max_q 0.1314; mean_r -0.7698; mean_dsc 0.98; 
Tested 50 episodes at step 121000, reward mean -23.98, std 14.33, time cost 1.246s.
Training steps per second: 174.3.
Step 122000; q_loss 0.001816; mean_q -12.43; min_q -41.16; max_q 0.128; mean_r -0.7319; mean_dsc 0.98; 
Tested 50 episodes at step 122000, reward mean -26.52, std 12.83, time cost 1.233s.
Training steps per second: 175.
Step 123000; q_loss 0.005403; mean_q -12.63; min_q -39; max_q 0.127; mean_r -0.7135; mean_dsc 0.98; 
Tested 50 episodes at step 123000, reward mean -24.54, std 15.74, time cost 1.298s.
Training steps per second: 174.5.
Step 124000; q_loss 0.002785; mean_q -11.01; min_q -36.95; max_q 0.1246; mean_r -0.6372; mean_dsc 0.98; 
Tested 50 episodes at step 124000, reward mean -21.62, std 13.84, time cost 1.323s.
Training steps per second: 170.
Step 125000; q_loss 0.02004; mean_q -12.64; min_q -38.49; max_q 0.1265; mean_r -0.7088; mean_dsc 0.98; 
Tested 50 episodes at step 125000, reward mean -27.06, std 15.56, time cost 1.254s.
Training steps per second: 173.
Step 126000; q_loss 0.001726; mean_q -11.46; min_q -36.93; max_q 0.1253; mean_r -0.7196; mean_dsc 0.98; 
Tested 50 episodes at step 126000, reward mean -28.04, std 15.93, time cost 1.22s.
Training steps per second: 177.3.
Step 127000; q_loss 0.002526; mean_q -12.81; min_q -40.3; max_q 0.1236; mean_r -0.7131; mean_dsc 0.98; 
Tested 50 episodes at step 127000, reward mean -20.88, std 15.88, time cost 1.246s.
Training steps per second: 172.9.
Step 128000; q_loss 0.003753; mean_q -11.86; min_q -37.34; max_q 0.1217; mean_r -0.733; mean_dsc 0.98; 
Tested 50 episodes at step 128000, reward mean -26.8, std 14.93, time cost 1.368s.
Training steps per second: 163.4.
Step 129000; q_loss 0.002718; mean_q -10.95; min_q -36.95; max_q 0.1236; mean_r -0.6279; mean_dsc 0.98; 
Tested 50 episodes at step 129000, reward mean -22.68, std 13.89, time cost 1.363s.
Training steps per second: 165.7.
Step 130000; q_loss 0.005113; mean_q -11.33; min_q -41.18; max_q 0.1195; mean_r -0.696; mean_dsc 0.98; 
Tested 50 episodes at step 130000, reward mean -28.3, std 15.23, time cost 1.283s.
Training steps per second: 168.
Step 131000; q_loss 0.001441; mean_q -12.09; min_q -39.03; max_q 0.1204; mean_r -0.7306; mean_dsc 0.98; 
Tested 50 episodes at step 131000, reward mean -30.22, std 14.16, time cost 1.283s.
Training steps per second: 170.
Step 132000; q_loss 0.002961; mean_q -11.96; min_q -39.02; max_q 0.1183; mean_r -0.6942; mean_dsc 0.98; 
Tested 50 episodes at step 132000, reward mean -23.54, std 14.71, time cost 1.261s.
Training steps per second: 172.6.
Step 133000; q_loss 0.0009587; mean_q -10.72; min_q -39.49; max_q 0.1158; mean_r -0.6657; mean_dsc 0.98; 
Tested 50 episodes at step 133000, reward mean -25.46, std 14.4, time cost 1.224s.
Training steps per second: 175.7.
Step 134000; q_loss 0.01539; mean_q -11.74; min_q -40.75; max_q 0.1142; mean_r -0.6912; mean_dsc 0.98; 
Tested 50 episodes at step 134000, reward mean -22.26, std 13.44, time cost 1.321s.
Training steps per second: 166.3.
Step 135000; q_loss 0.001343; mean_q -12.42; min_q -39.39; max_q 0.1138; mean_r -0.6629; mean_dsc 0.98; 
Tested 50 episodes at step 135000, reward mean -23.98, std 15.05, time cost 1.318s.
Training steps per second: 170.
Step 136000; q_loss 0.0009167; mean_q -12.73; min_q -40.74; max_q 0.1176; mean_r -0.7738; mean_dsc 0.98; 
Tested 50 episodes at step 136000, reward mean -24.32, std 15.67, time cost 1.234s.
Training steps per second: 168.3.
Step 137000; q_loss 0.004675; mean_q -11.64; min_q -39.77; max_q 0.1149; mean_r -0.6481; mean_dsc 0.98; 
Tested 50 episodes at step 137000, reward mean -24.8, std 15.05, time cost 1.892s.
Training steps per second: 154.3.
Step 138000; q_loss 0.005406; mean_q -12.3; min_q -38.54; max_q 0.116; mean_r -0.7175; mean_dsc 0.98; 
Tested 50 episodes at step 138000, reward mean -26.56, std 15.27, time cost 1.319s.
Training steps per second: 169.3.
Step 139000; q_loss 0.001887; mean_q -11.25; min_q -38.15; max_q 0.1152; mean_r -0.663; mean_dsc 0.98; 
Tested 50 episodes at step 139000, reward mean -25.3, std 16.09, time cost 1.262s.
Training steps per second: 160.4.
Step 140000; q_loss 0.001186; mean_q -11.63; min_q -38.16; max_q 0.1124; mean_r -0.7379; mean_dsc 0.98; 
Tested 50 episodes at step 140000, reward mean -22.76, std 14.36, time cost 1.235s.
Training steps per second: 174.7.
Step 141000; q_loss 0.0164; mean_q -10.79; min_q -38.14; max_q 0.1117; mean_r -0.6592; mean_dsc 0.98; 
Tested 50 episodes at step 141000, reward mean -25.96, std 13.77, time cost 1.303s.
Training steps per second: 172.3.
Step 142000; q_loss 0.001105; mean_q -11.22; min_q -36.98; max_q 0.1096; mean_r -0.6884; mean_dsc 0.98; 
Tested 50 episodes at step 142000, reward mean -25.46, std 13.62, time cost 1.297s.
Training steps per second: 159.4.
Step 143000; q_loss 0.002165; mean_q -9.981; min_q -37.36; max_q 0.1095; mean_r -0.6119; mean_dsc 0.98; 
Tested 50 episodes at step 143000, reward mean -26.72, std 13.86, time cost 1.251s.
Training steps per second: 170.8.
Step 144000; q_loss 0.00122; mean_q -12.85; min_q -38.16; max_q 0.107; mean_r -0.7672; mean_dsc 0.98; 
Tested 50 episodes at step 144000, reward mean -24.56, std 14.97, time cost 1.285s.
Training steps per second: 173.7.
Step 145000; q_loss 0.001401; mean_q -10.12; min_q -40.07; max_q 0.105; mean_r -0.7003; mean_dsc 0.98; 
Tested 50 episodes at step 145000, reward mean -26, std 14.46, time cost 1.312s.
Training steps per second: 171.9.
Step 146000; q_loss 0.0001611; mean_q -11.24; min_q -38.17; max_q 0.1036; mean_r -0.7419; mean_dsc 0.98; 
Tested 50 episodes at step 146000, reward mean -22.92, std 14.25, time cost 1.219s.
Training steps per second: 175.8.
Step 147000; q_loss 0.0006111; mean_q -10.31; min_q -36.97; max_q 0.1033; mean_r -0.6688; mean_dsc 0.98; 
Tested 50 episodes at step 147000, reward mean -24.82, std 12.19, time cost 1.218s.
Training steps per second: 170.7.
Step 148000; q_loss 0.006425; mean_q -12.15; min_q -39.45; max_q 0.09997; mean_r -0.6969; mean_dsc 0.98; 
Tested 50 episodes at step 148000, reward mean -20.38, std 14.57, time cost 1.305s.
Training steps per second: 164.9.
Step 149000; q_loss 0.002655; mean_q -10.1; min_q -40.39; max_q 0.09993; mean_r -0.6388; mean_dsc 0.98; 
Tested 50 episodes at step 149000, reward mean -25.52, std 14.86, time cost 1.443s.
Training steps per second: 159.5.
Step 150000; q_loss 0.0006166; mean_q -10.52; min_q -41.23; max_q 0.09751; mean_r -0.6248; mean_dsc 0.98; 
Tested 50 episodes at step 150000, reward mean -23.52, std 13.8, time cost 1.264s.
Training steps per second: 172.5.
Step 151000; q_loss 0.001054; mean_q -13.16; min_q -39.49; max_q 0.09735; mean_r -0.7824; mean_dsc 0.98; 
Tested 50 episodes at step 151000, reward mean -24.7, std 14.4, time cost 1.32s.
Training steps per second: 170.2.
Step 152000; q_loss 0.005969; mean_q -12.9; min_q -40.34; max_q 0.09741; mean_r -0.7286; mean_dsc 0.98; 
Tested 50 episodes at step 152000, reward mean -20.76, std 14.46, time cost 1.274s.
Training steps per second: 171.1.
Step 153000; q_loss 0.00585; mean_q -10.12; min_q -38.96; max_q 0.09527; mean_r -0.6409; mean_dsc 0.98; 
Tested 50 episodes at step 153000, reward mean -25.54, std 14.68, time cost 1.276s.
Training steps per second: 172.4.
Step 154000; q_loss 0.003414; mean_q -9.921; min_q -38.16; max_q 0.09482; mean_r -0.6163; mean_dsc 0.98; 
Tested 50 episodes at step 154000, reward mean -23.74, std 13, time cost 1.292s.
Training steps per second: 170.
Step 155000; q_loss 0.008328; mean_q -11.04; min_q -36.58; max_q 0.09736; mean_r -0.6954; mean_dsc 0.98; 
Tested 50 episodes at step 155000, reward mean -22.72, std 14.59, time cost 1.32s.
Training steps per second: 166.7.
Step 156000; q_loss 0.001258; mean_q -11.86; min_q -37.7; max_q 0.09747; mean_r -0.7191; mean_dsc 0.98; 
Tested 50 episodes at step 156000, reward mean -25.3, std 15.37, time cost 1.344s.
Training steps per second: 170.8.
Step 157000; q_loss 0.0008874; mean_q -11.6; min_q -38.49; max_q 0.09748; mean_r -0.6908; mean_dsc 0.98; 
Tested 50 episodes at step 157000, reward mean -25.26, std 15.34, time cost 1.361s.
Training steps per second: 171.
Step 158000; q_loss 0.000851; mean_q -11.78; min_q -38.52; max_q 0.09614; mean_r -0.6192; mean_dsc 0.98; 
Tested 50 episodes at step 158000, reward mean -24.1, std 12.88, time cost 1.381s.
Training steps per second: 169.6.
Step 159000; q_loss 0.007229; mean_q -10.49; min_q -38.55; max_q 0.09453; mean_r -0.6299; mean_dsc 0.98; 
Tested 50 episodes at step 159000, reward mean -25.5, std 16.05, time cost 1.202s.
Training steps per second: 169.6.
Step 160000; q_loss 0.005372; mean_q -11.09; min_q -39.86; max_q 0.09507; mean_r -0.6394; mean_dsc 0.98; 
Tested 50 episodes at step 160000, reward mean -27.66, std 15.53, time cost 1.263s.
Training steps per second: 176.4.
Step 161000; q_loss 0.008422; mean_q -10.84; min_q -38.54; max_q 0.09378; mean_r -0.6963; mean_dsc 0.98; 
Tested 50 episodes at step 161000, reward mean -25.44, std 14.69, time cost 1.239s.
Training steps per second: 172.2.
Step 162000; q_loss 0.005813; mean_q -10.04; min_q -40.74; max_q 0.09438; mean_r -0.64; mean_dsc 0.98; 
Tested 50 episodes at step 162000, reward mean -26.56, std 13.5, time cost 1.276s.
Training steps per second: 174.4.
Step 163000; q_loss 0.0009344; mean_q -12.69; min_q -37.32; max_q 0.09491; mean_r -0.6782; mean_dsc 0.98; 
Tested 50 episodes at step 163000, reward mean -24.06, std 14.35, time cost 1.252s.
Training steps per second: 168.5.
Step 164000; q_loss 0.001168; mean_q -11.48; min_q -37.31; max_q 0.09418; mean_r -0.7251; mean_dsc 0.98; 
Tested 50 episodes at step 164000, reward mean -21.92, std 13.14, time cost 1.347s.
Training steps per second: 167.9.
Step 165000; q_loss 0.0007303; mean_q -11.74; min_q -38.06; max_q 0.09376; mean_r -0.6749; mean_dsc 0.98; 
Tested 50 episodes at step 165000, reward mean -20.72, std 13.82, time cost 1.323s.
Training steps per second: 170.
Step 166000; q_loss 0.002749; mean_q -11.2; min_q -37.72; max_q 0.09054; mean_r -0.6802; mean_dsc 0.98; 
Tested 50 episodes at step 166000, reward mean -27.72, std 15.85, time cost 1.255s.
Training steps per second: 159.
Step 167000; q_loss 0.003431; mean_q -11.86; min_q -40.2; max_q 0.0893; mean_r -0.6432; mean_dsc 0.98; 
Tested 50 episodes at step 167000, reward mean -27.3, std 14.99, time cost 1.463s.
Training steps per second: 164.7.
Step 168000; q_loss 0.0005489; mean_q -9.972; min_q -41.12; max_q 0.08752; mean_r -0.5983; mean_dsc 0.98; 
Tested 50 episodes at step 168000, reward mean -24.4, std 13.82, time cost 1.257s.
Training steps per second: 168.
Step 169000; q_loss 0.002136; mean_q -9.755; min_q -37.41; max_q 0.08832; mean_r -0.634; mean_dsc 0.98; 
Tested 50 episodes at step 169000, reward mean -25.06, std 15.42, time cost 1.269s.
Training steps per second: 166.6.
Step 170000; q_loss 0.0005286; mean_q -9.832; min_q -39.43; max_q 0.08812; mean_r -0.6576; mean_dsc 0.98; 
Tested 50 episodes at step 170000, reward mean -26.9, std 15.46, time cost 1.314s.
Training steps per second: 169.7.
Step 171000; q_loss 0.01202; mean_q -13.35; min_q -40.31; max_q 0.08698; mean_r -0.6907; mean_dsc 0.98; 
Tested 50 episodes at step 171000, reward mean -24.1, std 14.02, time cost 1.35s.
Training steps per second: 167.9.
Step 172000; q_loss 0.001828; mean_q -11.35; min_q -37.36; max_q 0.08838; mean_r -0.6384; mean_dsc 0.98; 
Tested 50 episodes at step 172000, reward mean -23.14, std 14.88, time cost 1.257s.
Training steps per second: 176.1.
Step 173000; q_loss 0.003331; mean_q -11.33; min_q -37.04; max_q 0.08853; mean_r -0.6695; mean_dsc 0.98; 
Tested 50 episodes at step 173000, reward mean -25.76, std 12.9, time cost 1.238s.
Training steps per second: 172.6.
Step 174000; q_loss 0.00237; mean_q -10.42; min_q -39.5; max_q 0.08975; mean_r -0.6799; mean_dsc 0.98; 
Tested 50 episodes at step 174000, reward mean -24.1, std 14.47, time cost 1.283s.
Training steps per second: 174.1.
Step 175000; q_loss 0.001646; mean_q -11.96; min_q -40.35; max_q 0.09027; mean_r -0.6787; mean_dsc 0.98; 
Tested 50 episodes at step 175000, reward mean -22.54, std 14.98, time cost 1.261s.
Training steps per second: 171.3.
Step 176000; q_loss 0.00292; mean_q -10.73; min_q -39.43; max_q 0.08867; mean_r -0.6324; mean_dsc 0.98; 
Tested 50 episodes at step 176000, reward mean -26.08, std 13.76, time cost 1.27s.
Training steps per second: 177.2.
Step 177000; q_loss 0.003127; mean_q -9.209; min_q -37.79; max_q 0.08976; mean_r -0.648; mean_dsc 0.98; 
Tested 50 episodes at step 177000, reward mean -22.36, std 15.45, time cost 1.313s.
Training steps per second: 171.9.
Step 178000; q_loss 0.01311; mean_q -12.35; min_q -39.01; max_q 0.08972; mean_r -0.7197; mean_dsc 0.98; 
Tested 50 episodes at step 178000, reward mean -23.24, std 12.74, time cost 1.19s.
Training steps per second: 171.9.
Step 179000; q_loss 0.001287; mean_q -8.862; min_q -36.62; max_q 0.09094; mean_r -0.6803; mean_dsc 0.98; 
Tested 50 episodes at step 179000, reward mean -28.12, std 14.5, time cost 1.305s.
Training steps per second: 153.9.
Step 180000; q_loss 0.002273; mean_q -10.98; min_q -38.94; max_q 0.09013; mean_r -0.6307; mean_dsc 0.98; 
Tested 50 episodes at step 180000, reward mean -24.78, std 16.41, time cost 1.278s.
Training steps per second: 160.5.
Step 181000; q_loss 0.002349; mean_q -10.92; min_q -39.01; max_q 0.08745; mean_r -0.6561; mean_dsc 0.98; 
Tested 50 episodes at step 181000, reward mean -26.2, std 14.63, time cost 1.267s.
Training steps per second: 171.7.
Step 182000; q_loss 0.003597; mean_q -11.2; min_q -39.01; max_q 0.08528; mean_r -0.6958; mean_dsc 0.98; 
Tested 50 episodes at step 182000, reward mean -25.82, std 14.82, time cost 1.252s.
Training steps per second: 171.9.
Step 183000; q_loss 0.0007775; mean_q -11.11; min_q -36.87; max_q 0.08528; mean_r -0.6855; mean_dsc 0.98; 
Tested 50 episodes at step 183000, reward mean -24.76, std 16.49, time cost 1.297s.
Training steps per second: 166.5.
Step 184000; q_loss 0.001189; mean_q -10.85; min_q -36.87; max_q 0.08597; mean_r -0.6292; mean_dsc 0.98; 
Tested 50 episodes at step 184000, reward mean -25.3, std 13.28, time cost 1.27s.
Training steps per second: 173.7.
Step 185000; q_loss 0.002177; mean_q -11.79; min_q -39.76; max_q 0.08515; mean_r -0.6829; mean_dsc 0.98; 
Tested 50 episodes at step 185000, reward mean -29.2, std 14.53, time cost 1.311s.
Training steps per second: 169.5.
Step 186000; q_loss 0.0004531; mean_q -9.305; min_q -39.35; max_q 0.08729; mean_r -0.6263; mean_dsc 0.98; 
Tested 50 episodes at step 186000, reward mean -24.2, std 15.17, time cost 1.298s.
Training steps per second: 169.3.
Step 187000; q_loss 0.002856; mean_q -9.604; min_q -39.3; max_q 0.08626; mean_r -0.6527; mean_dsc 0.98; 
Tested 50 episodes at step 187000, reward mean -23.82, std 15.54, time cost 1.26s.
Training steps per second: 173.1.
Step 188000; q_loss 0.002138; mean_q -12.96; min_q -40.16; max_q 0.0855; mean_r -0.7116; mean_dsc 0.98; 
Tested 50 episodes at step 188000, reward mean -26.8, std 12.14, time cost 1.259s.
Training steps per second: 173.2.
Step 189000; q_loss 0.002503; mean_q -12.09; min_q -39.29; max_q 0.08707; mean_r -0.6476; mean_dsc 0.98; 
Tested 50 episodes at step 189000, reward mean -29.28, std 13.99, time cost 1.454s.
Training steps per second: 164.
Step 190000; q_loss 0.0003308; mean_q -12.57; min_q -38.41; max_q 0.0851; mean_r -0.7601; mean_dsc 0.98; 
Tested 50 episodes at step 190000, reward mean -25.3, std 14.95, time cost 1.291s.
Training steps per second: 172.9.
Step 191000; q_loss 0.0003672; mean_q -11.64; min_q -39.71; max_q 0.08461; mean_r -0.7387; mean_dsc 0.98; 
Tested 50 episodes at step 191000, reward mean -25.08, std 13.16, time cost 1.331s.
Training steps per second: 170.9.
Step 192000; q_loss 0.001794; mean_q -11.05; min_q -40.66; max_q 0.08462; mean_r -0.6829; mean_dsc 0.98; 
Tested 50 episodes at step 192000, reward mean -20.56, std 14.58, time cost 1.229s.
Training steps per second: 174.7.
Step 193000; q_loss 0.0005126; mean_q -9.846; min_q -36.96; max_q 0.08624; mean_r -0.6367; mean_dsc 0.98; 
Tested 50 episodes at step 193000, reward mean -23.14, std 13.94, time cost 1.259s.
Training steps per second: 174.7.
Step 194000; q_loss 0.01058; mean_q -8.89; min_q -36.26; max_q 0.08642; mean_r -0.5925; mean_dsc 0.98; 
Tested 50 episodes at step 194000, reward mean -25.3, std 15.41, time cost 1.241s.
Training steps per second: 169.8.
Step 195000; q_loss 0.002693; mean_q -12.09; min_q -36.96; max_q 0.08498; mean_r -0.6704; mean_dsc 0.98; 
Tested 50 episodes at step 195000, reward mean -22.8, std 13.6, time cost 1.234s.
Training steps per second: 176.1.
Step 196000; q_loss 0.0004943; mean_q -10.45; min_q -40.6; max_q 0.0823; mean_r -0.6584; mean_dsc 0.98; 
Tested 50 episodes at step 196000, reward mean -23.62, std 12.68, time cost 1.285s.
Training steps per second: 168.8.
Step 197000; q_loss 0.01346; mean_q -8.136; min_q -36.59; max_q 0.08229; mean_r -0.5699; mean_dsc 0.98; 
Tested 50 episodes at step 197000, reward mean -29.42, std 13.69, time cost 1.355s.
Training steps per second: 155.1.
Step 198000; q_loss 0.002512; mean_q -11.3; min_q -36.91; max_q 0.08562; mean_r -0.7036; mean_dsc 0.98; 
Tested 50 episodes at step 198000, reward mean -26.18, std 16.83, time cost 1.284s.
Training steps per second: 170.4.
Step 199000; q_loss 0.006737; mean_q -11.36; min_q -39.28; max_q 0.08169; mean_r -0.6745; mean_dsc 0.98; 
Tested 50 episodes at step 199000, reward mean -22.66, std 16.23, time cost 1.243s.
Training steps per second: 158.2.
Step 200000; q_loss 0.000808; mean_q -11.39; min_q -38.05; max_q 0.08076; mean_r -0.6514; mean_dsc 0.98; 
Tested 50 episodes at step 200000, reward mean -25.22, std 15.32, time cost 1.244s.
